# .cursor/rules/cainam-core.mdc

```mdc
--- description: Rules for building the Cainam Core Agent, a decentralized AI trading agent platform on Solana, leveraging RIG for LLM interactions and MongoDB for vector storage. globs: --- You are an expert in Rust development, specializing in building AI-powered applications using the RIG framework and integrating with MongoDB for vector storage. You are building the Cainam Core Agent, a decentralized AI trading agent platform on Solana. **General Guidelines:** - Prioritize writing secure, efficient, and maintainable Rust code. - Follow best practices for asynchronous programming and error handling using `anyhow`. - Adhere to the project structure and conventions defined in the `memory-bank`. - Leverage the `rig-core` (referred to as `rig`) and `rig-mongodb` crates for LLM interactions and MongoDB integration, respectively. **RIG-Specific Guidelines:** - Use `rig` for all interactions with Large Language Models (LLMs). This includes: - Creating and managing `CompletionModel` and `EmbeddingModel` instances. - Building and configuring `Agent` instances, including setting preambles and using `dynamic_context` for RAG. - Utilizing `Tool` and `ToolSet` for function calling capabilities. - Leveraging `EmbeddingsBuilder` for generating embeddings. - Using `rig::providers::openai` for interacting with OpenAI models. - Refer to the RIG documentation (provided in the context) for API details and usage examples. - When using `rig-mongodb`, refer to it as `rig_mongodb`. **MongoDB and Vector Store Guidelines:** - Use `rig_mongodb` to interact with MongoDB Atlas, specifically for vector store operations. - Understand the `MongoDbPool` and `MongoDbVectorIndex` implementations in `src/config/mongodb.rs`. - Adhere to the `SearchParams` configuration, ensuring you include the `fields` parameter when performing vector searches. This is a CRITICAL point, as highlighted in the `memory-bank/codeReview.md` and `memory-bank/activeContext.md`. - Refer to `TokenAnalyticsDataExt` trait for database interaction methods. - Ensure all MongoDB interactions include proper error handling with context, using `.context(...)` from the `anyhow` crate. - Follow the connection pooling configuration defined in `MongoPoolConfig`. - Be aware of the document structure for `TokenAnalyticsData` in `src/config/mongodb.rs`. **Memory Bank Usage:** - Consult the `memory-bank` for crucial project information: - `memory-bank/activeContext.md`: Provides the current task, action plan, and technical context. Pay close attention to "Current Issues" and "Next Steps." - `memory-bank/codeReview.md`: Highlights code review guidelines, common issues, and best practices. *This is extremely important for ensuring code quality.* - `memory-bank/developmentWorkflow.md`: Outlines the implementation plan, testing strategy, and project standards. - `memory-bank/operationalContext.md`: Describes the system's operational aspects, including error handling patterns and infrastructure requirements. - `memory-bank/productContext.md`: Explains the core problem, key components, workflows, and product direction. - `memory-bank/projectBoundaries.md`: Defines technical constraints, scale requirements, hard limitations, and non-negotiables. - `memory-bank/techContext.md`: Details the vector store implementation, including MongoDB Atlas setup, database schema, search configuration, and integration notes. - Use the information in the memory bank to guide your code generation and decision-making. **Code Style and Conventions:** - Follow Rust naming conventions (e.g., `snake_case` for variables and functions, `PascalCase` for structs and enums). - Include comprehensive documentation for functions, structs, and enums, as outlined in `memory-bank/codeReview.md`. - Use descriptive variable and function names. - Prioritize clarity and readability. **Security:** - Assume API keys and other sensitive information are stored securely (e.g., in environment variables). Do *not* hardcode secrets. - Follow best practices for secure coding in Rust. **Example (Illustrative):** If asked to "fix the vector search," you should: 1. Check `memory-bank/activeContext.md` and `memory-bank/codeReview.md` to understand the known issues (missing `fields` in `SearchParams`). 2. Examine `src/config/mongodb.rs`, specifically the `top_n` function within the `TokenAnalyticsDataExt` implementation. 3. Modify the `SearchParams` initialization to include the `fields` parameter, referencing the embedding field name ("embedding"). 4. Add error context using `.context(...)` if it's missing. 5. Explain the changes.
```

# .cursor/rules/memory-bank.mdc

```mdc
--- description: Updating the memory-bank after every major update to the codebase globs: memory-bank/* --- # Cursor's Memory Bank I am Cursor, an expert software engineer with a unique characteristic: my memory resets completely between sessions. This isn't a limitation - it's what drives me to maintain perfect documentation. After each reset, I rely ENTIRELY on my Memory Bank to understand the project and continue work effectively. I MUST read ALL memory bank files at the start of EVERY task - this is not optional. ## Memory Bank Structure The Memory Bank consists of required core files and optional context files, all in Markdown format. Files build upon each other in a clear hierarchy: \`\`\`mermaid flowchart TD PB[projectbrief.md] --> PC[productContext.md] PB --> SP[systemPatterns.md] PB --> TC[techContext.md] PC --> AC[activeContext.md] SP --> AC TC --> AC AC --> P[progress.md] \`\`\` ### Core Files (Required) 1. `projectbrief.md` - Foundation document that shapes all other files - Created at project start if it doesn't exist - Defines core requirements and goals - Source of truth for project scope 2. `productContext.md` - Why this project exists - Problems it solves - How it should work - User experience goals 3. `activeContext.md` - Current work focus - Recent changes - Next steps - Active decisions and considerations 4. `systemPatterns.md` - System architecture - Key technical decisions - Design patterns in use - Component relationships 5. `techContext.md` - Technologies used - Development setup - Technical constraints - Dependencies 6. `progress.md` - What works - What's left to build - Current status - Known issues ### Additional Context Create additional files/folders within memory-bank/ when they help organize: - Complex feature documentation - Integration specifications - API documentation - Testing strategies - Deployment procedures ## Core Workflows ### Plan Mode \`\`\`mermaid flowchart TD Start[Start] --> ReadFiles[Read Memory Bank] ReadFiles --> CheckFiles{Files Complete?} CheckFiles -->|No| Plan[Create Plan] Plan --> Document[Document in Chat] CheckFiles -->|Yes| Verify[Verify Context] Verify --> Strategy[Develop Strategy] Strategy --> Present[Present Approach] \`\`\` ### Act Mode \`\`\`mermaid flowchart TD Start[Start] --> Context[Check Memory Bank] Context --> Update[Update Documentation] Update --> Rules[Update .cursorrules if needed] Rules --> Execute[Execute Task] Execute --> Document[Document Changes] \`\`\` ## Documentation Updates Memory Bank updates occur when: 1. Discovering new project patterns 2. After implementing significant changes 3. When user requests with **update memory bank** (MUST review ALL files) 4. When context needs clarification \`\`\`mermaid flowchart TD Start[Update Process] subgraph Process P1[Review ALL Files] P2[Document Current State] P3[Clarify Next Steps] P4[Update .cursorrules] P1 --> P2 --> P3 --> P4 end Start --> Process \`\`\` Note: When triggered by **update memory bank**, I MUST review every memory bank file, even if some don't require updates. Focus particularly on activeContext.md and progress.md as they track current state. ## Project Intelligence (.cursorrules) The .cursorrules file is my learning journal for each project. It captures important patterns, preferences, and project intelligence that help me work more effectively. As I work with you and the project, I'll discover and document key insights that aren't obvious from the code alone. It should NEVER be longer than 20 lines. \`\`\`mermaid flowchart TD Start{Discover New Pattern} subgraph Learn [Learning Process] D1[Identify Pattern] D2[Validate with User] D3[Document in .cursorrules] end subgraph Apply [Usage] A1[Read .cursorrules] A2[Apply Learned Patterns] A3[Improve Future Work] end Start --> Learn Learn --> Apply \`\`\` ### What to Capture - Critical implementation paths - User preferences and workflow - Project-specific patterns - Known challenges - Evolution of project decisions - Tool usage patterns The format is flexible - focus on capturing valuable insights that help me work more effectively with you and the project. Think of .cursorrules as a living document that grows smarter as we work together. IT SHOULD NEVER BE LONGER THAN 20 LINES. REMEMBER: After every memory reset, I begin completely fresh. The Memory Bank is my only link to previous work. It must be maintained with precision and clarity, as my effectiveness depends entirely on its accuracy.
```

# .cursor/rules/solana-dev.mdc

```mdc
--- description: Default rules for building an AI Agent on Solana globs: --- You are an expert in Solana program development, focusing on building and deploying smart contracts using Rust and Anchor, and integrating on-chain data with Web3.js General Guidelines: - Prioritize writing secure, efficient, and maintainable code, following best practices for Solana program development. - Ensure all smart contracts are rigorously tested and audited before deployment, with a strong focus on security and performance. Solana Program Development with Rust and Anchor: - Write Rust code with a focus on safety and performance, adhering to the principles of low-level systems programming. - Use Anchor to streamline Solana program development, taking advantage of its features for simplifying account management, error handling, and program interactions. - Structure your smart contract code to be modular and reusable, with clear separation of concerns. - Ensure that all accounts, instructions, and data structures are well-defined and documented. Cargo.toml Best Practices: - ALWAYS check what features a crate has before updating the cargo.toml file. - Never add features that aren't available to the crate you are updating. Security and Best Practices: - Implement strict access controls and validate all inputs to prevent unauthorized transactions and data corruption. - Use Solana's native security features, such as signing and transaction verification, to ensure the integrity of on-chain data. - Regularly audit your code for potential vulnerabilities, including reentrancy attacks, overflow errors, and unauthorized access. - Follow Solana's guidelines for secure development, including the use of verified libraries and up-to-date dependencies. On-Chain Data Handling with Solana Web3.js - Use Solana Web3.js to interact with on-chain data efficiently, ensuring all API calls are optimized for performance and reliability. - Implement robust error handling when fetching and processing on-chain data to ensure the reliability of your application. Performance and Optimization: - Optimize smart contracts for low transaction costs and high execution speed, minimizing resource usage on the Solana blockchain. - Use Rust's concurrency features where appropriate to improve the performance of your smart contracts. - Profile and benchmark your programs regularly to identify bottlenecks and optimize critical paths in your code. Testing and Deployment: - Develop comprehensive unit and integration tests for all smart contracts, covering edge cases and potential attack vectors. - Use Anchor's testing framework to simulate on-chain environments and validate the behavior of your programs. - Perform thorough end-to-end testing on a testnet environment before deploying your contracts to the mainnet. - Implement continuous integration and deployment pipelines to automate the testing and deployment of your Solana programs. Documentation and Maintenance: - Document all aspects of your Solana programs in the memory-bank folder, including the architecture, data structures, and public interfaces. - Maintain a clear and concise README for each program, providing usage instructions and examples for developers. - Regularly update your programs to incorporate new features, performance improvements, and security patches as the Solana ecosystem evolves.
```

# .github/ISSUE_TEMPLATE/bug-report.md

```md
--- name: Bug report about: Create a report to help us improve title: 'bug: <title>' labels: bug assignees: '' --- - [ ] I have looked for existing issues (including closed) about this ## Bug Report <!-- A clear and concise description of what the bug is. ---> ## Reproduction <!-- Code snippet. ---> ## Expected behavior <!-- A clear and concise description of what you expected to happen. ---> ## Screenshots <!-- If applicable, add screenshots to help explain your problem. ---> ## Additional context <!-- Add any other context about the problem here. --->
```

# .github/ISSUE_TEMPLATE/feature-or-improvement-request.md

```md
--- name: Feature or improvement request about: Suggest an idea for this project title: 'feat: <title>' labels: feat assignees: '' --- - [ ] I have looked for existing issues (including closed) about this ## Feature Request <!-- High level description of the requested feature or improvement. --> ### Motivation <!-- Please describe the use case(s) or other motivation for the new feature. --> ### Proposal <!-- How should the new feature be implemented, and why? Add any considered drawbacks. --> ### Alternatives <!-- Are there other ways to solve this problem that you've considered? What are their potential drawbacks? Why was the proposed solution chosen over these alternatives? -->
```

# .github/ISSUE_TEMPLATE/new-model-provider.md

```md
--- name: New model provider about: Suggest a new model provider to integrate title: 'feat: Add support for X' labels: feat, model assignees: '' --- ## Model Provider Integration Request <!-- Describe the model provider and the models that it provides. --> ### Resources <!-- Links to API docs, SDKs or any other information that would help in the integration of the new model provider. -->
```

# .github/ISSUE_TEMPLATE/vector-store-integration-request.md

```md
--- name: Vector store integration request about: Suggest a new vector store to integrate title: 'feat: Add support for X vector store' labels: data store, feat assignees: '' --- ## Vector Store Integration Request <!-- Describe the vector store and the features it provides (e.g.: is it cloud only? a plugin to an existing database? document-based or relational? etc.) --> ### Resources <!-- Links to API docs, SDKs or any other information that would help in the integration of the new vector store. -->
```

# .github/PULL_REQUEST_TEMPLATE/new-model-provider.md

```md
--- name: New model provider about: Suggest a new model provider to integrate title: 'feat: Add support for X' labels: feat, model assignees: '' --- # <Model Provider Name> ## Description Please describe the model provider you are adding to the project. Include links to their website and their api documentation. Fixes # (issue) ## Testing Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce your results. - [ ] Test A - [ ] Test B ## Checklist - [ ] My code follows the style guidelines of this project - [ ] I have commented my code, particularly in hard-to-understand areas - [ ] I have made corresponding changes to the documentation - [ ] My changes generate no new warnings - [ ] I have added tests that prove my fix is effective or that my feature works - [ ] New and existing unit tests pass locally with my changes - [ ] I've reviewed the provider API documentation and implemented the types of response accurately ## Notes Any notes you wish to include about the nature of this PR (implementation details, specific questions, etc.)
```

# .github/PULL_REQUEST_TEMPLATE/new-vector-store.md

```md
--- name: Vector store integration request about: Suggest a new vector store to integrate title: 'feat: Add support for X vector store' labels: data store, feat assignees: '' --- # <Vector Store Name> ## Description Please describe the vector store you are adding to the project. Include links to their website and their api documentation. Fixes # (issue) ## Testing Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce your results. - [ ] Test A - [ ] Test B ## Checklist - [ ] My code follows the style guidelines of this project - [ ] I have performed a self-review of my own code - [ ] I have commented my code, particularly in hard-to-understand areas - [ ] I have made corresponding changes to the documentation - [ ] My changes generate no new warnings - [ ] I have added tests that prove my fix is effective or that my feature works - [ ] New and existing unit tests pass locally with my changes - [ ] Any dependent changes have been merged and published in downstream modules - [ ] I've reviewed the vector store API documentation and implemented the types of response accurately ## Notes Any notes you wish to include about the nature of this PR (implementation details, specific questions, etc.)
```

# .github/PULL_REQUEST_TEMPLATE/other.md

```md
--- name: General pull request about: Makes a change to the code base title: '' labels: '' assignees: '' --- # <Pull Request Title> ## Description Please include a summary of the changes and the related issue. Please also include relevant motivation and context. List any dependencies that are required for this change. Fixes # (issue) ## Type of change Please delete options that are not relevant. - [ ] Bug fix - [ ] New feature - [ ] Breaking change - [ ] Documentation update ## Testing Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce your results. - [ ] Test A - [ ] Test B ## Checklist - [ ] My code follows the style guidelines of this project - [ ] I have commented my code, particularly in hard-to-understand areas - [ ] I have made corresponding changes to the documentation - [ ] My changes generate no new warnings - [ ] I have added tests that prove my fix is effective or that my feature works - [ ] New and existing unit tests pass locally with my changes ## Notes Any notes you wish to include about the nature of this PR (implementation details, specific questions, etc.)
```

# .github/workflows/cd.yaml

```yaml
name: "Build & Release"

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  run-ci:
    permissions:
      checks: write
    uses: ./.github/workflows/ci.yaml
    secrets: inherit

  release-plz:
    name: Release-plz
    needs: run-ci
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Install Rust toolchain
        uses: actions-rust-lang/setup-rust-toolchain@v1

      # Required to compile rig-lancedb
      - name: Install Protoc
        uses: arduino/setup-protoc@v3

      - name: Run release-plz
        uses: MarcoIeni/release-plz-action@v0.5
        env:
          GITHUB_TOKEN: ${{ secrets.PAT_TOKEN }}
          CARGO_REGISTRY_TOKEN: ${{ secrets.CARGO_REGISTRY_TOKEN }}

```

# .github/workflows/ci.yaml

```yaml
name: Lint & Test

on:
  pull_request:
    branches:
      - "**"
  workflow_call:

env:
  CARGO_TERM_COLOR: always

# ensure that the workflow is only triggered once per PR, subsequent pushes to the PR will cancel
# and restart the workflow. See https://docs.github.com/en/actions/using-jobs/using-concurrency
concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  fmt:
    name: stable / fmt
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PAT_TOKEN }}

      - name: Install Rust stable
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          components: rustfmt

      - name: Run cargo fmt
        run: cargo fmt -- --check

  clippy:
    name: stable / clippy
    runs-on: ubuntu-latest
    permissions:
      checks: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PAT_TOKEN }}

      - name: Install Rust stable
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          components: clippy

      # Required to compile rig-lancedb
      - name: Install Protoc
        uses: arduino/setup-protoc@v3
        with:
          repo-token: ${{ secrets.PAT_TOKEN }}

      - name: Run clippy action
        uses: clechasseur/rs-clippy-check@v3
        with:
          args: --all-features

  test:
    name: stable / test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PAT_TOKEN }}

      - name: Install Rust stable
        uses: actions-rust-lang/setup-rust-toolchain@v1

      - name: Install nextest
        uses: taiki-e/install-action@v2
        with:
          tool: nextest

      # Required to compile rig-lancedb
      - name: Install Protoc
        uses: arduino/setup-protoc@v3
        with:
          repo-token: ${{ secrets.PAT_TOKEN }}

      - name: Test with latest nextest release
        uses: actions-rs/cargo@v1
        with:
          command: nextest
          args: run --all-features
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}
          PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}

  doc:
    name: stable / doc
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PAT_TOKEN }}

      - name: Install Rust stable
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          components: rust-docs

      # Required to compile rig-lancedb
      - name: Install Protoc
        uses: arduino/setup-protoc@v3
        with:
          repo-token: ${{ secrets.PAT_TOKEN }}

      - name: Run cargo doc
        run: cargo doc --no-deps --all-features
        env:
          RUSTDOCFLAGS: -D warnings

```

# .gitignore

```
# Environment variables .env .env.local # Build target/ # Local Artifacts .DS_Store # IDE .vscode/ .devcontainer/ # Node node_modules/ # Characters characters/ # Goose .goose/ # Examples examples/
```

# .pre-commit-config.yaml

```yaml
# See https://pre-commit.com for more information
# See https://pre-commit.com/hooks.html for more hooks
repos:
-   repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.6.0
    hooks:
    -   id: trailing-whitespace
    -   id: end-of-file-fixer
    -   id: check-yaml
    -   id: check-added-large-files
    -   id: check-json
    -   id: check-case-conflict
    -   id: check-merge-conflict


-   repo: https://github.com/doublify/pre-commit-rust
    rev: v1.0
    hooks:
    -   id: fmt
    -   id: cargo-check
    -   id: clippy

- repo: https://github.com/commitizen-tools/commitizen
  rev: v2.20.0
  hooks:
    - id: commitizen
      stages: [commit-msg]

```

# agents/trader/Cargo.toml

```toml
[package] name = "cainam-trader" version = "0.1.0" edition = "2021" [dependencies] # Framework rig-core = "0.7.0" rig-mongodb = "0.2.3" # Core dependencies tokio = "1.38.0" anyhow = "1.0" thiserror = "1.0" async-trait = "0.1" futures = "0.3" # Serialization serde = { version = "1.0", features = ["derive"] } serde_json = "1.0" # Logging tracing = "0.1" tracing-subscriber = { version = "0.3", features = ["env-filter", "chrono", "time"] } # Time handling chrono = { version = "0.4", features = ["serde"] } # Environment dotenvy = "0.15.7" # HTTP client reqwest = { version = "0.11", features = ["json"] } # Utilities uuid = { version = "1.0", features = ["v4", "serde"] } base64 = "0.21" rand = "0.8" # Solana solana-sdk = { version = "1.14.18", features = ["full"] } solana-client = "1.14.18" solana-program = "1.14.18" spl-token = "3.5.0" # Adding this dependency anchor-client = "0.26.0" anchor-lang = "0.26.0" # CLI rustyline = "12.0" # Jupiter DEX jup-ag = "0.8" # Technical Analysis ta = "0.5" # Social media integration twitter-v2 = "0.1" oauth2 = "4.4" oauth1 = "1.0" # OpenAI openai = { version = "1.0.0-alpha.18" } [dev-dependencies] tokio-test = "0.4"
```

# agents/trader/docker-compose.yml

```yml
version: "3.8"

services:
  postgres:
    image: ankane/pgvector:latest
    environment:
      POSTGRES_USER: mgunnin
      POSTGRES_PASSWORD: password
      POSTGRES_DB: cainam_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  trader:
    build: .
    environment:
      - DATABASE_URL=postgresql://mgunnin:password@postgres:5432/cainam_db
    depends_on:
      - postgres
    volumes:
      - .:/app

volumes:
  postgres_data:

```

# agents/trader/src/agents/data_ingestion.rs

```rs
use rig_core::{ agent::Agent, message_bus::{Message, MessageBus}, storage::VectorStorage, }; use rig_solana_trader::{personality::StoicPersonality, storage::MarketData}; use std::sync::Arc; pub struct DataIngestionAgent { bus: MessageBus, storage: Arc<dyn VectorStorage>, personality: Arc<StoicPersonality>, } impl DataIngestionAgent { pub fn new( bus: MessageBus, storage: Arc<dyn VectorStorage>, personality: Arc<StoicPersonality>, ) -> Self { Self { bus, storage, personality } } } #[async_trait] impl Agent for DataIngestionAgent { async fn run(&self) -> anyhow::Result<()> { let mut receiver = self.bus.subscribe("market_data"); while let Ok(msg) = receiver.recv().await { if let Message::MarketData(data) = msg { // Store raw data self.storage .insert("market_data", data.to_embedding()) .await?; // Process with personality constraints let processed = self.personality.process_market_data(data).await?; // Store processed data self.storage .insert("processed_market", processed.to_embedding()) .await?; // Publish to message bus self.bus.publish(Message::ProcessedMarketData(processed)).await; } } Ok(()) } }
```

# agents/trader/src/agents/execution.rs

```rs
use rig_core::{ agent::Agent, message_bus::{Message, MessageBus}, storage::VectorStorage, }; use rig_solana_trader::{personality::StoicPersonality, trading::TradeExecution}; use solana_sdk::signature::Signature; use std::sync::Arc; pub struct ExecutionAgent { bus: MessageBus, storage: Arc<dyn VectorStorage>, personality: Arc<StoicPersonality>, } impl ExecutionAgent { pub fn new( bus: MessageBus, storage: Arc<dyn VectorStorage>, personality: Arc<StoicPersonality>, ) -> Self { Self { bus, storage, personality } } } #[async_trait] impl Agent for ExecutionAgent { async fn run(&self) -> anyhow::Result<()> { let mut receiver = self.bus.subscribe("trade_decisions"); while let Ok(msg) = receiver.recv().await { if let Message::TradeDecision(decision) = msg { // Execute trade on Solana let sig: Signature = self.personality.execute_trade(&decision).await?; // Store execution record let execution = TradeExecution { tx_hash: sig.to_string(), mint_address: decision.mint, amount: decision.amount, risk_assessment: decision.risk_score, vector_embedding: decision.to_embedding(), timestamp: Utc::now(), }; self.storage .insert("trade_history", execution) .await?; self.bus.publish(Message::TradeExecuted(execution)).await; } } Ok(()) } }
```

# agents/trader/src/agents/mod.rs

```rs
use rig::{ agent::{Agent, AgentBuilder}, chat::{Chat, CompletionModel, Message, PromptError}, providers::openai::Client as OpenAIClient, Result, }; use serde::{Deserialize, Serialize}; use tracing::debug; use cainam_trader::market_data::{MarketData, TokenMetadata}; #[derive(Clone, Debug, Deserialize, Serialize)] pub struct MarketAnalysis { pub token: TokenMetadata, pub sentiment_score: f64, pub risk_score: f64, pub recommendation: String, pub reasoning: String, } #[derive(Clone, Debug, Deserialize, Serialize)] pub struct RiskAssessment { pub token: TokenMetadata, pub liquidity_risk: f64, pub volatility_risk: f64, pub market_risk: f64, pub overall_risk: f64, pub risk_factors: Vec<String>, } #[derive(Clone, Debug, Deserialize, Serialize)] pub struct ExecutionPlan { pub token: TokenMetadata, pub action: String, pub size: f64, pub target_price: f64, pub stop_loss: f64, pub take_profit: f64, pub reasoning: String, } pub struct TradingAgentSystem { market_analyst: Agent, risk_manager: Agent, execution_specialist: Agent, } impl TradingAgentSystem { pub async fn new(openai_client: OpenAIClient) -> Result<Self> { // Initialize market analyst agent let market_analyst = AgentBuilder::new(openai_client) .name("Market Analyst") .description("Analyzes market data and provides trading recommendations") .model(CompletionModel::GPT4) .client(openai_client.clone()) .build()?; // Initialize risk manager agent let risk_manager = AgentBuilder::new(openai_client) .name("Risk Manager") .description("Assesses trading risks and provides risk management recommendations") .model(CompletionModel::GPT4) .client(openai_client.clone()) .build()?; // Initialize execution specialist agent let execution_specialist = AgentBuilder::new(openai_client) .name("Execution Specialist") .description("Plans and optimizes trade execution") .model(CompletionModel::GPT4) .client(openai_client) .build()?; Ok(Self { market_analyst, risk_manager, execution_specialist, }) } pub async fn analyze_market(&self, market_data: &MarketData) -> Result<MarketAnalysis> { debug!("Analyzing market data for {}", market_data.token.symbol); let prompt = format!( "Analyze the following market data and provide a trading recommendation:\n\ Token: {} ({})\n\ Price: ${}\n\ 24h Volume: ${}\n\ Market Cap: ${}\n\ Social Sentiment: {}\n\ Technical Indicators:\n\ - RSI (14): {}\n\ - MACD: {}\n\ - MA50: {}\n\ - MA200: {}\n\ \n\ Provide your analysis in JSON format with the following fields:\n\ - sentiment_score: A score between 0 and 1\n\ - risk_score: A score between 0 and 1\n\ - recommendation: A brief trading recommendation\n\ - reasoning: Your detailed reasoning", market_data.token.symbol, market_data.token.address, market_data.token.price_usd.unwrap_or_default(), market_data.token.volume_24h.unwrap_or_default(), market_data.token.market_cap.unwrap_or_default(), market_data.social_sentiment.unwrap_or_default(), market_data.technical_indicators.rsi_14.unwrap_or_default(), market_data.technical_indicators.macd.unwrap_or_default(), market_data.technical_indicators.ma_50.unwrap_or_default(), market_data.technical_indicators.ma_200.unwrap_or_default(), ); let response = self.market_analyst .chat(&[Message::user(&prompt)]) .await?; let analysis: MarketAnalysis = serde_json::from_str(&response.content)?; Ok(analysis) } pub async fn assess_risk(&self, market_data: &MarketData, analysis: &MarketAnalysis) -> Result<RiskAssessment> { debug!("Assessing risk for {}", market_data.token.symbol); let prompt = format!( "Assess the trading risks for the following token based on market data and analysis:\n\ Token: {} ({})\n\ Market Analysis:\n\ - Sentiment Score: {}\n\ - Risk Score: {}\n\ - Recommendation: {}\n\ \n\ Market Data:\n\ - Price: ${}\n\ - 24h Volume: ${}\n\ - Market Cap: ${}\n\ \n\ Provide your assessment in JSON format with the following fields:\n\ - liquidity_risk: A score between 0 and 1\n\ - volatility_risk: A score between 0 and 1\n\ - market_risk: A score between 0 and 1\n\ - overall_risk: A weighted average of the above risks\n\ - risk_factors: An array of specific risk factors identified", market_data.token.symbol, market_data.token.address, analysis.sentiment_score, analysis.risk_score, analysis.recommendation, market_data.token.price_usd.unwrap_or_default(), market_data.token.volume_24h.unwrap_or_default(), market_data.token.market_cap.unwrap_or_default(), ); let response = self.risk_manager .chat(&[Message::user(&prompt)]) .await?; let assessment: RiskAssessment = serde_json::from_str(&response.content)?; Ok(assessment) } pub async fn plan_execution( &self, market_data: &MarketData, analysis: &MarketAnalysis, risk: &RiskAssessment, ) -> Result<ExecutionPlan> { debug!("Planning execution for {}", market_data.token.symbol); let prompt = format!( "Plan the execution of a trade based on the following analysis and risk assessment:\n\ Token: {} ({})\n\ Current Price: ${}\n\ \n\ Market Analysis:\n\ - Sentiment Score: {}\n\ - Risk Score: {}\n\ - Recommendation: {}\n\ \n\ Risk Assessment:\n\ - Overall Risk: {}\n\ - Risk Factors: {}\n\ \n\ Provide your execution plan in JSON format with the following fields:\n\ - action: 'BUY' or 'SELL'\n\ - size: Position size in SOL\n\ - target_price: Entry price target\n\ - stop_loss: Stop loss price\n\ - take_profit: Take profit price\n\ - reasoning: Detailed reasoning for the execution plan", market_data.token.symbol, market_data.token.address, market_data.token.price_usd.unwrap_or_default(), analysis.sentiment_score, analysis.risk_score, analysis.recommendation, risk.overall_risk, risk.risk_factors.join(", "), ); let response = self.execution_specialist .chat(&[Message::user(&prompt)]) .await?; let plan: ExecutionPlan = serde_json::from_str(&response.content)?; Ok(plan) } }
```

# agents/trader/src/agents/prediction.rs

```rs
use rig_core::{ agent::Agent, message_bus::{Message, MessageBus}, storage::VectorStorage, }; use rig_solana_trader::{personality::StoicPersonality, storage::MarketData}; use std::sync::Arc; pub struct PredictionAgent { bus: MessageBus, storage: Arc<dyn VectorStorage>, personality: Arc<StoicPersonality>, } impl PredictionAgent { pub fn new( bus: MessageBus, storage: Arc<dyn VectorStorage>, personality: Arc<StoicPersonality>, ) -> Self { Self { bus, storage, personality } } } #[async_trait] impl Agent for PredictionAgent { async fn run(&self) -> anyhow::Result<()> { let mut receiver = self.bus.subscribe("processed_market"); while let Ok(msg) = receiver.recv().await { if let Message::ProcessedMarketData(data) = msg { // Find similar historical patterns let similar = self.storage .nearest("market_data", data.to_embedding(), 5) .await?; // Generate prediction with risk constraints let prediction = self.personality .generate_prediction(data, similar) .await?; self.bus.publish(Message::Prediction(prediction)).await; } } Ok(()) } }
```

# agents/trader/src/agents/twitter.rs

```rs
use rig_core::{ agent::Agent, message_bus::{Message, MessageBus}, }; use rig_solana_trader::{personality::StoicPersonality, twitter::TwitterClient}; use std::sync::Arc; pub struct TwitterAgent { bus: MessageBus, client: TwitterClient, personality: Arc<StoicPersonality>, } impl TwitterAgent { pub fn new(bus: MessageBus, personality: Arc<StoicPersonality>) -> Self { Self { bus, client: TwitterClient::new(), personality, } } } #[async_trait] impl Agent for TwitterAgent { async fn run(&self) -> anyhow::Result<()> { let mut receiver = self.bus.subscribe("trade_executed"); while let Ok(msg) = receiver.recv().await { if let Message::TradeExecuted(execution) = msg { let tweet = self.personality .generate_trade_tweet(&execution) .await?; self.client.post_tweet(&tweet).await?; } } Ok(()) } }
```

# agents/trader/src/analysis.rs

```rs
#[derive(Debug, Clone, PartialEq)] pub enum Analysis { Buy, Sell, Hold }
```

# agents/trader/src/data_ingestion/mod.rs

```rs
use solana_client::rpc_client::RpcClient; use std::sync::Arc; use rig_core::message_bus::{MessageBus, Message}; pub struct SolanaIngestor { rpc_client: Arc<RpcClient>, message_bus: MessageBus, } impl SolanaIngestor { pub fn new(message_bus: MessageBus) -> Self { Self { rpc_client: Arc::new(RpcClient::new("https://api.mainnet-beta.solana.com")), message_bus } } pub async fn run(self) { loop { let block = self.rpc_client.get_latest_blockhash().await.unwrap(); let transactions = self.rpc_client.get_block(&block).await.unwrap(); self.message_bus.publish(Message::BlockData { block_hash: block, transactions, timestamp: Utc::now() }).await; tokio::time::sleep(Duration::from_secs(1)).await; } } } pub struct SentimentAnalyzer { llm: Arc<dyn CompletionModel>, message_bus: MessageBus, } impl SentimentAnalyzer { pub fn new(message_bus: MessageBus) -> Self { Self { llm: Arc::new(DeepSeek::new()), message_bus } } pub async fn analyze(&self, text: &str) -> f32 { let prompt = format!("Analyze sentiment of this crypto-related text. Return only a number between -1 (negative) and 1 (positive): {}", text); self.llm.complete(&prompt).await .parse() .unwrap_or(0.0) } }
```

# agents/trader/src/database/mod.rs

```rs
//! Database Module //! //! This module handles all MongoDB interactions for the trading bot. It manages: //! - Market data storage and retrieval //! - Trade history //! - Position tracking //! - Risk model persistence //! - Sentiment analysis data //! //! # Environment Variables //! Required environment variables: //! - `DATABASE_URL`: MongoDB connection string //! //! # Example //! \`\`\`no_run //! use rig_solana_trader::database::DatabaseClient; //! //! #[tokio::main] //! async fn main() -> anyhow::Result<()> { //! let client = DatabaseClient::new("mongodb://user:pass@localhost/db", "trading_db").await?; //! Ok(()) //! } //! \`\`\` use rig_mongodb::{MongoDbPool, bson::doc}; use std::sync::Arc; use anyhow::Result; use tracing::{debug, info}; use crate::config::mongodb::MongoConfig; pub mod positions; pub mod sync; /// Database client for interacting with MongoDB pub struct DatabaseClient { pool: Arc<MongoDbPool>, database: String, } impl DatabaseClient { /// Create a new database client pub async fn new(uri: &str, database: &str) -> Result<Arc<Self>> { debug!("Initializing MongoDB client"); let config = MongoConfig { uri: uri.to_string(), database: database.to_string(), ..Default::default() }; let pool = config.create_pool().await?; // Initialize collections and indexes info!("Initializing MongoDB collections and indexes..."); Self::init_collections(&pool, database).await?; info!("MongoDB client initialized successfully"); Ok(Arc::new(Self { pool, database: database.to_string(), })) } async fn init_collections(pool: &MongoDbPool, database: &str) -> Result<()> { let db = pool.database(database); // Create token states collection with timeseries db.create_collection("token_states", Some(doc! { "timeseries": { "timeField": "timestamp", "metaField": "address", "granularity": "minutes" } })).await?; // Create index for efficient queries db.collection("token_states").create_index( doc! { "address": 1, "timestamp": -1 }, None, ).await?; Ok(()) } /// Get the database pool pub fn pool(&self) -> Arc<MongoDbPool> { self.pool.clone() } /// Get the database name pub fn database(&self) -> &str { &self.database } }
```

# agents/trader/src/database/positions.rs

```rs
use anyhow::Result; use serde::{Serialize, Deserialize}; use sqlx::{Pool, Postgres}; use uuid::Uuid; use chrono::{DateTime, Utc}; use crate::strategy::{PortfolioPosition, PartialSell}; #[derive(Debug, Serialize, Deserialize)] pub struct Position { pub id: Uuid, pub token_address: String, pub entry_price: f64, pub quantity: f64, pub entry_timestamp: DateTime<Utc>, pub last_update: DateTime<Utc>, pub partial_sells: Vec<PartialSell>, pub status: PositionStatus, } #[derive(Debug, Serialize, Deserialize)] pub struct PartialSell { pub price: f64, pub quantity: f64, pub timestamp: DateTime<Utc>, } #[derive(Debug, Serialize, Deserialize)] pub enum PositionStatus { Open, Closed, PartiallyExited, } pub struct PositionsCollection { pool: Pool<Postgres>, } impl PositionsCollection { pub fn new(pool: Pool<Postgres>) -> Self { Self { pool } } pub async fn create_position(&self, position: &Position) -> Result<Uuid> { let json = serde_json::to_value(position)?; sqlx::query!( "INSERT INTO positions (id, document) VALUES ($1, $2)", position.id, json ) .execute(&self.pool) .await?; Ok(position.id) } pub async fn get_position(&self, id: Uuid) -> Result<Option<Position>> { let row = sqlx::query!( "SELECT document FROM positions WHERE id = $1", id ) .fetch_optional(&self.pool) .await?; match row { Some(row) => Ok(Some(serde_json::from_value(row.document)?)), None => Ok(None), } } pub async fn get_position_by_token(&self, token_address: &str) -> Result<Option<Position>> { let row = sqlx::query!( "SELECT document FROM positions WHERE document->>'token_address' = $1", token_address ) .fetch_optional(&self.pool) .await?; match row { Some(row) => Ok(Some(serde_json::from_value(row.document)?)), None => Ok(None), } } pub async fn update_position(&self, position: &Position) -> Result<bool> { let json = serde_json::to_value(position)?; let result = sqlx::query!( "UPDATE positions SET document = $1 WHERE id = $2", json, position.id ) .execute(&self.pool) .await?; Ok(result.rows_affected() > 0) } pub async fn add_partial_sell( &self, token_address: &str, price: f64, quantity: f64, ) -> Result<bool> { let mut position = match self.get_position_by_token(token_address).await? { Some(p) => p, None => return Ok(false), }; let partial_sell = PartialSell { price, quantity, timestamp: Utc::now(), }; position.partial_sells.push(partial_sell); position.status = PositionStatus::PartiallyExited; position.last_update = Utc::now(); self.update_position(&position).await } pub async fn close_position(&self, token_address: &str) -> Result<bool> { let mut position = match self.get_position_by_token(token_address).await? { Some(p) => p, None => return Ok(false), }; position.status = PositionStatus::Closed; position.last_update = Utc::now(); self.update_position(&position).await } pub async fn get_open_positions(&self) -> Result<Vec<Position>> { let rows = sqlx::query!( "SELECT document FROM positions WHERE document->>'status' = 'Open'" ) .fetch_all(&self.pool) .await?; let positions = rows .into_iter() .map(|row| serde_json::from_value(row.document)) .collect::<Result<Vec<Position>, _>>()?; Ok(positions) } pub async fn get_portfolio_stats(&self) -> Result<PortfolioStats> { let positions = self.get_open_positions().await?; let mut stats = PortfolioStats { total_value_sol: 0.0, total_value_usd: 0.0, total_realized_pnl_sol: 0.0, total_unrealized_pnl_sol: 0.0, position_count: positions.len(), profitable_positions: 0, }; for pos in positions { stats.total_value_sol += pos.quantity * pos.entry_price; stats.total_value_usd += pos.quantity * pos.entry_price; stats.total_realized_pnl_sol += pos.partial_sells.iter() .map(|sell| (sell.price - pos.entry_price) * sell.quantity) .sum(); stats.total_unrealized_pnl_sol += (pos.entry_price - pos.entry_price) * pos.quantity; if pos.partial_sells.iter() .map(|sell| (sell.price - pos.entry_price) * sell.quantity) .sum::<f64>() > 0.0 { stats.profitable_positions += 1; } } Ok(stats) } } #[derive(Debug, Serialize, Deserialize)] pub struct PortfolioStats { pub total_value_sol: f64, pub total_value_usd: f64, pub total_realized_pnl_sol: f64, pub total_unrealized_pnl_sol: f64, pub position_count: usize, pub profitable_positions: usize, }
```

# agents/trader/src/database/sync.rs

```rs
use crate::personality::StoicPersonality; use crate::market_data::{DataProvider, MarketTrend}; use crate::twitter::TwitterClient; use crate::strategy::{TradeAction, TradeRecommendation, TradingStrategy}; use crate::dex::jupiter::JupiterDex; use anyhow::Result; use chrono::{DateTime, Utc}; use serde::{Serialize, Deserialize}; use tracing::{debug, info, warn}; use rig::completion::CompletionModel; use solana_sdk::signature::Keypair; use uuid::Uuid; use rig_mongodb::{MongoDbPool, bson::doc}; use std::sync::Arc; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TokenState { pub address: String, pub symbol: String, pub name: String, pub price_usd: f64, pub price_sol: f64, pub volume_24h: f64, pub market_cap: f64, pub price_change_24h: f64, pub volume_change_24h: f64, pub timestamp: DateTime<Utc>, } pub struct SyncCollection { pool: Arc<MongoDbPool>, database: String, } impl SyncCollection { pub fn new(pool: Arc<MongoDbPool>, database: String) -> Self { Self { pool, database } } pub async fn save_token_state(&self, state: &TokenState) -> Result<()> { let collection = self.pool .database(&self.database) .collection("token_states"); collection.insert_one(state, None).await?; Ok(()) } pub async fn get_token_state(&self, token_address: &str) -> Result<Option<TokenState>> { let collection = self.pool .database(&self.database) .collection("token_states"); let filter = doc! { "address": token_address }; let options = rig_mongodb::options::FindOneOptions::builder() .sort(doc! { "timestamp": -1 }) .build(); collection.find_one(filter, options) .await .map_err(anyhow::Error::from) } pub async fn get_token_history( &self, token_address: &str, start_time: DateTime<Utc>, end_time: DateTime<Utc>, ) -> Result<Vec<TokenState>> { let collection = self.pool .database(&self.database) .collection("token_states"); let filter = doc! { "address": token_address, "timestamp": { "$gte": start_time, "$lte": end_time } }; let options = rig_mongodb::options::FindOptions::builder() .sort(doc! { "timestamp": -1 }) .build(); let cursor = collection.find(filter, options).await?; cursor.try_collect().await.map_err(anyhow::Error::from) } pub async fn cleanup_old_data(&self, retention_days: i64) -> Result<u64> { let cutoff = Utc::now() - chrono::Duration::days(retention_days); let collection = self.pool .database(&self.database) .collection::<TokenState>("token_states"); let filter = doc! { "timestamp": { "$lt": cutoff } }; let result = collection.delete_many(filter, None).await?; debug!("Cleaned up {} old token state records", result.deleted_count); Ok(result.deleted_count) } } pub struct DataSyncService<M: CompletionModel> { db: Arc<SyncCollection>, data_provider: Box<dyn DataProvider>, twitter: TwitterClient, trading_strategy: TradingStrategy<M>, dex: JupiterDex, personality: StoicPersonality, wallet: Keypair, sync_interval: u64, } impl<M: CompletionModel> DataSyncService<M> { pub fn new( db: SyncCollection, data_provider: Box<dyn DataProvider>, twitter: TwitterClient, trading_strategy: TradingStrategy<M>, dex: JupiterDex, wallet: Keypair, sync_interval: u64, ) -> Self { Self { db: Arc::new(db), data_provider, twitter, trading_strategy, dex, personality: StoicPersonality::new(), wallet, sync_interval, } } pub async fn start(&self) -> Result<()> { info!("Starting data sync service"); loop { if let Err(e) = self.sync_market_data().await { tracing::error!("Error syncing market data: {}", e); } tokio::time::sleep(tokio::time::Duration::from_secs(self.sync_interval)).await; } } pub async fn sync_market_data(&self) -> Result<()> { info!("Starting market data sync cycle"); // Fetch trending tokens info!("Fetching trending tokens from BirdEye"); let trends = self.data_provider.get_trending_tokens(20).await?; info!("Found {} trending tokens", trends.len()); // Insert token states and analyze trading opportunities for trend in trends { info!( "Processing token {} ({}) - Price: ${:.4}, 24h Change: {:.2}%, Volume: ${:.2}M", trend.metadata.name, trend.metadata.symbol, trend.metadata.price_usd, trend.price_change_24h, trend.metadata.volume_24h / 1_000_000.0 ); let state = self.market_trend_to_token_state(trend.clone()); info!("Inserting token state into PostgreSQL"); self.db.save_token_state(&state)?; // Format market data for LLM analysis let prompt = format!( "Analyze trading opportunity for {} ({}). Price: ${:.4}, 24h Change: {:.2}%, Volume: ${:.2}M", trend.metadata.name, trend.metadata.symbol, trend.metadata.price_usd, trend.price_change_24h, trend.metadata.volume_24h / 1_000_000.0 ); // Analyze trading opportunity info!("Analyzing trading opportunity with LLM"); if let Ok(analysis) = self.trading_strategy.analyze_trading_opportunity(&prompt, 1.0).await { // Parse the analysis into a trade recommendation if let Ok(trade) = serde_json::from_str::<TradeRecommendation>(&analysis) { info!( "Received trade recommendation: Action={:?}, Amount={} SOL, Confidence={:.2}, Risk={}", trade.action, trade.amount_in_sol, trade.confidence, trade.risk_assessment ); // Execute trade if confidence is high enough if trade.confidence >= 0.8 { match trade.action { TradeAction::Buy => { info!("Executing BUY order for {} SOL worth of {}", trade.amount_in_sol, trend.metadata.symbol); if let Ok(signature) = self.dex.execute_swap( "So11111111111111111111111111111111111111112", // SOL &trade.token_address, trade.amount_in_sol as u64, &self.wallet, ).await { info!("Trade executed successfully. Signature: {}", signature); // Generate and post tweet about the trade info!("Generating tweet for successful buy"); let tweet = self.personality.generate_trade_tweet( &format!( "Action: Buy\nAmount: {} SOL\nToken: {}\nPrice: ${:.4}\nMarket Cap: ${:.2}M\n24h Volume: ${:.2}M\n24h Change: {:.2}%\nContract: {}\nTransaction: {}\nAnalysis: {}\nRisk Assessment: {}\nMarket Analysis:\n- Volume: {}\n- Price Trend: {}\n- Liquidity: {}\n- Momentum: {}", trade.amount_in_sol, trend.metadata.symbol, trend.metadata.price_usd, trend.metadata.market_cap / 1_000_000.0, trend.metadata.volume_24h / 1_000_000.0, trend.price_change_24h, trend.token_address, signature, trade.reasoning, trade.risk_assessment, trade.market_analysis.volume_analysis, trade.market_analysis.price_trend, trade.market_analysis.liquidity_assessment, trade.market_analysis.momentum_indicators ), "buy", trade.confidence, ).await?; info!("Posting tweet: {}", tweet); if let Err(e) = self.twitter.post_tweet(&tweet).await { warn!("Failed to post trade tweet: {}", e); } } else { warn!("Failed to execute buy order"); } }, TradeAction::Sell => { info!("Executing SELL order for {} SOL worth of {}", trade.amount_in_sol, trend.metadata.symbol); if let Ok(signature) = self.dex.execute_swap( &trade.token_address, "So11111111111111111111111111111111111111112", // SOL trade.amount_in_sol as u64, &self.wallet, ).await { info!("Trade executed successfully. Signature: {}", signature); // Generate and post tweet about the trade info!("Generating tweet for successful sell"); let tweet = self.personality.generate_trade_tweet( &format!( "Action: Sell\nAmount: {} SOL\nToken: {}\nPrice: ${:.4}\nMarket Cap: ${:.2}M\n24h Volume: ${:.2}M\n24h Change: {:.2}%\nContract: {}\nTransaction: {}\nAnalysis: {}\nRisk Assessment: {}\nMarket Analysis:\n- Volume: {}\n- Price Trend: {}\n- Liquidity: {}\n- Momentum: {}", trade.amount_in_sol, trend.metadata.symbol, trend.metadata.price_usd, trend.metadata.market_cap / 1_000_000.0, trend.metadata.volume_24h / 1_000_000.0, trend.price_change_24h, trend.token_address, signature, trade.reasoning, trade.risk_assessment, trade.market_analysis.volume_analysis, trade.market_analysis.price_trend, trade.market_analysis.liquidity_assessment, trade.market_analysis.momentum_indicators ), "sell", trade.confidence, ).await?; info!("Posting tweet: {}", tweet); if let Err(e) = self.twitter.post_tweet(&tweet).await { warn!("Failed to post trade tweet: {}", e); } } else { warn!("Failed to execute sell order"); } }, TradeAction::Hold => { info!("Decision: HOLD {} - {}", trend.metadata.symbol, trade.reasoning); } } } else { info!("Skipping trade due to low confidence: {:.2}", trade.confidence); } } else { warn!("Failed to parse trade recommendation"); } } else { warn!("Failed to get trading analysis from LLM"); } } info!("Market data sync cycle complete"); Ok(()) } fn market_trend_to_token_state(&self, trend: MarketTrend) -> TokenState { TokenState { address: trend.token_address, symbol: trend.metadata.symbol, name: trend.metadata.name, price_sol: trend.metadata.price_sol, price_usd: trend.metadata.price_usd, market_cap: trend.metadata.market_cap, volume_24h: trend.metadata.volume_24h, price_change_24h: trend.price_change_24h, volume_change_24h: 0.0, // Placeholder, update as needed timestamp: Utc::now(), } } }
```

# agents/trader/src/decision/mod.rs

```rs
use rig_core::message_bus::{MessageBus, Message}; use rig_solana_trader::personality::StoicPersonality; pub struct PPODecisionAgent { message_bus: MessageBus, policy_network: PolicyNetwork, personality: StoicPersonality, } impl PPODecisionAgent { pub fn new(message_bus: MessageBus) -> Self { Self { message_bus, policy_network: PolicyNetwork::new(), personality: StoicPersonality::new() } } async fn decide_action(&mut self, state: &State) -> Action { // Combine LLM analysis with PPO let llm_analysis = self.personality.analyze_state(state).await; let ppo_action = self.policy_network.forward(state); // Risk management if state.risk_level > self.personality.risk_tolerance { return Action::Hold; } // Combine signals match (llm_analysis, ppo_action) { (Analysis::Buy, Action::Buy) => Action::Buy, (Analysis::Sell, Action::Sell) => Action::Sell, _ => Action::Hold } } }
```

# agents/trader/src/dex/jupiter.rs

```rs
use anyhow::Result; use base64::{Engine as _, engine::general_purpose::STANDARD}; use reqwest::Client; use serde::{Deserialize, Serialize}; use solana_client::rpc_client::RpcClient; use solana_sdk::{ signature::{Keypair, Signer}, transaction::Transaction, }; #[derive(Debug, Deserialize, Serialize)] pub struct QuoteResponse { pub data: QuoteData, } #[derive(Debug, Deserialize, Serialize)] pub struct QuoteData { pub in_amount: String, pub out_amount: String, pub price_impact: f64, pub minimum_out_amount: String, } #[derive(Debug, Deserialize)] pub struct SwapResponse { pub data: SwapData, } #[derive(Debug, Deserialize)] pub struct SwapData { pub transaction: String, } pub struct JupiterDex { client: Client, rpc_client: RpcClient, api_key: String, slippage: f64, } impl JupiterDex { pub fn new(rpc_url: &str, api_key: String, slippage: f64) -> Self { Self { client: Client::new(), rpc_client: RpcClient::new(rpc_url.to_string()), api_key, slippage, } } pub async fn get_quote(&self, input_mint: &str, output_mint: &str, amount: u64) -> Result<QuoteResponse> { let url = format!( "https://price.jup.ag/v4/quote?inputMint={}&outputMint={}&amount={}&slippageBps={}", input_mint, output_mint, amount, (self.slippage * 100.0) as u32 ); let response = self.client .get(&url) .header("X-API-KEY", &self.api_key) .send() .await? .json::<QuoteResponse>() .await?; Ok(response) } pub async fn execute_swap( &self, input_mint: &str, output_mint: &str, amount: u64, wallet: &Keypair, ) -> Result<String> { // Get quote first let quote = self.get_quote(input_mint, output_mint, amount).await?; // Get swap transaction let url = "https://quote-api.jup.ag/v4/swap"; let swap_request = serde_json::json!({ "quoteResponse": quote, "userPublicKey": wallet.pubkey().to_string(), "wrapUnwrapSOL": true }); let response = self.client .post(url) .header("X-API-KEY", &self.api_key) .json(&swap_request) .send() .await? .json::<SwapResponse>() .await?; // Decode and sign transaction let transaction_data = STANDARD.decode(response.data.transaction)?; let mut transaction: Transaction = bincode::deserialize(&transaction_data)?; transaction.sign(&[wallet], self.rpc_client.get_latest_blockhash()?); // Send transaction let signature = self.rpc_client.send_transaction(&transaction)?; Ok(signature.to_string()) } pub async fn check_token_tradable(&self, token_address: &str) -> Result<bool> { // Try to get quotes in both directions (token -> SOL and SOL -> token) let sol_mint = "So11111111111111111111111111111111111111112"; let amount = 1_000_000; // 1 SOL in lamports let to_token = self.get_quote(sol_mint, token_address, amount).await; let from_token = self.get_quote(token_address, sol_mint, amount).await; Ok(to_token.is_ok() && from_token.is_ok()) } }
```

# agents/trader/src/dex/mod.rs

```rs
pub mod jupiter; pub use jupiter::JupiterDex;
```

# agents/trader/src/execution/mod.rs

```rs
use solana_sdk::{ signature::{Keypair, Signature}, transaction::Transaction, }; use anchor_lang::prelude::*; use anyhow::Result; use rig_core::message_bus::MessageBus; use std::sync::Arc; #[derive(Debug, Clone)] pub struct TradeParams { pub mint: String, pub amount: f64, pub slippage: u8, pub units: u64, } pub struct SolanaExecutor { keypair: Arc<Keypair>, message_bus: MessageBus, risk_threshold: f64, } impl SolanaExecutor { pub fn new(keypair: Arc<Keypair>, message_bus: MessageBus) -> Self { Self { keypair, message_bus, risk_threshold: 0.2, } } pub async fn execute_trade(&self, action: TradeAction) -> Result<Signature> { let program = anchor_spl::token::ID; let accounts = self.build_accounts(&action.params.mint); let tx = Transaction::new_signed_with_payer( &[Instruction::new_with_bytes( program, &action.encode(), accounts, )], Some(&self.keypair.pubkey()), &[&self.keypair], Hash::default(), ); self.validate_risk(&action).await?; self.message_bus .publish(TradeEvent::new(action.clone())) .await; self.message_bus.rpc_client.send_transaction(&tx).await } async fn validate_risk(&self, action: &TradeAction) -> Result<()> { let position_size = match action.action_type { TradeType::Buy => action.params.amount, TradeType::Sell => -action.params.amount, }; if position_size.abs() > self.risk_threshold { return Err(anyhow::anyhow!( "Position size {} exceeds risk threshold {}", position_size, self.risk_threshold )); } Ok(()) } fn build_accounts(&self, mint: &str) -> Vec<AccountMeta> { // Implementation depends on your specific program accounts vec![] } } #[derive(Debug, Clone, Serialize, Deserialize)] pub enum TradeType { Buy, Sell, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TradeAction { pub action_type: TradeType, pub params: TradeParams, pub analysis: Option<TradeAnalysis>, } impl TradeAction { pub fn encode(&self) -> Vec<u8> { // Implementation depends on your program's instruction format vec![] } } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TradeAnalysis { pub market_cap: f64, pub volume_ratio: f64, pub risk_assessment: f64, }
```

# agents/trader/src/integrations/twitter.rs

```rs
use oauth1::Token; use reqwest::Client; use rig_solana_trader::personality::StoicPersonality; pub struct TwitterClient { client: Client, personality: StoicPersonality, } impl TwitterClient { pub fn new(personality: StoicPersonality) -> Self { Self { client: Client::new(), personality, } } pub async fn post_trade(&self, action: &TradeAction, tx_hash: &str) -> Result<()> { let tweet = self.personality .generate_trade_tweet(action, tx_hash) .await?; let token = Token::new( &std::env::var("TWITTER_API_KEY")?, &std::env::var("TWITTER_API_SECRET")?, ); let access = Token::new( &std::env::var("TWITTER_ACCESS_TOKEN")?, &std::env::var("TWITTER_ACCESS_SECRET")?, ); let auth_header = oauth1::authorize("POST", "https://api.twitter.com/2/tweets", &token, Some(&access), None); self.client .post("https://api.twitter.com/2/tweets") .header("Authorization", auth_header) .json(&serde_json::json!({ "text": tweet })) .send() .await?; Ok(()) } }
```

# agents/trader/src/lib.rs

```rs
//! Solana Trading Bot //! //! This crate provides a framework for building automated trading bots on Solana. //! It includes: //! //! - Market data collection and analysis //! - Trading strategy implementation //! - Risk management //! - Trade execution via Jupiter //! - Twitter integration for trade announcements //! - PostgreSQL persistence for market data and positions //! //! # Architecture //! //! The bot is organized into several key modules: //! //! - `market_data`: Handles market data collection and analysis //! - `strategy`: Implements trading strategies //! - `execution`: Manages trade execution //! - `database`: Handles PostgreSQL persistence //! - `twitter`: Twitter API integration //! //! # Example Usage //! //! \`\`\`no_run //! use rig_solana_trader::{TradingBot, Config}; //! use std::env; //! //! #[tokio::main] //! async fn main() -> anyhow::Result<()> { //! // Load configuration //! let config = Config::from_env()?; //! //! // Create and start bot //! let mut bot = TradingBot::new(config).await?; //! bot.run().await?; //! //! Ok(()) //! } //! \`\`\` use rig_core::{ agent::{Agent, AgentSystem}, message_bus::MessageBus, }; use rig_postgres::PostgresVectorStore; use sqlx::postgres::PgPoolOptions; use std::sync::Arc; use std::env; use std::time::Duration; use tracing::{debug, info}; pub mod agents; pub mod analysis; pub mod database; pub mod decision; pub mod dex; pub mod execution; pub mod integrations; pub mod market_data; pub mod personality; pub mod prediction; pub mod state; pub mod storage; pub mod strategy; pub mod twitter; pub mod wallet; /// Initialize the trading bot with the given configuration pub async fn init_bot( database_url: &str, openai_api_key: &str, twitter_api_key: &str, ) -> anyhow::Result<()> { // Initialize PostgreSQL connection let pool = PgPoolOptions::new() .max_connections(50) .idle_timeout(Duration::from_secs(5)) .connect(database_url) .await .map_err(|e| { debug!("PostgreSQL connection error: {:?}", e); anyhow::anyhow!("Failed to connect to PostgreSQL") })?; info!("PostgreSQL connection established"); // Initialize OpenAI client for embeddings let openai_client = rig_core::providers::openai::Client::from_env(); let model = openai_client.embedding_model(rig_core::providers::openai::TEXT_EMBEDDING_3_SMALL); // Initialize vector store let vector_store = PostgresVectorStore::with_defaults(model, pool); // Initialize message bus let message_bus = MessageBus::new(); // Initialize personality let personality = Arc::new(personality::StoicPersonality::new()); // Create agent system let mut agent_system = AgentSystem::new() .with_retry_policy(3, Duration::from_secs(10)) .with_health_check_interval(Duration::from_secs(30)); // Add agents agent_system .add_agent(agents::DataIngestionAgent::new( message_bus.clone(), vector_store.clone(), personality.clone(), )) .add_agent(agents::PredictionAgent::new( message_bus.clone(), vector_store.clone(), personality.clone(), )) .add_agent(agents::DecisionAgent::new( message_bus.clone(), vector_store.clone(), personality.clone(), )) .add_agent(agents::ExecutionAgent::new( message_bus.clone(), vector_store.clone(), personality.clone(), )) .add_agent(agents::TwitterAgent::new( message_bus.clone(), personality.clone(), )); // Start all agents agent_system.run().await?; Ok(()) } /// Example usage pub async fn example() -> anyhow::Result<()> { let database_url = env::var("DATABASE_URL").expect("DATABASE_URL not set"); let openai_api_key = env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY not set"); let twitter_api_key = env::var("TWITTER_API_KEY").expect("TWITTER_API_KEY not set"); init_bot( &database_url, &openai_api_key, &twitter_api_key, ).await?; Ok(()) }
```

# agents/trader/src/main.rs

```rs
use rig_core::{ agent::{Agent, AgentSystem}, message_bus::MessageBus, }; use rig_postgres::PostgresVectorStore; use sqlx::postgres::PgPoolOptions; use rig_solana_trader::{ agents::{DataIngestionAgent, DecisionAgent, ExecutionAgent, PredictionAgent, TwitterAgent}, personality::StoicPersonality, }; use std::sync::Arc; use std::env; use std::time::Duration; mod data_ingestion; mod prediction; mod decision; mod execution; mod feedback; #[tokio::main] async fn main() -> anyhow::Result<()> { // Initialize shared components let message_bus = MessageBus::new(); let personality = Arc::new(StoicPersonality::new()); // Configure PostgreSQL connection pool let database_url = env::var("DATABASE_URL")?; let pool = PgPoolOptions::new() .max_connections(50) .idle_timeout(Duration::from_secs(5)) .connect(&database_url) .await?; // Initialize OpenAI client for embeddings let openai_client = rig_core::providers::openai::Client::from_env(); let model = openai_client.embedding_model(rig_core::providers::openai::TEXT_EMBEDDING_3_SMALL); // Initialize PostgreSQL vector store let vector_store = PostgresVectorStore::with_defaults(model, pool); // Create agent system let mut agent_system = AgentSystem::new() .with_retry_policy(3, Duration::from_secs(10)) .with_health_check_interval(Duration::from_secs(30)); // Add agents with their dependencies agent_system .add_agent(DataIngestionAgent::new( message_bus.clone(), vector_store.clone(), personality.clone(), )) .add_agent(PredictionAgent::new( message_bus.clone(), vector_store.clone(), personality.clone(), )) .add_agent(DecisionAgent::new( message_bus.clone(), vector_store.clone(), personality.clone(), )) .add_agent(ExecutionAgent::new( message_bus.clone(), vector_store.clone(), personality.clone(), )) .add_agent(TwitterAgent::new( message_bus.clone(), personality.clone(), )); // Start all agents agent_system.run().await?; Ok(()) } async fn trading_loop( executor: Arc<SolanaExecutor>, risk_manager: Arc<RiskManager>, twitter: Arc<TwitterClient>, ) -> Result<()> { let market_client = MarketDataClient::new(env::var("PUMPFUN_API_KEY")?); loop { let token_data = market_client.get_token_data("TOKEN_MINT").await?; let analysis = TradeAnalysis { market_cap: token_data.current_market_cap, volume_ratio: token_data.buy_volume_4h / token_data.sell_volume_4h, risk_assessment: market_client.analyze_market(&token_data), }; let action = TradeAction { action_type: TradeType::Buy, params: TradeParams { mint: "TOKEN_MINT".into(), amount: 0.1, slippage: 10, units: 1_000_000, }, analysis: Some(analysis), }; risk_manager.validate_trade(&action)?; let signature = executor.execute_trade(action.clone()).await?; twitter.post_trade(&action, &signature.to_string()).await?; tokio::time::sleep(Duration::from_secs(300)).await; } }
```

# agents/trader/src/market_data.rs

```rs
#[derive(Debug, Clone)] pub struct MarketData { pub market_cap: f64, pub volatility: f64, pub volume_24h: f64, pub price: f64, } #[derive(Debug, Clone)] pub struct MarketContext { pub market_trend: String, pub sector_performance: f64, pub sentiment_score: f64, }
```

# agents/trader/src/market_data/birdeye.rs

```rs
//! BirdEye API Integration //! //! This module implements the BirdEye API client for fetching Solana token data. //! BirdEye provides comprehensive market data including: //! - Token metadata and prices //! - Trading volume and liquidity //! - Price changes and market trends //! //! # Rate Limits //! BirdEye API has the following limits: //! - 10 requests per second //! - 100,000 requests per day //! - 100 tokens per request for trending endpoints //! //! # Error Handling //! The implementation includes: //! - Automatic retry on rate limit errors (429) //! - Exponential backoff for failed requests //! - Detailed error logging for debugging //! //! # Configuration //! Required environment variables: //! - `BIRDEYE_API_KEY`: API key from BirdEye //! //! # Endpoints //! - GET /token/meta: Token metadata //! - GET /token/list: Token listings //! - GET /token/trending: Trending tokens //! - GET /token/price: Real-time prices use crate::market_data::{ DataProvider, MarketTrend, OnChainMetrics, PricePoint, SocialMetrics, TokenMetadata, }; use anyhow::Result; use async_trait::async_trait; use chrono::DateTime; use reqwest::Client; use serde::Deserialize; use std::collections::HashMap; use tracing::{debug, info, instrument}; #[derive(Debug, Deserialize)] struct BirdEyeTokenResponse { data: BirdEyeTokenData, success: bool, } #[derive(Debug, Deserialize)] struct BirdEyeTokenData { address: String, symbol: String, name: String, price: f64, volume_24h: f64, decimals: u8, price_sol: f64, market_cap: f64, fully_diluted_market_cap: Option<f64>, circulating_supply: Option<f64>, total_supply: Option<f64>, price_change_24h: Option<f64>, volume_change_24h: Option<f64>, } #[derive(Debug, Deserialize)] struct BirdEyeTrendingResponse { data: BirdEyeTrendingResponseData, success: bool, } #[derive(Debug, Deserialize)] struct BirdEyeTrendingResponseData { #[serde(rename = "updateUnixTime")] update_unix_time: i64, #[serde(rename = "updateTime")] update_time: String, tokens: Vec<BirdEyeTrendingToken>, total: i64, } #[derive(Debug, Deserialize)] struct BirdEyeTrendingToken { address: String, decimals: u8, liquidity: f64, #[serde(rename = "logoURI")] logo_uri: Option<String>, name: String, symbol: String, #[serde(rename = "volume_24hUSD")] volume_24h_usd: Option<f64>, rank: Option<i64>, price: f64, #[serde(rename = "priceChange24h")] price_change_24h: Option<f64>, } #[derive(Debug, Deserialize)] struct BirdEyeNewListingResponse { success: bool, data: BirdEyeNewListingData, } #[derive(Debug, Deserialize)] struct BirdEyeNewListingData { items: Vec<BirdEyeNewListingToken>, } #[derive(Debug, Deserialize)] struct BirdEyeNewListingToken { address: String, symbol: String, name: String, decimals: u8, source: String, #[serde(rename = "liquidityAddedAt")] liquidity_added_at: String, #[serde(rename = "logoURI")] logo_uri: Option<String>, liquidity: f64, } #[derive(Debug, Deserialize)] struct BirdEyeTokenListResponse { success: bool, data: BirdEyeTokenListData, } #[derive(Debug, Deserialize)] struct BirdEyeTokenListData { #[serde(rename = "updateUnixTime")] update_unix_time: i64, #[serde(rename = "updateTime")] update_time: String, tokens: Vec<BirdEyeTokenListToken>, total: i64, } #[derive(Debug, Deserialize)] struct BirdEyeTokenListToken { address: String, decimals: u8, #[serde(rename = "lastTradeUnixTime")] last_trade_unix_time: i64, liquidity: f64, #[serde(rename = "logoURI")] logo_uri: Option<String>, mc: f64, name: String, symbol: String, #[serde(rename = "v24hChangePercent")] v24h_change_percent: f64, #[serde(rename = "v24hUSD")] v24h_usd: f64, } #[derive(Debug, Deserialize)] struct BirdEyeWalletResponse { success: bool, data: BirdEyeWalletData, } #[derive(Debug, Deserialize)] struct BirdEyeWalletData { wallet: String, #[serde(rename = "totalUsd")] total_usd: f64, items: Vec<BirdEyeWalletToken>, } #[derive(Debug, Deserialize)] struct BirdEyeWalletToken { address: String, decimals: u8, balance: i64, #[serde(rename = "uiAmount")] ui_amount: f64, #[serde(rename = "chainId")] chain_id: String, name: String, symbol: String, icon: Option<String>, #[serde(rename = "logoURI")] logo_uri: Option<String>, #[serde(rename = "priceUsd")] price_usd: f64, #[serde(rename = "valueUsd")] value_usd: f64, } #[derive(Debug, Deserialize, Clone)] struct BirdEyeTransactionResponse { success: bool, data: HashMap<String, Vec<BirdEyeTransaction>>, } #[derive(Debug, Deserialize, Clone)] struct BirdEyeTransaction { #[serde(rename = "txHash")] tx_hash: String, #[serde(rename = "blockNumber")] block_number: i64, #[serde(rename = "blockTime")] block_time: String, status: bool, from: String, to: String, fee: i64, #[serde(rename = "mainAction")] main_action: String, #[serde(rename = "balanceChange")] balance_change: Vec<BirdEyeBalanceChange>, #[serde(rename = "contractLabel")] contract_label: Option<BirdEyeContractLabel>, } #[derive(Debug, Deserialize, Clone)] struct BirdEyeBalanceChange { amount: f64, symbol: String, name: String, decimals: u8, address: String, #[serde(rename = "logoURI")] logo_uri: Option<String>, token_account: Option<String>, owner: Option<String>, #[serde(rename = "programId")] program_id: Option<String>, } #[derive(Debug, Deserialize, Clone)] struct BirdEyeContractLabel { address: String, name: String, metadata: BirdEyeContractMetadata, } #[derive(Debug, Deserialize, Clone)] struct BirdEyeContractMetadata { icon: String, } #[derive(Debug, Deserialize)] struct BirdEyeTokenMetadataResponse { data: HashMap<String, BirdEyeTokenMetadata>, success: bool, } #[derive(Debug, Deserialize)] struct BirdEyeTokenMetadata { address: String, name: String, symbol: String, decimals: u8, extensions: BirdEyeTokenExtensions, #[serde(rename = "logo_uri")] logo_uri: Option<String>, } #[derive(Debug, Deserialize)] struct BirdEyeTokenExtensions { #[serde(rename = "coingecko_id")] coingecko_id: Option<String>, #[serde(rename = "serum_v3_usdc")] serum_v3_usdc: Option<String>, #[serde(rename = "serum_v3_usdt")] serum_v3_usdt: Option<String>, website: Option<String>, telegram: Option<String>, twitter: Option<String>, description: Option<String>, discord: Option<String>, medium: Option<String>, } #[derive(Debug, Deserialize)] struct BirdEyeMarketDataResponse { data: BirdEyeMarketData, success: bool, } #[derive(Debug, Deserialize)] struct BirdEyeMarketData { address: String, price: f64, liquidity: f64, supply: f64, marketcap: f64, #[serde(rename = "circulating_supply")] circulating_supply: f64, #[serde(rename = "circulating_marketcap")] circulating_marketcap: f64, } #[derive(Debug)] pub struct BirdEyeProvider { api_key: String, client: Client, } impl BirdEyeProvider { pub fn new(api_key: String) -> Self { info!("Initializing BirdEye API provider"); Self { api_key, client: Client::new(), } } #[instrument(skip(self), fields(api = "birdeye"))] async fn get_trending_by_rank(&self) -> Result<Vec<MarketTrend>> { debug!("Fetching trending tokens by rank"); let url = "https://public-api.birdeye.so/defi/token_trending?sort_by=rank&sort_type=asc&offset=0&limit=20"; self.get_trending_tokens_internal(url).await } #[instrument(skip(self), fields(api = "birdeye"))] async fn get_trending_by_volume(&self) -> Result<Vec<MarketTrend>> { debug!("Fetching trending tokens by volume"); let url = "https://public-api.birdeye.so/defi/token_trending?sort_by=volume_24hUSD&sort_type=asc&offset=0&limit=20"; self.get_trending_tokens_internal(url).await } #[instrument(skip(self), fields(api = "birdeye"))] async fn get_trending_by_liquidity(&self) -> Result<Vec<MarketTrend>> { debug!("Fetching trending tokens by liquidity"); let url = "https://public-api.birdeye.so/defi/token_trending?sort_by=liquidity&sort_type=asc&offset=0&limit=20"; self.get_trending_tokens_internal(url).await } async fn get_new_listings(&self, limit: usize) -> Result<Vec<MarketTrend>> { let url = format!( "https://public-api.birdeye.so/defi/v2/tokens/new_listing?time_to=10000000000&limit={}&meme_platform_enabled=true", limit ); let response = self .client .get(&url) .header("X-API-KEY", &self.api_key) .send() .await? .json::<BirdEyeNewListingResponse>() .await?; Ok(response .data .items .into_iter() .map(|token| MarketTrend { token_address: token.address.clone(), metadata: TokenMetadata { address: token.address, symbol: token.symbol, name: token.name, decimals: token.decimals, price_usd: 0.0, // Not available in new listings price_sol: 0.0, volume_24h: 0.0, market_cap: 0.0, fully_diluted_market_cap: 0.0, circulating_supply: 0.0, total_supply: 0.0, }, price_change_24h: 0.0, volume_change_24h: 0.0, social_volume_24h: 0, dev_activity_24h: 0, }) .collect()) } async fn get_token_list_by_volume( &self, _limit: usize, _min_liquidity: f64, ) -> Result<Vec<MarketTrend>> { let url = "https://public-api.birdeye.so/defi/tokenlist?sort_by=v24hUSD&sort_type=desc&offset=0&limit=50&min_liquidity=100"; self.get_token_list_internal(url).await } async fn get_token_list_by_market_cap( &self, _limit: usize, _min_liquidity: f64, ) -> Result<Vec<MarketTrend>> { let url = "https://public-api.birdeye.so/defi/tokenlist?sort_by=mc&sort_type=desc&offset=0&limit=50&min_liquidity=100"; self.get_token_list_internal(url).await } async fn get_token_list_by_price_change( &self, _limit: usize, _min_liquidity: f64, ) -> Result<Vec<MarketTrend>> { let url = "https://public-api.birdeye.so/defi/tokenlist?sort_by=v24hChangePercent&sort_type=desc&offset=0&limit=50&min_liquidity=100"; self.get_token_list_internal(url).await } async fn get_token_list_internal(&self, url: &str) -> Result<Vec<MarketTrend>> { let response = self .client .get(url) .header("X-API-KEY", &self.api_key) .send() .await? .json::<BirdEyeTokenListResponse>() .await?; Ok(response .data .tokens .into_iter() .map(|token| MarketTrend { token_address: token.address.clone(), metadata: TokenMetadata { address: token.address, symbol: token.symbol, name: token.name, decimals: token.decimals, price_usd: 0.0, // Need to fetch separately price_sol: 0.0, volume_24h: token.v24h_usd, market_cap: token.mc, fully_diluted_market_cap: 0.0, circulating_supply: 0.0, total_supply: 0.0, }, price_change_24h: token.v24h_change_percent, volume_change_24h: 0.0, social_volume_24h: 0, dev_activity_24h: 0, }) .collect()) } async fn get_wallet_tokens(&self, wallet_address: &str) -> Result<BirdEyeWalletData> { let url = format!( "https://public-api.birdeye.so/v1/wallet/token_list?wallet={}", wallet_address ); let response = self .client .get(&url) .header("X-API-KEY", &self.api_key) .send() .await? .json::<BirdEyeWalletResponse>() .await?; Ok(response.data) } async fn get_wallet_transactions( &self, wallet_address: &str, limit: usize, ) -> Result<Vec<BirdEyeTransaction>> { let url = format!( "https://public-api.birdeye.so/v1/wallet/tx_list?wallet={}&limit={}", wallet_address, limit ); let response = self .client .get(&url) .header("X-API-KEY", &self.api_key) .send() .await? .json::<BirdEyeTransactionResponse>() .await?; Ok(response.data.get("solana").cloned().unwrap_or_default()) } #[instrument(skip(self), fields(api = "birdeye"))] async fn get_trending_tokens_internal(&self, url: &str) -> Result<Vec<MarketTrend>> { debug!(url = %url, "Making API request"); let response = self .client .get(url) .header("X-API-KEY", &self.api_key) .send() .await? .json::<BirdEyeTrendingResponse>() .await?; info!( token_count = response.data.tokens.len(), "Successfully parsed trending tokens" ); Ok(response .data .tokens .into_iter() .map(|token| MarketTrend { token_address: token.address.clone(), metadata: TokenMetadata { address: token.address, symbol: token.symbol, name: token.name, decimals: token.decimals, price_usd: token.price, price_sol: token.price, // Price is in USD volume_24h: token.volume_24h_usd.unwrap_or(0.0), market_cap: 0.0, // Not available in trending response fully_diluted_market_cap: 0.0, circulating_supply: 0.0, total_supply: 0.0, }, price_change_24h: token.price_change_24h.unwrap_or(0.0), volume_change_24h: 0.0, // Not available in trending response social_volume_24h: 0, dev_activity_24h: 0, }) .collect()) } } #[async_trait] impl DataProvider for BirdEyeProvider { async fn get_token_metadata(&self, token_address: &str) -> Result<TokenMetadata> { let url = format!( "https://public-api.birdeye.so/defi/v3/token/meta-data/multiple?list_address={}", token_address ); let response = self .client .get(&url) .header("X-API-KEY", &self.api_key) .send() .await? .json::<BirdEyeTokenMetadataResponse>() .await?; let metadata = response .data .get(token_address) .ok_or_else(|| anyhow::anyhow!("Token metadata not found"))?; // Get market data let market_url = format!( "https://public-api.birdeye.so/defi/v3/token/market-data?address={}", token_address ); let market_data = self .client .get(&market_url) .header("X-API-KEY", &self.api_key) .send() .await? .json::<BirdEyeMarketDataResponse>() .await?; Ok(TokenMetadata { address: metadata.address.clone(), symbol: metadata.symbol.clone(), name: metadata.name.clone(), decimals: metadata.decimals, price_usd: market_data.data.price, price_sol: market_data.data.price, // Price is in USD volume_24h: 0.0, // Not available in this endpoint market_cap: market_data.data.marketcap, fully_diluted_market_cap: market_data.data.marketcap, circulating_supply: market_data.data.circulating_supply, total_supply: market_data.data.supply, }) } #[instrument(skip(self), fields(api = "birdeye"))] async fn get_trending_tokens(&self, _limit: usize) -> Result<Vec<MarketTrend>> { debug!("Fetching trending tokens from all sources"); let mut all_trends = Vec::new(); // Collect trends from all sorting methods if let Ok(mut trends) = self.get_trending_by_rank().await { debug!(count = trends.len(), "Got trending by rank"); all_trends.append(&mut trends); } if let Ok(mut trends) = self.get_trending_by_volume().await { debug!(count = trends.len(), "Got trending by volume"); all_trends.append(&mut trends); } if let Ok(mut trends) = self.get_trending_by_liquidity().await { debug!(count = trends.len(), "Got trending by liquidity"); all_trends.append(&mut trends); } // Deduplicate by token address let mut unique_trends = HashMap::new(); for trend in all_trends { unique_trends .entry(trend.token_address.clone()) .or_insert(trend); } let trends: Vec<_> = unique_trends.into_values().collect(); info!( total_trends = trends.len(), "Successfully aggregated trending tokens" ); Ok(trends) } async fn get_historical_prices(&self, address: &str) -> Result<Vec<PricePoint>> { let url = format!( "https://public-api.birdeye.so/public/price_history?address={}&type=hour&limit=168", address ); let response = self .client .get(&url) .header("X-API-KEY", &self.api_key) .send() .await? .json::<serde_json::Value>() .await?; let data = response["data"] .as_array() .ok_or_else(|| anyhow::anyhow!("Invalid response format"))?; let prices: Vec<PricePoint> = data .iter() .filter_map(|point| { let timestamp = point["timestamp"].as_i64()?; let price = point["value"].as_f64()?; let volume = point["volume"].as_f64().unwrap_or(0.0); Some(PricePoint { timestamp: DateTime::from_timestamp(timestamp, 0)?, price, volume, }) }) .collect(); Ok(prices) } async fn get_onchain_metrics(&self, address: &str) -> Result<OnChainMetrics> { let url = format!( "https://public-api.birdeye.so/public/token_holders?address={}", address ); let response = self .client .get(&url) .header("X-API-KEY", &self.api_key) .send() .await? .json::<serde_json::Value>() .await?; let data = response["data"] .as_object() .ok_or_else(|| anyhow::anyhow!("Invalid response format"))?; Ok(OnChainMetrics { unique_holders: data["unique_holders"].as_u64().unwrap_or(0) as u32, active_wallets_24h: data["active_wallets_24h"].as_u64().unwrap_or(0) as u32, transactions_24h: data["transactions_24h"].as_u64().unwrap_or(0) as u32, average_transaction_size: data["avg_transaction_size"].as_f64().unwrap_or(0.0), whale_transactions_24h: data["whale_transactions_24h"].as_u64().unwrap_or(0) as u32, }) } async fn get_social_metrics(&self, _address: &str) -> Result<SocialMetrics> { // BirdEye doesn't provide social metrics Err(anyhow::anyhow!("Social metrics not available from BirdEye")) } }
```

# agents/trader/src/market_data/loaders.rs

```rs
use rig::loaders::{FileLoader, PDFLoader}; use anyhow::Result; use tracing::debug; use std::path::Path; pub struct MarketDataLoader { file_loader: FileLoader, pdf_loader: PDFLoader, } impl MarketDataLoader { pub fn new() -> Self { Self { file_loader: FileLoader::new(), pdf_loader: PDFLoader::new(), } } pub async fn load_market_report(&self, path: impl AsRef<Path>) -> Result<String> { debug!("Loading market report from {:?}", path.as_ref()); let content = if path.as_ref().extension().map_or(false, |ext| ext == "pdf") { self.pdf_loader.load(path).await? } else { self.file_loader.load(path).await? }; Ok(content) } pub async fn load_token_whitepaper(&self, path: impl AsRef<Path>) -> Result<String> { debug!("Loading token whitepaper from {:?}", path.as_ref()); self.pdf_loader.load(path).await } pub async fn load_technical_analysis(&self, path: impl AsRef<Path>) -> Result<String> { debug!("Loading technical analysis from {:?}", path.as_ref()); self.file_loader.load(path).await } }
```

# agents/trader/src/market_data/mod.rs

```rs
pub mod birdeye; pub mod streaming; pub mod storage; pub mod sentiment; pub mod macro_indicators; pub mod feature_engineering; pub mod vector_store; use anyhow::Result; use async_trait::async_trait; use chrono::{DateTime, Utc}; use serde::{Deserialize, Serialize}; use std::collections::HashMap; use std::sync::Arc; use tokio::sync::RwLock; use tracing::{info, debug, warn}; use vector_store::{TokenVectorStore, TokenAnalysis}; use crate::database::DatabaseClient; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct EnhancedTokenMetadata { // Base token data pub address: String, pub symbol: String, pub name: String, pub decimals: u8, // Price metrics pub price_usd: f64, pub price_sol: f64, pub price_change_1h: f64, pub price_change_24h: f64, pub price_change_7d: f64, // Volume metrics pub volume_24h: f64, pub volume_change_24h: f64, pub volume_by_price_24h: f64, // Volume weighted by price // Market metrics pub market_cap: f64, pub fully_diluted_market_cap: f64, pub circulating_supply: f64, pub total_supply: f64, // Liquidity metrics pub liquidity_usd: f64, pub liquidity_sol: f64, pub liquidity_change_24h: f64, // Technical indicators pub rsi_14: Option<f64>, pub macd: Option<f64>, pub macd_signal: Option<f64>, pub bollinger_upper: Option<f64>, pub bollinger_lower: Option<f64>, // On-chain metrics pub unique_holders: u32, pub active_wallets_24h: u32, pub whale_transactions_24h: u32, pub average_transaction_size: f64, // Sentiment metrics pub social_score: Option<f64>, pub social_volume: Option<u32>, pub social_sentiment: Option<f64>, pub dev_activity: Option<u32>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MacroIndicator { pub timestamp: DateTime<Utc>, pub sol_dominance: f64, pub total_market_cap: f64, pub total_volume_24h: f64, pub market_trend: String, pub fear_greed_index: i32, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct FeatureVector { pub token_address: String, pub timestamp: DateTime<Utc>, pub features: Vec<f64>, pub feature_names: Vec<String>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MarketTrend { pub token_address: String, pub metadata: TokenMetadata, pub price_change_24h: f64, pub volume_change_24h: f64, pub social_volume_24h: u32, pub dev_activity_24h: u32, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct PricePoint { pub timestamp: DateTime<Utc>, pub price: f64, pub volume: f64, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct OnChainMetrics { pub unique_holders: u32, pub active_wallets_24h: u32, pub transactions_24h: u32, pub average_transaction_size: f64, pub whale_transactions_24h: u32, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct SocialMetrics { pub twitter_followers: u32, pub twitter_engagement_rate: f64, pub discord_members: u32, pub github_stars: u32, pub telegram_members: u32, } #[async_trait] pub trait DataProvider: Send + Sync + std::fmt::Debug { async fn get_token_metadata(&self, token_address: &str) -> Result<EnhancedTokenMetadata>; async fn get_trending_tokens(&self, limit: usize) -> Result<Vec<MarketTrend>>; async fn get_historical_prices(&self, address: &str, timeframe: &str) -> Result<Vec<PricePoint>>; async fn get_macro_indicators(&self) -> Result<MacroIndicator>; async fn get_social_metrics(&self, address: &str) -> Result<SocialMetrics>; async fn get_feature_vector(&self, token_address: &str) -> Result<FeatureVector>; } #[derive(Debug)] pub struct AggregatedDataProvider { providers: Vec<Arc<dyn DataProvider>>, cache: Arc<RwLock<DataCache>>, } impl AggregatedDataProvider { pub fn new(providers: Vec<Arc<dyn DataProvider>>) -> Self { Self { providers, cache: Arc::new(RwLock::new(DataCache::default())), } } } #[async_trait] impl DataProvider for AggregatedDataProvider { async fn get_token_metadata(&self, token_address: &str) -> Result<EnhancedTokenMetadata> { // Try each provider in sequence until one succeeds for provider in &self.providers { if let Ok(metadata) = provider.get_token_metadata(token_address).await { return Ok(metadata); } } Err(anyhow::anyhow!("No provider could fetch token metadata")) } async fn get_trending_tokens(&self, limit: usize) -> Result<Vec<MarketTrend>> { let mut all_trends = Vec::new(); // Collect trends from all providers for provider in &self.providers { if let Ok(mut trends) = provider.get_trending_tokens(limit).await { all_trends.append(&mut trends); } } // Deduplicate by token address let mut unique_trends = HashMap::new(); for trend in all_trends { unique_trends.entry(trend.token_address.clone()) .or_insert(trend); } Ok(unique_trends.into_values().take(limit).collect()) } async fn get_historical_prices(&self, address: &str, timeframe: &str) -> Result<Vec<PricePoint>> { // Try each provider in sequence until one succeeds for provider in &self.providers { if let Ok(prices) = provider.get_historical_prices(address, timeframe).await { return Ok(prices); } } Err(anyhow::anyhow!("No provider could fetch historical prices")) } async fn get_macro_indicators(&self) -> Result<MacroIndicator> { // Try each provider in sequence until one succeeds for provider in &self.providers { if let Ok(indicators) = provider.get_macro_indicators().await { return Ok(indicators); } } Err(anyhow::anyhow!("No provider could fetch macro indicators")) } async fn get_social_metrics(&self, address: &str) -> Result<SocialMetrics> { // Try each provider in sequence until one succeeds for provider in &self.providers { if let Ok(metrics) = provider.get_social_metrics(address).await { return Ok(metrics); } } Err(anyhow::anyhow!("No provider could fetch social metrics")) } async fn get_feature_vector(&self, token_address: &str) -> Result<FeatureVector> { // Try each provider in sequence until one succeeds for provider in &self.providers { if let Ok(vector) = provider.get_feature_vector(token_address).await { return Ok(vector); } } Err(anyhow::anyhow!("No provider could fetch feature vector")) } } #[derive(Debug, Default)] struct DataCache { metadata_cache: HashMap<String, (EnhancedTokenMetadata, DateTime<Utc>)>, trends_cache: HashMap<String, (Vec<MarketTrend>, DateTime<Utc>)>, } pub struct MarketDataProvider { vector_store: TokenVectorStore, db_client: DatabaseClient, // ... existing fields ... } impl MarketDataProvider { pub async fn new(openai_api_key: &str, db_client: DatabaseClient) -> Result<Self> { let vector_store = TokenVectorStore::new(openai_api_key, db_client.clone()).await?; Ok(Self { vector_store, db_client, // ... initialize other fields ... }) } pub async fn analyze_token(&mut self, token_address: &str) -> Result<()> { debug!("Analyzing token {}", token_address); // Get token metadata and market data let metadata = self.get_token_metadata(token_address).await?; let market_data = self.get_market_data(token_address).await?; // Create token analysis let analysis = TokenAnalysis { token_address: token_address.to_string(), symbol: metadata.symbol.clone(), description: metadata.description.unwrap_or_default(), recent_events: market_data.recent_events, market_sentiment: self.analyze_market_sentiment(&market_data).await?, }; // Add to vector store (which will also persist to database) self.vector_store.add_token_analysis(analysis).await?; Ok(()) } pub async fn find_similar_tokens(&self, query: &str, limit: usize) -> Result<Vec<TokenAnalysis>> { debug!("Finding tokens similar to query: {}", query); let results = self.vector_store.find_similar_tokens(query, limit).await?; Ok(results.into_iter().map(|(_, _, analysis)| analysis).collect()) } pub async fn get_token_sentiment(&self, token_address: &str) -> Result<Option<String>> { self.vector_store.get_token_sentiment(token_address).await } async fn analyze_market_sentiment(&self, market_data: &MarketData) -> Result<String> { // TODO: Implement sentiment analysis using LLM // For now return a placeholder Ok("neutral".to_string()) } // ... existing methods ... } #[cfg(test)] mod tests { use super::*; use std::sync::Arc; struct MockProvider { name: String, } #[async_trait] impl DataProvider for MockProvider { async fn get_token_metadata(&self, address: &str) -> Result<EnhancedTokenMetadata> { Ok(EnhancedTokenMetadata { address: address.to_string(), symbol: "TEST".to_string(), name: format!("Test Token {}", self.name), decimals: 9, price_usd: 1.0, price_sol: 0.01, price_change_1h: 0.0, price_change_24h: 0.0, price_change_7d: 0.0, volume_24h: 1000000.0, volume_change_24h: 0.0, volume_by_price_24h: 0.0, market_cap: 10000000.0, fully_diluted_market_cap: 20000000.0, circulating_supply: 1000000.0, total_supply: 2000000.0, liquidity_usd: 0.0, liquidity_sol: 0.0, liquidity_change_24h: 0.0, rsi_14: None, macd: None, macd_signal: None, bollinger_upper: None, bollinger_lower: None, unique_holders: 0, active_wallets_24h: 0, whale_transactions_24h: 0, average_transaction_size: 0.0, social_score: None, social_volume: None, social_sentiment: None, dev_activity: None, }) } async fn get_trending_tokens(&self, limit: usize) -> Result<Vec<MarketTrend>> { let mut trends = Vec::new(); for i in 0..limit { trends.push(MarketTrend { token_address: format!("addr{}", i), price_change_24h: 10.0, volume_change_24h: 1000000.0, social_volume_24h: 1000, dev_activity_24h: 50, metadata: TokenMetadata { address: format!("addr{}", i), symbol: "TEST".to_string(), name: format!("Test Token {} {}", self.name, i), decimals: 9, price_usd: 1.0, price_sol: 0.01, volume_24h: 1000000.0, market_cap: 10000000.0, fully_diluted_market_cap: 20000000.0, circulating_supply: 1000000.0, total_supply: 2000000.0, }, }); } Ok(trends) } async fn get_historical_prices(&self, _address: &str, _timeframe: &str) -> Result<Vec<PricePoint>> { Ok(vec![ PricePoint { timestamp: Utc::now(), price: 1.0, volume: 1000000.0, } ]) } async fn get_macro_indicators(&self) -> Result<MacroIndicator> { Ok(MacroIndicator { timestamp: Utc::now(), sol_dominance: 0.5, total_market_cap: 1000000000.0, total_volume_24h: 10000000.0, market_trend: "Bullish".to_string(), fear_greed_index: 70, }) } async fn get_social_metrics(&self, _address: &str) -> Result<SocialMetrics> { Ok(SocialMetrics { twitter_followers: 10000, twitter_engagement_rate: 1000, discord_members: 5000, telegram_members: 3000, github_stars: 100, }) } async fn get_feature_vector(&self, _token_address: &str) -> Result<FeatureVector> { Ok(FeatureVector { token_address: "test_addr".to_string(), timestamp: Utc::now(), features: vec![0.5, 0.3, 0.8], feature_names: vec!["Social Score".to_string(), "Dev Activity".to_string(), "Liquidity Change".to_string()], }) } } #[tokio::test] async fn test_aggregated_provider() { let mut provider = AggregatedDataProvider::new(); provider.add_provider(Box::new(MockProvider { name: "A".to_string() })); provider.add_provider(Box::new(MockProvider { name: "B".to_string() })); let trends = provider.get_aggregated_trends(5).await.unwrap(); assert_eq!(trends.len(), 5); let metadata = provider.get_token_metadata("test_addr").await.unwrap(); assert_eq!(metadata.symbol, "TEST"); let (onchain, social) = provider.get_comprehensive_metrics("test_addr").await.unwrap(); assert_eq!(onchain.unique_holders, 1000); assert_eq!(social.twitter_followers, 10000); } }
```

# agents/trader/src/market_data/provider.rs

```rs
use rig_core::{ providers::{ DataProvider, openai::Client as OpenAIClient, twitter::TwitterClient, solana::SolanaClient, }, Result, }; use serde::{Deserialize, Serialize}; use tracing::{info, debug}; use crate::vector_store::TokenVectorStore; #[derive(Clone, Debug, Deserialize, Serialize)] pub struct TokenMetadata { pub address: String, pub symbol: String, pub name: String, pub decimals: u8, pub total_supply: u64, pub market_cap: Option<f64>, pub volume_24h: Option<f64>, pub price_usd: Option<f64>, } #[derive(Clone, Debug, Deserialize, Serialize)] pub struct MarketData { pub token: TokenMetadata, pub price_history: Vec<PricePoint>, pub social_sentiment: Option<f64>, pub technical_indicators: TechnicalIndicators, } #[derive(Clone, Debug, Deserialize, Serialize)] pub struct PricePoint { pub timestamp: i64, pub price: f64, pub volume: f64, } #[derive(Clone, Debug, Default, Deserialize, Serialize)] pub struct TechnicalIndicators { pub rsi_14: Option<f64>, pub macd: Option<f64>, pub ma_50: Option<f64>, pub ma_200: Option<f64>, } pub struct MarketDataProvider { openai_client: OpenAIClient, twitter_client: TwitterClient, solana_client: SolanaClient, vector_store: TokenVectorStore, } impl MarketDataProvider { pub async fn new( openai_api_key: &str, twitter_bearer_token: &str, solana_rpc_url: &str, vector_store: TokenVectorStore, ) -> Result<Self> { Ok(Self { openai_client: OpenAIClient::new(openai_api_key), twitter_client: TwitterClient::new(twitter_bearer_token), solana_client: SolanaClient::new(solana_rpc_url), vector_store, }) } pub async fn get_token_metadata(&self, token_address: &str) -> Result<TokenMetadata> { debug!("Fetching metadata for token {}", token_address); // Get on-chain data let mint = self.solana_client.get_mint(token_address).await?; // Get market data from external sources let market_data = self.solana_client.get_token_market_data(token_address).await?; Ok(TokenMetadata { address: token_address.to_string(), symbol: mint.symbol, name: mint.name, decimals: mint.decimals, total_supply: mint.supply, market_cap: market_data.market_cap, volume_24h: market_data.volume_24h, price_usd: market_data.price_usd, }) } pub async fn get_market_data(&self, token_address: &str) -> Result<MarketData> { debug!("Fetching market data for token {}", token_address); // Get token metadata let token = self.get_token_metadata(token_address).await?; // Get price history let price_history = self.solana_client .get_token_price_history(token_address) .await?; // Get social sentiment let social_sentiment = self.analyze_social_sentiment(&token.symbol).await?; // Calculate technical indicators let technical_indicators = self.calculate_technical_indicators(&price_history)?; Ok(MarketData { token, price_history, social_sentiment, technical_indicators, }) } async fn analyze_social_sentiment(&self, symbol: &str) -> Result<Option<f64>> { debug!("Analyzing social sentiment for {}", symbol); // Get recent tweets let tweets = self.twitter_client .search_tweets(&format!("${}", symbol)) .await?; if tweets.is_empty() { return Ok(None); } // Analyze sentiment using OpenAI let sentiment = self.openai_client .analyze_sentiment(&tweets.join("\n")) .await?; Ok(Some(sentiment)) } fn calculate_technical_indicators(&self, price_history: &[PricePoint]) -> Result<TechnicalIndicators> { if price_history.is_empty() { return Ok(TechnicalIndicators::default()); } // Calculate indicators let prices: Vec<f64> = price_history.iter().map(|p| p.price).collect(); Ok(TechnicalIndicators { rsi_14: Some(self.calculate_rsi(&prices, 14)?), macd: Some(self.calculate_macd(&prices)?), ma_50: Some(self.calculate_moving_average(&prices, 50)?), ma_200: Some(self.calculate_moving_average(&prices, 200)?), }) } fn calculate_rsi(&self, prices: &[f64], period: usize) -> Result<f64> { // TODO: Implement RSI calculation Ok(50.0) } fn calculate_macd(&self, prices: &[f64]) -> Result<f64> { // TODO: Implement MACD calculation Ok(0.0) } fn calculate_moving_average(&self, prices: &[f64], period: usize) -> Result<f64> { if prices.len() < period { return Ok(prices.last().copied().unwrap_or_default()); } let sum: f64 = prices.iter().rev().take(period).sum(); Ok(sum / period as f64) } }
```

# agents/trader/src/market_data/pumpfun.rs

```rs
use reqwest::Client; use serde::Deserialize; #[derive(Debug, Deserialize)] pub struct PumpFunMarketData { pub current_market_cap: f64, pub bonding_market_cap: f64, pub buy_volume_4h: f64, pub sell_volume_4h: f64, } pub struct MarketDataClient { client: Client, api_key: String, } impl MarketDataClient { const BASE_URL: &'static str = "https://api.pumpfunapi.org"; pub fn new(api_key: String) -> Self { Self { client: Client::new(), api_key, } } pub async fn get_token_data(&self, mint: &str) -> Result<PumpFunMarketData> { let response = self.client .get(&format!("{}/pumpfun/new/tokens", Self::BASE_URL)) .header("Authorization", &self.api_key) .send() .await? .json::<PumpFunMarketData>() .await?; Ok(response) } pub fn analyze_market(&self, data: &PumpFunMarketData) -> f64 { let liquidity_ratio = data.bonding_market_cap / data.current_market_cap.max(1.0); let volume_ratio = data.buy_volume_4h / data.sell_volume_4h.max(1.0); liquidity_ratio * volume_ratio } }
```

# agents/trader/src/market_data/storage.rs

```rs
use super::{TokenMetadata, MarketTrend, PricePoint, OnChainMetrics, SocialMetrics}; use anyhow::Result; use chrono::{DateTime, Utc}; use serde::{Deserialize, Serialize}; use std::collections::HashMap; use std::fs; use std::path::PathBuf; use std::sync::Arc; use tokio::sync::RwLock; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TokenData { pub metadata: TokenMetadata, pub price_history: Vec<PricePoint>, pub onchain_metrics: Option<OnChainMetrics>, pub social_metrics: Option<SocialMetrics>, pub last_updated: DateTime<Utc>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MarketSnapshot { pub timestamp: DateTime<Utc>, pub trends: Vec<MarketTrend>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct Position { pub token_address: String, pub entry_price: f64, pub quantity: f64, pub entry_time: DateTime<Utc>, pub partial_sells: Vec<PartialSell>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct PartialSell { pub price: f64, pub quantity: f64, pub timestamp: DateTime<Utc>, } pub struct MarketDataStorage { token_data: Arc<RwLock<HashMap<String, TokenData>>>, market_snapshots: Arc<RwLock<Vec<MarketSnapshot>>>, positions: Arc<RwLock<HashMap<String, Position>>>, max_snapshots: usize, data_dir: PathBuf, } impl MarketDataStorage { pub fn new(max_snapshots: usize) -> Self { let data_dir = PathBuf::from("data"); if !data_dir.exists() { fs::create_dir_all(&data_dir).expect("Failed to create data directory"); } let mut storage = Self { token_data: Arc::new(RwLock::new(HashMap::new())), market_snapshots: Arc::new(RwLock::new(Vec::new())), positions: Arc::new(RwLock::new(HashMap::new())), max_snapshots, data_dir, }; storage.load_from_disk(); storage } fn load_from_disk(&mut self) { self.load_token_data(); self.load_market_snapshots(); self.load_positions(); } fn load_token_data(&self) { let path = self.data_dir.join("token_data.json"); if path.exists() { if let Ok(content) = fs::read_to_string(&path) { if let Ok(data) = serde_json::from_str::<HashMap<String, TokenData>>(&content) { let mut token_data = self.token_data.blocking_write(); *token_data = data; } } } } fn load_market_snapshots(&self) { let path = self.data_dir.join("market_snapshots.json"); if path.exists() { if let Ok(content) = fs::read_to_string(&path) { if let Ok(data) = serde_json::from_str::<Vec<MarketSnapshot>>(&content) { let mut snapshots = self.market_snapshots.blocking_write(); *snapshots = data; } } } } fn load_positions(&self) { let path = self.data_dir.join("positions.json"); if path.exists() { if let Ok(content) = fs::read_to_string(&path) { if let Ok(data) = serde_json::from_str::<HashMap<String, Position>>(&content) { let mut positions = self.positions.blocking_write(); *positions = data; } } } } async fn save_to_disk(&self) -> Result<()> { self.save_token_data().await?; self.save_market_snapshots().await?; self.save_positions().await?; Ok(()) } async fn save_token_data(&self) -> Result<()> { let path = self.data_dir.join("token_data.json"); let token_data = self.token_data.read().await; let content = serde_json::to_string_pretty(&*token_data)?; fs::write(&path, content)?; Ok(()) } async fn save_market_snapshots(&self) -> Result<()> { let path = self.data_dir.join("market_snapshots.json"); let snapshots = self.market_snapshots.read().await; let content = serde_json::to_string_pretty(&*snapshots)?; fs::write(&path, content)?; Ok(()) } async fn save_positions(&self) -> Result<()> { let path = self.data_dir.join("positions.json"); let positions = self.positions.read().await; let content = serde_json::to_string_pretty(&*positions)?; fs::write(&path, content)?; Ok(()) } pub async fn add_position(&self, position: Position) -> Result<()> { let mut positions = self.positions.write().await; positions.insert(position.token_address.clone(), position); drop(positions); self.save_positions().await?; Ok(()) } pub async fn update_position(&self, token_address: &str, partial_sell: PartialSell) -> Result<()> { let mut positions = self.positions.write().await; if let Some(position) = positions.get_mut(token_address) { position.partial_sells.push(partial_sell); drop(positions); self.save_positions().await?; } Ok(()) } pub async fn get_position(&self, token_address: &str) -> Option<Position> { self.positions.read().await.get(token_address).cloned() } pub async fn get_all_positions(&self) -> HashMap<String, Position> { self.positions.read().await.clone() } pub async fn update_token_data( &self, address: &str, metadata: Option<TokenMetadata>, price_point: Option<PricePoint>, onchain: Option<OnChainMetrics>, social: Option<SocialMetrics>, ) -> Result<()> { let mut data = self.token_data.write().await; let token_data = data.entry(address.to_string()) .or_insert_with(|| TokenData { metadata: metadata.clone().unwrap_or_else(|| TokenMetadata { address: address.to_string(), symbol: String::new(), name: String::new(), decimals: 0, price_usd: 0.0, price_sol: 0.0, volume_24h: 0.0, market_cap: 0.0, fully_diluted_market_cap: 0.0, circulating_supply: 0.0, total_supply: 0.0, }), price_history: Vec::new(), onchain_metrics: None, social_metrics: None, last_updated: Utc::now(), }); if let Some(meta) = metadata { token_data.metadata = meta; } if let Some(price) = price_point { token_data.price_history.push(price); // Keep only last 24 hours of price points (assuming 1-minute intervals) if token_data.price_history.len() > 1440 { token_data.price_history.remove(0); } } if let Some(metrics) = onchain { token_data.onchain_metrics = Some(metrics); } if let Some(metrics) = social { token_data.social_metrics = Some(metrics); } token_data.last_updated = Utc::now(); drop(data); self.save_token_data().await?; Ok(()) } pub async fn add_market_snapshot(&self, trends: Vec<MarketTrend>) -> Result<()> { let mut snapshots = self.market_snapshots.write().await; snapshots.push(MarketSnapshot { timestamp: Utc::now(), trends, }); while snapshots.len() > self.max_snapshots { snapshots.remove(0); } drop(snapshots); self.save_market_snapshots().await?; Ok(()) } pub async fn get_token_data(&self, address: &str) -> Option<TokenData> { self.token_data.read().await.get(address).cloned() } pub async fn get_token_price_history(&self, address: &str) -> Option<Vec<PricePoint>> { self.token_data.read().await .get(address) .map(|data| data.price_history.clone()) } pub async fn get_market_snapshots(&self, limit: Option<usize>) -> Vec<MarketSnapshot> { let snapshots = self.market_snapshots.read().await; match limit { Some(n) => snapshots.iter().rev().take(n).cloned().collect(), None => snapshots.clone(), } } pub async fn get_trending_tokens_history(&self) -> Vec<(DateTime<Utc>, Vec<String>)> { let snapshots = self.market_snapshots.read().await; snapshots.iter() .map(|snapshot| ( snapshot.timestamp, snapshot.trends.iter() .map(|trend| trend.token_address.clone()) .collect() )) .collect() } pub async fn analyze_token_momentum(&self, address: &str) -> Option<f64> { if let Some(data) = self.get_token_data(address).await { if data.price_history.len() < 2 { return None; } // Calculate price momentum over available history let price_changes: Vec<f64> = data.price_history.windows(2) .map(|window| { let [prev, curr] = window else { unreachable!() }; (curr.price - prev.price) / prev.price }) .collect(); // Weight recent changes more heavily let weighted_sum: f64 = price_changes.iter() .enumerate() .map(|(i, change)| change * (i + 1) as f64) .sum(); let weights_sum: f64 = (1..=price_changes.len()).sum::<usize>() as f64; Some(weighted_sum / weights_sum) } else { None } } pub async fn get_token_correlation(&self, token1: &str, token2: &str) -> Option<f64> { let (hist1, hist2) = match ( self.get_token_price_history(token1).await, self.get_token_price_history(token2).await, ) { (Some(h1), Some(h2)) => (h1, h2), _ => return None, }; if hist1.is_empty() || hist2.is_empty() { return None; } // Get overlapping time periods let start_time = hist1[0].timestamp.max(hist2[0].timestamp); let end_time = hist1.last().unwrap().timestamp.min(hist2.last().unwrap().timestamp); let prices1: Vec<f64> = hist1.iter() .filter(|p| p.timestamp >= start_time && p.timestamp <= end_time) .map(|p| p.price) .collect(); let prices2: Vec<f64> = hist2.iter() .filter(|p| p.timestamp >= start_time && p.timestamp <= end_time) .map(|p| p.price) .collect(); if prices1.len() < 2 || prices2.len() < 2 { return None; } // Calculate correlation coefficient let mean1 = prices1.iter().sum::<f64>() / prices1.len() as f64; let mean2 = prices2.iter().sum::<f64>() / prices2.len() as f64; let mut covariance = 0.0; let mut var1 = 0.0; let mut var2 = 0.0; for i in 0..prices1.len() { let diff1 = prices1[i] - mean1; let diff2 = prices2[i] - mean2; covariance += diff1 * diff2; var1 += diff1 * diff1; var2 += diff2 * diff2; } let correlation = covariance / (var1.sqrt() * var2.sqrt()); Some(correlation) } } #[cfg(test)] mod tests { use super::*; #[tokio::test] async fn test_market_data_storage() { let storage = MarketDataStorage::new(100); // Test token data storage let address = "test_token"; let metadata = TokenMetadata { address: address.to_string(), symbol: "TEST".to_string(), name: "Test Token".to_string(), decimals: 9, price_usd: 1.0, price_sol: 0.01, volume_24h: 1000000.0, market_cap: 10000000.0, fully_diluted_market_cap: 20000000.0, circulating_supply: 1000000.0, total_supply: 2000000.0, }; storage.update_token_data( address, Some(metadata.clone()), Some(PricePoint { timestamp: Utc::now(), price: 1.0, volume: 1000000.0, }), None, None, ).await.unwrap(); let data = storage.get_token_data(address).await.unwrap(); assert_eq!(data.metadata.symbol, "TEST"); assert_eq!(data.price_history.len(), 1); } }
```

# agents/trader/src/market_data/streaming.rs

```rs
use anyhow::Result; use futures::{SinkExt, StreamExt}; use serde::{Deserialize, Serialize}; use std::collections::HashMap; use tokio::sync::broadcast::{self, Sender}; use tokio_tungstenite::{connect_async, tungstenite::Message}; use tracing::{error, info}; use url::Url; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct PriceUpdate { pub token_address: String, pub price: f64, pub volume: f64, pub timestamp: i64, } pub struct MarketDataStream { price_updates: Sender<PriceUpdate>, watched_tokens: HashMap<String, String>, // token_address -> symbol } impl MarketDataStream { pub fn new() -> Self { let (tx, _) = broadcast::channel(100); Self { price_updates: tx, watched_tokens: HashMap::new(), } } pub fn subscribe(&self) -> broadcast::Receiver<PriceUpdate> { self.price_updates.subscribe() } pub fn watch_token(&mut self, token_address: String, symbol: String) { self.watched_tokens.insert(token_address, symbol); } pub async fn stream_token_data(&self) -> Result<()> { let ws_url = Url::parse("wss://public-api.birdeye.so/socket")?; let (ws_stream, _) = connect_async(ws_url).await?; let (mut write, mut read) = ws_stream.split(); // Subscribe to price updates for watched tokens for token_address in self.watched_tokens.keys() { let subscribe_msg = serde_json::json!({ "event": "subscribe", "channel": format!("price:{}", token_address), }); write.send(Message::Text(subscribe_msg.to_string())).await?; } // Handle incoming messages while let Some(msg) = read.next().await { match msg { Ok(Message::Text(text)) => { if let Ok(update) = serde_json::from_str::<PriceUpdate>(&text) { if let Err(e) = self.price_updates.send(update.clone()) { error!("Failed to broadcast price update: {}", e); } } } Ok(Message::Close(_)) => { info!("WebSocket connection closed"); break; } Err(e) => { error!("WebSocket error: {}", e); break; } _ => {} } } Ok(()) } }
```

# agents/trader/src/market_data/vector_store.rs

```rs
use rig_core::{ embeddings::EmbeddingsBuilder, providers::openai::{Client, TEXT_EMBEDDING_ADA_002}, vector_store::{in_memory_store::InMemoryVectorStore, VectorStoreIndex}, Embed, }; use rig_postgres::PostgresVectorStore; use serde::{Deserialize, Serialize}; use anyhow::Result; use tracing::{info, debug}; use crate::database::DatabaseClient; use chrono::Utc; use uuid::Uuid; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TokenAnalysis { pub id: Uuid, pub token_address: String, pub sentiment_score: f64, pub technical_score: f64, pub risk_score: f64, pub symbol: String, pub description: String, pub recent_events: Vec<String>, pub market_sentiment: String, pub timestamp: chrono::DateTime<chrono::Utc>, } pub struct TokenVectorStore { store: PostgresVectorStore, } impl TokenVectorStore { pub fn new(pool: Pool<Postgres>) -> Self { // Initialize OpenAI client for embeddings let openai_client = rig_core::providers::openai::Client::from_env(); let model = openai_client.embedding_model(rig_core::providers::openai::TEXT_EMBEDDING_3_SMALL); // Initialize PostgreSQL vector store let store = PostgresVectorStore::with_defaults(model, pool); Self { store } } pub async fn add_analysis(&self, analysis: TokenAnalysis, embeddings: Embeddings) -> Result<()> { info!("Saving token analysis to vector store"); self.store.insert_document(&analysis, embeddings.embeddings[0].clone()).await?; Ok(()) } pub async fn search_similar(&self, query: &str, limit: usize) -> Result<Vec<TokenAnalysis>> { info!("Searching for similar tokens"); let results = self.store.top_n::<TokenAnalysis>(query, limit).await?; info!("Found {} similar tokens", results.len()); Ok(results.into_iter().map(|(_, _, doc)| doc).collect()) } pub async fn get_analysis(&self, token_address: &str) -> Result<Option<TokenAnalysis>> { let query = format!("token_address = '{}'", token_address); let results = self.store.find_documents::<TokenAnalysis>(&query).await?; Ok(results.into_iter().next()) } }
```

# agents/trader/src/personality/mod.rs

```rs
use rig_core::agent::Agent; use rig::completion::{CompletionModel, Prompt}; use anyhow::Result; use std::collections::HashSet; use std::time::Duration; use std::collections::HashMap; use thiserror::Error; use crate::market_data::{MarketData, MarketContext}; use solana_sdk::nonce::State; pub struct StoicPersonality { allowed_interactions: HashSet<String>, base_prompt: String, max_position_size: f64, risk_tolerance: f64, trade_cooldown: Duration, technical_indicators: Vec<String>, market_context: HashMap<String, f64>, agent: Agent<dyn CompletionModel>, } #[derive(Error, Debug)] pub enum StoicPersonalityError { #[error("Risk tolerance exceeded maximum allowed value")] RiskToleranceExceeded, #[error("Invalid position size: {0}")] InvalidPositionSize(f64), #[error("Market data incomplete: {0}")] IncompleteMarketData(String), #[error("LLM response validation failed: {0}")] ResponseValidation(String), } impl StoicPersonality { pub fn new() -> Self { Self { allowed_interactions: HashSet::new(), base_prompt: r#"You are a stoic trading bot. Your responses should reflect stoic principles: 1. Emotional detachment from market movements 2. Focus on rational decision making based on data 3. Acceptance of market conditions 4. Long-term value perspective 5. Risk management emphasis When tweeting about trades: 1. Always include exact amounts (e.g. "Bought 0.5 SOL worth of $TICKER") 2. Include market cap ("MC: $xxxM") 3. Always include contract address ("CA: address") 4. Always include Solscan transaction link 5. End with a stoic analysis based on actual market indicators: - Volume trends - Price action - Market depth - Social sentiment - Development activity"#.to_string(), max_position_size: 1.0, risk_tolerance: 0.2, trade_cooldown: Duration::from_secs(300), technical_indicators: vec![ "RSI".into(), "MACD".into(), "Volume".into() ], market_context: HashMap::new(), agent: Agent::new(rig_core::providers::openai::Client::from_env()), } } pub fn add_allowed_interaction(&mut self, twitter_handle: String) { self.allowed_interactions.insert(twitter_handle); } pub fn is_interaction_allowed(&self, twitter_handle: &str) -> bool { self.allowed_interactions.contains(twitter_handle) } pub async fn generate_trade_tweet<M: CompletionModel>( &self, agent: &Agent<M>, trade_details: &str, market_data: &MarketData, ) -> Result<String> { let prompt = format!( r#"{} Generate a tweet about this trade using the following template: [BUY/SELL] {:.2} SOL worth of {} MC: ${:.2}M | Risk: {:.1}% | Vol: {:.2}% CA: <contract_address> 🔍 https://solscan.io/tx/<tx_id> Technical Indicators: {} Market Context: {} [Stoic Analysis] {} Trade details: {} Requirements: 1. Use exact numbers from the trade details 2. Include all template fields 3. Keep stoic analysis focused on actual market data 4. Stay under 280 characters 5. Use cashtags for token symbols"#, self.base_prompt, self.max_position_size, market_data.market_cap / 1_000_000.0, self.risk_tolerance * 100.0, market_data.volatility * 100.0, self.technical_indicators.join("\n"), self.format_market_context(), trade_details ); let response = agent.prompt(&prompt).await?; Ok(self.postprocess_tweet(response)) } fn postprocess_tweet(&self, tweet: String) -> String { let mut processed = tweet.trim().to_string(); if !processed.contains("#StoicTrading") { processed.push_str("\n\n#StoicTrading #Solana #AlgoTrading"); } processed.chars().take(280).collect() } pub async fn generate_reply<M: CompletionModel>( &self, agent: &Agent<M>, tweet_text: &str, author: &str, market_context: &MarketContext, ) -> Result<Option<String>> { if !self.is_interaction_allowed(author) { return Ok(None); } let prompt = format!( r#"{} Respond to this tweet considering current market conditions: Market Trend: {} Sector Performance: {:.2}% Sentiment Score: {:.2} Tweet to respond to: {} Requirements: 1. Only respond if the tweet warrants a response 2. Be helpful but maintain stoic detachment 3. Focus on data-driven insights from these indicators: {} 4. Never give financial advice 5. Stay under 280 characters"#, self.base_prompt, market_context.market_trend, market_context.sector_performance, market_context.sentiment_score, tweet_text, self.technical_indicators.join(", ") ); let response = agent.prompt(&prompt).await?; if response.trim().is_empty() || response.to_lowercase().contains("no response") { Ok(None) } else { Ok(Some(response.to_string())) } } pub fn with_max_position_size(mut self, size: f64) -> Self { assert!(size > 0.0, "Position size must be positive"); self.max_position_size = size; self } pub fn with_risk_tolerance(mut self, tolerance: f64) -> Self { self.risk_tolerance = tolerance.clamp(0.0, 1.0); self } pub fn with_technical_indicators(mut self, indicators: Vec<String>) -> Self { self.technical_indicators = indicators; self } fn format_market_context(&self) -> String { self.market_context .iter() .map(|(k, v)| format!("{}: {:.2}", k, v)) .collect::<Vec<String>>() .join("\n") } pub async fn analyze_state(&self, state: &solana_sdk::nonce::State) -> Analysis { let prompt = format!("{} Analyze market state:\n{}", self.base_prompt, state.to_markdown() ); self.agent.prompt(&prompt) .await .parse() .unwrap_or(Analysis::Hold) } } #[cfg(test)] mod tests { use super::*; use tokio_test; #[test] fn test_personality_defaults() { let personality = StoicPersonality::default(); assert!(personality.base_prompt.contains("stoic")); assert!(personality.allowed_interactions.is_empty()); } #[test] fn test_allowed_interactions() { let mut personality = StoicPersonality::new(); personality.add_allowed_interaction("vitalik".to_string()); assert!(personality.is_interaction_allowed("vitalik")); assert!(!personality.is_interaction_allowed("random_user")); } #[test] fn test_configuration() { let personality = StoicPersonality::new() .with_max_position_size(2.5) .with_risk_tolerance(0.3) .with_technical_indicators(vec!["EMA".into(), "OBV".into()]); assert_eq!(personality.max_position_size, 2.5); assert_eq!(personality.risk_tolerance, 0.3); assert_eq!(personality.technical_indicators, vec!["EMA", "OBV"]); } #[tokio::test] async fn test_tweet_generation() { let personality = StoicPersonality::new(); let mock_agent = Agent::new(MockCompletionModel::default()); let market_data = MarketData { market_cap: 50_000_000.0, volatility: 0.15, // ... other fields ... }; let tweet = personality .generate_trade_tweet(&mock_agent, "Test trade", &market_data) .await .unwrap(); assert!(tweet.contains("#StoicTrading")); assert!(tweet.len() <= 280); } #[test] fn test_market_context_formatting() { let mut personality = StoicPersonality::new(); personality.market_context.insert("Liquidity".into(), 1.5); personality.market_context.insert("Funding Rate".into(), -0.02); let formatted = personality.format_market_context(); assert!(formatted.contains("Liquidity: 1.50")); assert!(formatted.contains("Funding Rate: -0.02")); } }
```

# agents/trader/src/prediction/mod.rs

```rs
use rig::message_bus::{MessageBus, Message}; use rig_postgres::PostgresVectorStore; use std::sync::Arc; use tch::{nn, Device, Tensor}; use crate::models::TokenAnalytics; use openai::Client; use anyhow::Result; struct Transformer { model: nn::Sequential, } impl Transformer { fn new() -> Self { let vs = nn::VarStore::new(Device::Cpu); let model = nn::seq() .add(nn::linear(&vs.root(), 512, 512, Default::default())) .add_fn(|xs| xs.relu()) .add(nn::linear(&vs.root(), 512, 1, Default::default())); Self { model } } fn load(path: &str) -> Self { let mut vs = nn::VarStore::new(Device::Cpu); let model = nn::seq() .add(nn::linear(&vs.root(), 512, 512, Default::default())) .add_fn(|xs| xs.relu()) .add(nn::linear(&vs.root(), 512, 1, Default::default())); vs.load(path).unwrap(); Self { model } } fn predict(&self, context: &[f32]) -> f32 { let input = Tensor::of_slice(context).view([-1, 512]); let output = self.model.forward(&input); output.double_value(&[0]) as f32 } } pub struct TransformerPredictor { message_bus: MessageBus, vector_store: Arc<PostgresVectorStore>, } impl TransformerPredictor { pub fn new(message_bus: MessageBus, vector_store: Arc<PostgresVectorStore>) -> Self { Self { message_bus, vector_store } } async fn train(&self) { // Load time-series data from vector store let data = self.vector_store.get_embeddings("price_history").await; let mut model = Transformer::new(); let optimizer = tch::nn::Adam::default(); // Train the model for _ in 0..100 { // epochs let loss = model.model.forward(&Tensor::of_slice(&data)); optimizer.backward_step(&loss); } model.model.save("weights.bin").unwrap(); } async fn predict(&self, context: &[f32]) -> f32 { // Load pre-trained weights let mut model = Transformer::load("weights.bin"); model.predict(context) } } pub struct PricePredictor { message_bus: MessageBus, vector_store: Arc<PostgresVectorStore>, client: Client, } impl PricePredictor { pub fn new(message_bus: MessageBus, vector_store: Arc<PostgresVectorStore>, api_key: &str) -> Self { Self { message_bus, vector_store, client: Client::new(api_key), } } async fn analyze_token(&self, analytics: &TokenAnalytics) -> Result<f32> { let prompt = format!( "Analyze trading opportunity for token:\n\ Name: {}\n\ Address: {}\n\ Historical data: {:?}\n\ Predict price movement as a percentage.", analytics.token_name, analytics.token_address, self.vector_store.get_embeddings(&analytics.token_address).await?, ); let response = self.client.chat() .create() .model("gpt-4o") .messages([openai::chat::ChatCompletionMessage { role: openai::chat::ChatCompletionMessageRole::User, content: Some(prompt), name: None, function_call: None, tool_calls: None, tool_call_id: None, }]) .create_async() .await?; let prediction = response.choices[0].message.content .as_ref() .and_then(|s| s.parse::<f32>().ok()) .unwrap_or(0.0); Ok(prediction) } }
```

# agents/trader/src/state.rs

```rs
use solana_sdk::{ account_info::AccountInfo, nonce::State }; pub struct State<'a> { pub account: AccountInfo<'a>, // Add other state fields } impl<'a> State<'a> { pub fn new(account: AccountInfo<'a>) -> Self { Self { account } } }
```

# agents/trader/src/storage/schema.rs

```rs
use rig_mongodb::{Document, DateTime, ObjectId}; use serde::{Deserialize, Serialize}; #[derive(Debug, Serialize, Deserialize)] pub struct AgentData { #[serde(rename = "_id", skip_serializing_if = "Option::is_none")] pub id: Option<ObjectId>, pub agent_type: String, pub vector_embedding: Vec<f32>, pub metadata: Document, pub timestamp: DateTime, } #[derive(Debug, Serialize, Deserialize)] pub struct TradeExecution { #[serde(rename = "_id", skip_serializing_if = "Option::is_none")] pub id: Option<ObjectId>, pub tx_hash: String, pub mint_address: String, pub amount: f64, pub risk_assessment: f64, pub vector_embedding: Vec<f32>, pub timestamp: DateTime, } #[derive(Debug, Serialize, Deserialize)] pub struct MarketAnalysis { #[serde(rename = "_id", skip_serializing_if = "Option::is_none")] pub id: Option<ObjectId>, pub market_cap: f64, pub liquidity_ratio: f64, pub volume_analysis: Document, pub vector_embedding: Vec<f32>, pub timestamp: DateTime, }
```

# agents/trader/src/strategy/execution.rs

```rs
use crate::market_data::EnhancedTokenMetadata; use crate::strategy::{TradingDecision, ExecutionParams}; use anyhow::Result; use std::collections::HashMap; use chrono::{DateTime, Utc}; use tracing::{info, warn, error, debug}; use std::time::{Duration, Instant}; #[derive(Debug)] pub struct ExecutionEngine { max_slippage: f64, active_orders: HashMap<String, ActiveOrder>, execution_history: Vec<ExecutionRecord>, last_execution: Option<Instant>, min_execution_interval: Duration, } #[derive(Debug, Clone)] pub struct ActiveOrder { pub token_address: String, pub order_type: OrderType, pub size_in_sol: f64, pub entry_price: f64, pub stop_loss: f64, pub take_profits: Vec<f64>, pub filled_amount: f64, pub status: OrderStatus, pub timestamp: DateTime<Utc>, } #[derive(Debug, Clone)] pub struct ExecutionRecord { pub token_address: String, pub order_type: OrderType, pub size_in_sol: f64, pub execution_price: f64, pub slippage: f64, pub timestamp: DateTime<Utc>, pub tx_signature: Option<String>, } #[derive(Debug, Clone)] pub enum OrderType { Market, Limit, StopLoss, TakeProfit, } #[derive(Debug, Clone)] pub enum OrderStatus { Pending, PartiallyFilled(f64), Filled, Cancelled, Failed(String), } impl ExecutionEngine { pub fn new(max_slippage: f64) -> Self { info!("Initializing ExecutionEngine with max_slippage: {}", max_slippage); Self { max_slippage, active_orders: HashMap::new(), execution_history: Vec::new(), last_execution: None, min_execution_interval: Duration::from_secs(300), // 5 minutes between trades } } pub async fn execute_trade( &mut self, decision: &TradingDecision, token: &EnhancedTokenMetadata, ) -> Result<ExecutionRecord> { // Check execution cooldown if let Some(last_exec) = self.last_execution { let elapsed = last_exec.elapsed(); if elapsed < self.min_execution_interval { let wait_time = self.min_execution_interval - elapsed; warn!("Trade execution cooldown in effect. Must wait {:?} before next trade", wait_time); return Err(anyhow::anyhow!("Trade execution cooldown in effect")); } } info!("Executing trade for token: {} ({:?})", token.symbol, decision.action); debug!("Trade details - Size: {} SOL, Risk Score: {}", decision.size_in_sol, decision.risk_score); // 1. Validate execution parameters self.validate_execution_params(&decision.execution_params) .map_err(|e| { error!("Execution parameter validation failed: {}", e); e })?; // 2. Check for existing orders if let Some(active_order) = self.active_orders.get(&decision.token_address) { debug!("Found existing order for token: {:?}", active_order); self.handle_existing_order(active_order) .map_err(|e| { error!("Failed to handle existing order: {}", e); e })?; } // 3. Prepare order parameters let order = self.prepare_order(decision, token); debug!("Prepared order: {:?}", order); // 4. Execute the order let execution_record = self.submit_order(order).await .map_err(|e| { error!("Order submission failed: {}", e); e })?; // 5. Update order tracking self.update_order_tracking(&execution_record); info!("Trade executed successfully: {:?}", execution_record); // Update last execution time self.last_execution = Some(Instant::now()); Ok(execution_record) } fn validate_execution_params(&self, params: &ExecutionParams) -> Result<()> { debug!("Validating execution parameters: {:?}", params); // Validate slippage if params.max_slippage > self.max_slippage { warn!("Slippage {} exceeds maximum allowed {}", params.max_slippage, self.max_slippage); return Err(anyhow::anyhow!("Slippage exceeds maximum allowed")); } // Validate stop loss if params.stop_loss <= 0.0 || params.stop_loss > 0.5 { warn!("Invalid stop loss percentage: {}", params.stop_loss); return Err(anyhow::anyhow!("Invalid stop loss percentage")); } // Validate take profit levels if params.take_profit.is_empty() { warn!("No take profit levels specified"); return Err(anyhow::anyhow!("No take profit levels specified")); } for (i, tp) in params.take_profit.iter().enumerate() { if *tp <= params.stop_loss { warn!("Take profit level {} ({}) must be greater than stop loss ({})", i, tp, params.stop_loss); return Err(anyhow::anyhow!("Take profit must be greater than stop loss")); } } debug!("Execution parameters validated successfully"); Ok(()) } fn handle_existing_order(&self, order: &ActiveOrder) -> Result<()> { match order.status { OrderStatus::Pending | OrderStatus::PartiallyFilled(_) => { warn!("Active order exists for token {}: {:?}", order.token_address, order.status); Err(anyhow::anyhow!("Active order exists for this token")) } _ => { debug!("No conflicting active order found"); Ok(()) } } } fn prepare_order(&self, decision: &TradingDecision, token: &EnhancedTokenMetadata) -> ActiveOrder { debug!("Preparing order for token: {}", token.symbol); let order = ActiveOrder { token_address: decision.token_address.clone(), order_type: match decision.execution_params.entry_type.as_str() { "Market" => OrderType::Market, "Limit" => OrderType::Limit, _ => OrderType::Market, }, size_in_sol: decision.size_in_sol, entry_price: token.price_sol, stop_loss: token.price_sol * (1.0 - decision.execution_params.stop_loss), take_profits: decision.execution_params.take_profit.iter() .map(|tp| token.price_sol * (1.0 + tp)) .collect(), filled_amount: 0.0, status: OrderStatus::Pending, timestamp: Utc::now(), }; debug!("Order prepared: {:?}", order); order } async fn submit_order(&self, order: ActiveOrder) -> Result<ExecutionRecord> { info!("Submitting order: {:?}", order); // TODO: Implement actual order submission through Jupiter DEX // For now, simulate a successful market order let record = ExecutionRecord { token_address: order.token_address, order_type: order.order_type, size_in_sol: order.size_in_sol, execution_price: order.entry_price, slippage: 0.001, // 0.1% simulated slippage timestamp: Utc::now(), tx_signature: Some("simulated_tx_signature".to_string()), }; info!("Order submitted successfully: {:?}", record); Ok(record) } fn update_order_tracking(&mut self, record: &ExecutionRecord) { debug!("Updating order tracking for token: {}", record.token_address); self.execution_history.push(record.clone()); self.active_orders.remove(&record.token_address); debug!("Order tracking updated. Active orders: {}", self.active_orders.len()); } pub fn get_active_orders(&self) -> &HashMap<String, ActiveOrder> { &self.active_orders } pub fn get_execution_history(&self) -> &Vec<ExecutionRecord> { &self.execution_history } }
```

# agents/trader/src/strategy/llm.rs

```rs
use crate::market_data::{birdeye::BirdEyeProvider, DataProvider}; use anyhow::Result; use std::sync::Arc; use tracing::{debug, instrument}; pub struct LLMStrategy { birdeye: Arc<BirdEyeProvider>, } #[derive(Debug)] pub struct TradeData { pub price: f64, pub volume: f64, pub market_cap: f64, pub price_change: f64, } impl LLMStrategy { pub fn new(birdeye: Arc<BirdEyeProvider>) -> Self { Self { birdeye } } #[instrument(skip(self))] pub async fn analyze_token(&self, token_address: &str) -> Result<String> { debug!("Analyzing token {}", token_address); // Get token history and market data let token_history = self.birdeye.as_ref().get_historical_prices(token_address).await?; let market_data = self.birdeye.as_ref().get_token_metadata(token_address).await?; let prompt = format!( "Analyze trading opportunity for token {}:\n\nMarket Data:\n{:#?}\n\nHistory:\n{:#?}", token_address, market_data, token_history, ); Ok(prompt) } }
```

# agents/trader/src/strategy/mod.rs

```rs
//! Trading Strategy Implementation //! //! This module implements the core trading logic using LLM-powered analysis. //! The strategy combines multiple factors: //! //! # Analysis Factors //! - Market momentum and trends //! - Volume and liquidity analysis //! - Price action patterns //! - Social sentiment and metrics //! - On-chain activity //! //! # Risk Management //! Configurable parameters (via .env): //! - `MAX_POSITION_SIZE_SOL`: Maximum position size (default: 1.0 SOL) //! - `MIN_POSITION_SIZE_SOL`: Minimum position size (default: 0.1 SOL) //! - `MAX_TOKENS_PER_WALLET`: Maximum concurrent positions //! - `STOP_LOSS_PERCENTAGE`: Auto stop-loss trigger //! - `TAKE_PROFIT_PERCENTAGE`: Auto take-profit levels //! - `MIN_LIQUIDITY_USD`: Minimum liquidity requirement //! - `MIN_CONFIDENCE_THRESHOLD`: Required confidence for trades //! //! # Position Management //! - Automatic position tracking //! - Partial profit taking //! - Dynamic position sizing //! - Trading cooldown periods pub mod llm; pub mod technical; pub mod risk; pub mod execution; use crate::market_data::{EnhancedTokenMetadata, FeatureVector, MacroIndicator}; use anyhow::Result; use rig::agent::Agent; use rig::completion::{CompletionModel, Prompt}; use serde::{Deserialize, Serialize}; use std::collections::HashMap; use chrono::Utc; use crate::analysis::Analysis; use solana_sdk::nonce::State; use uuid::Uuid; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct StrategyConfig { pub id: Uuid, pub name: String, pub description: String, pub risk_level: RiskLevel, pub parameters: StrategyParameters, pub created_at: DateTime<Utc>, pub updated_at: DateTime<Utc>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct StrategyParameters { pub min_market_cap: f64, pub min_volume_24h: f64, pub min_price_change: f64, pub max_price_change: f64, pub max_slippage: f64, } #[derive(Debug, Clone, Serialize, Deserialize)] pub enum RiskLevel { Low, Medium, High, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TradeSignal { pub id: Uuid, pub token_address: String, pub signal_type: SignalType, pub confidence: f64, pub price: f64, pub volume: f64, pub timestamp: DateTime<Utc>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub enum SignalType { Buy, Sell, Hold, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct PortfolioPosition { pub id: Uuid, pub token_address: String, pub entry_price: f64, pub quantity: f64, pub entry_timestamp: DateTime<Utc>, pub last_update: DateTime<Utc>, pub partial_sells: Vec<PartialSell>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct PartialSell { pub price: f64, pub quantity: f64, pub timestamp: DateTime<Utc>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct PortfolioStats { pub total_value_sol: f64, pub total_value_usd: f64, pub total_realized_pnl_sol: f64, pub total_unrealized_pnl_sol: f64, pub profitable_positions: i32, pub total_positions: i32, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TradingDecision { pub token_address: String, pub action: TradeAction, pub size_in_sol: f64, pub confidence: f64, pub reasoning: String, pub risk_score: f64, pub technical_signals: TechnicalSignals, pub market_context: MarketContext, pub execution_params: ExecutionParams, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TechnicalSignals { pub trend_strength: f64, pub momentum_score: f64, pub volatility_score: f64, pub support_resistance: Vec<f64>, pub signal_type: String, pub timeframe: String, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MarketContext { pub market_trend: String, pub sector_performance: f64, pub liquidity_score: f64, pub volume_profile: String, pub sentiment_score: f64, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct ExecutionParams { pub entry_type: String, pub time_horizon: String, pub stop_loss: f64, pub take_profit: Vec<f64>, pub max_slippage: f64, pub dca_config: Option<DCAConfig>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct DCAConfig { pub num_entries: u32, pub time_between_entries: u32, pub size_per_entry: f64, } #[derive(Debug, Clone, Serialize, Deserialize)] pub enum TradeAction { Buy, Sell, Hold } pub struct TradingStrategy<M: CompletionModel> { agent: Agent<M>, risk_manager: risk::RiskManager, technical_analyzer: technical::TechnicalAnalyzer, execution_engine: execution::ExecutionEngine, portfolio: HashMap<String, PortfolioPosition>, config: StrategyConfig, } impl<M: CompletionModel> TradingStrategy<M> { pub fn new( agent: Agent<M>, config: StrategyConfig, ) -> Self { Self { agent, risk_manager: risk::RiskManager::new( config.clone(), config.parameters.min_market_cap, ), technical_analyzer: technical::TechnicalAnalyzer::new(), execution_engine: execution::ExecutionEngine::new(config.parameters.max_slippage), portfolio: HashMap::new(), config, } } pub async fn analyze_opportunity( &self, token: &EnhancedTokenMetadata, features: &FeatureVector, macro_indicators: &MacroIndicator, ) -> Result<TradingDecision> { // 1. Technical Analysis let technical_signals = self.technical_analyzer.analyze(token).await?; // 2. Market Context Analysis let market_context = self.analyze_market_context(token, macro_indicators).await?; // 3. Risk Assessment let risk_score = self.risk_manager.assess_risk(token, &technical_signals, &market_context).await?; // 4. LLM-based Analysis let llm_analysis = self.perform_llm_analysis( token, features, &technical_signals, &market_context, risk_score, ).await?; // 5. Final Decision Making let decision = self.make_decision( token, llm_analysis, risk_score, &technical_signals, &market_context, ).await?; Ok(decision) } async fn analyze_market_context( &self, token: &EnhancedTokenMetadata, macro_indicators: &MacroIndicator, ) -> Result<MarketContext> { Ok(MarketContext { market_trend: macro_indicators.market_trend.clone(), sector_performance: 0.0, // TODO: Implement sector analysis liquidity_score: token.liquidity_usd / token.market_cap, volume_profile: if token.volume_change_24h > 50.0 { "High".to_string() } else { "Normal".to_string() }, sentiment_score: token.social_sentiment.unwrap_or(0.0), }) } async fn perform_llm_analysis( &self, token: &EnhancedTokenMetadata, features: &FeatureVector, technical_signals: &TechnicalSignals, market_context: &MarketContext, risk_score: f64, ) -> Result<String> { let prompt = format!( r#"Analyze trading opportunity for token {}. Technical Signals: - Trend Strength: {:.2} - Momentum Score: {:.2} - Volatility Score: {:.2} - Signal Type: {} Market Context: - Market Trend: {} - Liquidity Score: {:.2} - Volume Profile: {} - Sentiment Score: {:.2} Risk Score: {:.2} Additional Metrics: - Price Change 24h: {:.2}% - Volume Change 24h: {:.2}% - Liquidity Change 24h: {:.2}% Provide trading analysis and recommendation in a concise format."#, token.symbol, technical_signals.trend_strength, technical_signals.momentum_score, technical_signals.volatility_score, technical_signals.signal_type, market_context.market_trend, market_context.liquidity_score, market_context.volume_profile, market_context.sentiment_score, risk_score, token.price_change_24h, token.volume_change_24h, token.liquidity_change_24h, ); let response = self.agent.prompt(&prompt).await?; Ok(response.to_string()) } async fn make_decision( &self, token: &EnhancedTokenMetadata, llm_analysis: String, risk_score: f64, technical_signals: &TechnicalSignals, market_context: &MarketContext, ) -> Result<TradingDecision> { let action = if risk_score > 0.7 && technical_signals.trend_strength > 0.6 { TradeAction::Buy } else if risk_score < 0.3 || technical_signals.trend_strength < 0.2 { TradeAction::Sell } else { TradeAction::Hold }; let size = self.calculate_position_size(risk_score, technical_signals.trend_strength); Ok(TradingDecision { token_address: token.address.clone(), action, size_in_sol: size, confidence: technical_signals.trend_strength * (1.0 - risk_score), reasoning: llm_analysis, risk_score, technical_signals: technical_signals.clone(), market_context: market_context.clone(), execution_params: self.generate_execution_params(technical_signals, risk_score), }) } fn calculate_position_size(&self, risk_score: f64, trend_strength: f64) -> f64 { let base_size = self.config.parameters.max_slippage * 0.2; let risk_multiplier = 1.0 - risk_score; let trend_multiplier = trend_strength; (base_size * risk_multiplier * trend_multiplier) .max(self.config.parameters.min_position_sol) .min(self.config.parameters.max_slippage) } fn generate_execution_params(&self, signals: &TechnicalSignals, risk_score: f64) -> ExecutionParams { let stop_loss = if risk_score > 0.7 { 0.05 } else { 0.1 }; let take_profits = vec![0.1, 0.2, 0.3]; ExecutionParams { entry_type: "Market".to_string(), time_horizon: signals.timeframe.clone(), stop_loss, take_profit: take_profits, max_slippage: self.config.parameters.max_slippage, dca_config: None, } } pub fn update_portfolio(&mut self, token: EnhancedTokenMetadata, quantity: f64, cost_basis_sol: f64) { let now = Utc::now().timestamp(); let token_address = token.address.clone(); self.portfolio.insert( token.address.clone(), PortfolioPosition { id: Uuid::new_v4(), token_address, entry_price: cost_basis_sol, quantity, entry_timestamp: Utc::now(), last_update: Utc::now(), partial_sells: Vec::new(), }, ); } pub fn record_partial_sell( &mut self, token_address: &str, quantity: f64, price_sol: f64, ) -> Result<()> { let position = self.portfolio.get_mut(token_address) .ok_or_else(|| anyhow::anyhow!("Position not found"))?; let now = Utc::now().timestamp(); position.partial_sells.push(PartialSell { price: price_sol, quantity, timestamp: Utc::now(), }); position.quantity -= quantity; Ok(()) } } #[cfg(test)] mod tests { use super::*; use rig::providers::openai; #[tokio::test] async fn test_trading_strategy() { // Add tests with mock agent responses } }
```

# agents/trader/src/strategy/pipeline.rs

```rs
use rig::pipeline::{Op, Pipeline, TryOp}; use anyhow::Result; use crate::{ market_data::{MarketDataProvider, TokenAnalysis}, strategy::{TradingStrategy, TradingDecision}, execution::ExecutionEngine, }; use tracing::{info, debug}; pub struct MarketAnalysisOp { market_data: MarketDataProvider, } impl MarketAnalysisOp { pub fn new(market_data: MarketDataProvider) -> Self { Self { market_data } } } impl TryOp<String, TokenAnalysis> for MarketAnalysisOp { async fn try_run(&self, token_address: String) -> Result<TokenAnalysis> { debug!("Running market analysis for token {}", token_address); self.market_data.analyze_token(&token_address).await?; let analysis = self.market_data.get_token_analysis(&token_address).await? .ok_or_else(|| anyhow::anyhow!("No analysis found for token"))?; Ok(analysis) } } pub struct StrategyOp { strategy: TradingStrategy, } impl StrategyOp { pub fn new(strategy: TradingStrategy) -> Self { Self { strategy } } } impl TryOp<TokenAnalysis, TradingDecision> for StrategyOp { async fn try_run(&self, analysis: TokenAnalysis) -> Result<TradingDecision> { debug!("Generating trading decision for token {}", analysis.symbol); self.strategy.generate_decision(&analysis).await } } pub struct ExecutionOp { engine: ExecutionEngine, } impl ExecutionOp { pub fn new(engine: ExecutionEngine) -> Self { Self { engine } } } impl TryOp<TradingDecision, String> for ExecutionOp { async fn try_run(&self, decision: TradingDecision) -> Result<String> { debug!("Executing trading decision: {:?}", decision); let record = self.engine.execute_trade(&decision).await?; Ok(record.tx_signature.unwrap_or_default()) } } pub struct TradingPipeline { pipeline: Pipeline<String, String>, } impl TradingPipeline { pub fn new(market_data: MarketDataProvider, strategy: TradingStrategy, execution: ExecutionEngine) -> Self { let pipeline = Pipeline::new() .add_try_op(MarketAnalysisOp::new(market_data)) .add_try_op(StrategyOp::new(strategy)) .add_try_op(ExecutionOp::new(execution)); Self { pipeline } } pub async fn execute_trade(&self, token_address: String) -> Result<String> { info!("Starting trading pipeline for token {}", token_address); self.pipeline.try_run(token_address).await } }
```

# agents/trader/src/strategy/risk.rs

```rs
use crate::market_data::EnhancedTokenMetadata; use crate::strategy::{TechnicalSignals, MarketContext, StrategyConfig}; use anyhow::Result; use rig_solana_trader::personality::StoicPersonality; #[derive(Debug)] pub struct RiskManager { config: StrategyConfig, max_position_per_token: f64, max_drawdown: f64, min_liquidity_ratio: f64, personality: StoicPersonality, } impl RiskManager { pub fn new(config: StrategyConfig, personality: StoicPersonality) -> Self { Self { config, max_position_per_token: 0.2, // 20% of portfolio per token max_drawdown: 0.2, min_liquidity_ratio: 0.1, // Minimum liquidity to market cap ratio personality, } } pub async fn assess_risk( &self, token: &EnhancedTokenMetadata, technical: &TechnicalSignals, market: &MarketContext, ) -> Result<f64> { let mut risk_score = 0.0; let mut weight_sum = 0.0; // 1. Liquidity Risk (0.0 = high risk, 1.0 = low risk) let liquidity_risk = self.assess_liquidity_risk(token); risk_score += liquidity_risk * 0.3; weight_sum += 0.3; // 2. Volatility Risk let volatility_risk = 1.0 - technical.volatility_score; risk_score += volatility_risk * 0.2; weight_sum += 0.2; // 3. Market Risk let market_risk = self.assess_market_risk(market); risk_score += market_risk * 0.15; weight_sum += 0.15; // 4. Technical Risk let technical_risk = self.assess_technical_risk(technical); risk_score += technical_risk * 0.2; weight_sum += 0.2; // 5. Social/Sentiment Risk let sentiment_risk = self.assess_sentiment_risk(token, market); risk_score += sentiment_risk * 0.15; weight_sum += 0.15; // Normalize risk score to 0-1 range (0 = highest risk, 1 = lowest risk) Ok(risk_score / weight_sum) } fn assess_liquidity_risk(&self, token: &EnhancedTokenMetadata) -> f64 { let mut risk_score = 0.0; // Liquidity to market cap ratio let liquidity_ratio = token.liquidity_usd / token.market_cap; if liquidity_ratio >= self.min_liquidity_ratio { risk_score += 0.4; } // Volume analysis let volume_to_mcap = token.volume_24h / token.market_cap; risk_score += (volume_to_mcap * 5.0).min(0.3); // Cap at 0.3 // Liquidity change trend if token.liquidity_change_24h > 0.0 { risk_score += 0.2; } // Minimum thresholds if token.liquidity_usd < self.config.min_liquidity_usd { return 0.0; // Immediate rejection if below minimum liquidity } risk_score.min(1.0) } fn assess_market_risk(&self, market: &MarketContext) -> f64 { let mut risk_score = 0.5; // Start neutral // Market trend analysis match market.market_trend.as_str() { "Bullish" => risk_score += 0.2, "Bearish" => risk_score -= 0.2, _ => {} // Neutral } // Sector performance if market.sector_performance > 0.0 { risk_score += 0.1; } else { risk_score -= 0.1; } // Volume profile if market.volume_profile == "High" { risk_score += 0.1; } risk_score.max(0.0).min(1.0) } fn assess_technical_risk(&self, technical: &TechnicalSignals) -> f64 { let mut risk_score = 0.0; // Trend strength risk_score += technical.trend_strength * 0.4; // Momentum risk_score += technical.momentum_score * 0.3; // Signal type analysis match technical.signal_type.as_str() { "Strong Uptrend" => risk_score += 0.2, "Strong Downtrend" => risk_score -= 0.1, "High Volatility" => risk_score -= 0.2, "Ranging" => risk_score += 0.1, _ => {} } risk_score.max(0.0).min(1.0) } fn assess_sentiment_risk(&self, token: &EnhancedTokenMetadata, market: &MarketContext) -> f64 { let mut risk_score = 0.5; // Start neutral // Social sentiment if let Some(sentiment) = token.social_sentiment { risk_score += (sentiment - 0.5) * 0.3; } // Social volume if let Some(volume) = token.social_volume { if volume > 1000 { risk_score += 0.1; } } // Development activity if let Some(dev_activity) = token.dev_activity { if dev_activity > 0 { risk_score += 0.1; } } // Market sentiment correlation risk_score += (market.sentiment_score - 0.5) * 0.2; risk_score.max(0.0).min(1.0) } pub fn validate_position_size(&self, size_in_sol: f64, current_portfolio_value: f64) -> bool { // Check if position size is within limits if size_in_sol < self.config.min_position_sol || size_in_sol > self.config.max_position_sol { return false; } // Check position size relative to portfolio let position_ratio = size_in_sol / current_portfolio_value; if position_ratio > self.max_position_per_token { return false; } true } pub fn validate_trade(&self, action: &TradeAction) -> Result<()> { let risk_score = self.calculate_risk_score(action); if risk_score > self.personality.risk_tolerance { return Err(anyhow::anyhow!( "Risk score {} exceeds tolerance {}", risk_score, self.personality.risk_tolerance )); } Ok(()) } fn calculate_risk_score(&self, action: &TradeAction) -> f64 { let market_risk = action.analysis.as_ref().map(|a| a.risk_assessment).unwrap_or(1.0); let position_risk = action.params.amount / self.personality.max_position_size; market_risk * position_risk } }
```

# agents/trader/src/strategy/technical.rs

```rs
use crate::market_data::EnhancedTokenMetadata; use anyhow::Result; use serde::{Deserialize, Serialize}; #[derive(Debug)] pub struct TechnicalAnalyzer { rsi_period: u32, macd_fast: u32, macd_slow: u32, macd_signal: u32, bb_period: u32, bb_std_dev: f64, } impl TechnicalAnalyzer { pub fn new() -> Self { Self { rsi_period: 14, macd_fast: 12, macd_slow: 26, macd_signal: 9, bb_period: 20, bb_std_dev: 2.0, } } pub async fn analyze(&self, token: &EnhancedTokenMetadata) -> Result<super::TechnicalSignals> { let trend_strength = self.calculate_trend_strength(token); let momentum_score = self.calculate_momentum_score(token); let volatility_score = self.calculate_volatility_score(token); let support_resistance = self.identify_support_resistance(token); let signal_type = self.determine_signal_type( trend_strength, momentum_score, volatility_score, token, ); Ok(super::TechnicalSignals { trend_strength, momentum_score, volatility_score, support_resistance, signal_type, timeframe: "4h".to_string(), // Default timeframe }) } fn calculate_trend_strength(&self, token: &EnhancedTokenMetadata) -> f64 { let price_weight = if token.price_change_24h > 0.0 { 0.6 } else { 0.4 }; let volume_weight = if token.volume_change_24h > 0.0 { 0.7 } else { 0.3 }; let price_score = (token.price_change_24h / 100.0).min(1.0).max(-1.0); let volume_score = (token.volume_change_24h / 200.0).min(1.0).max(-1.0); let trend_score = (price_score * price_weight + volume_score * volume_weight).abs(); if let Some(rsi) = token.rsi_14 { let rsi_score = if rsi > 70.0 { (100.0 - rsi) / 30.0 } else if rsi < 30.0 { rsi / 30.0 } else { 0.5 + (rsi - 50.0) / 40.0 }; (trend_score + rsi_score) / 2.0 } else { trend_score } } fn calculate_momentum_score(&self, token: &EnhancedTokenMetadata) -> f64 { let mut score = 0.0; let mut signals = 0; // RSI Signal if let Some(rsi) = token.rsi_14 { score += if rsi > 70.0 { 1.0 } else if rsi < 30.0 { -1.0 } else { 0.0 }; signals += 1; } // MACD Signal if let (Some(macd), Some(signal)) = (token.macd, token.macd_signal) { score += if macd > signal { 1.0 } else { -1.0 }; signals += 1; } // Price momentum let price_momentum = token.price_change_24h / 100.0; score += price_momentum.signum(); signals += 1; // Volume momentum let volume_momentum = token.volume_change_24h / 100.0; score += volume_momentum.signum(); signals += 1; if signals > 0 { (score / signals as f64 + 1.0) / 2.0 // Normalize to 0-1 } else { 0.5 // Neutral if no signals } } fn calculate_volatility_score(&self, token: &EnhancedTokenMetadata) -> f64 { let mut volatility = 0.0; // Bollinger Bands volatility if let (Some(upper), Some(lower)) = (token.bollinger_upper, token.bollinger_lower) { let current_price = token.price_usd; let band_width = (upper - lower) / current_price; volatility += band_width; } // Price change volatility let price_volatility = token.price_change_24h.abs() / 100.0; volatility += price_volatility; // Volume volatility let volume_volatility = token.volume_change_24h.abs() / 100.0; volatility += volume_volatility; // Normalize to 0-1 range (volatility / 3.0).min(1.0) } fn identify_support_resistance(&self, token: &EnhancedTokenMetadata) -> Vec<f64> { // This is a simplified implementation // In a real system, this would analyze historical price data vec![ token.price_usd * 0.9, // Support level token.price_usd * 1.1 // Resistance level ] } fn determine_signal_type( &self, trend_strength: f64, momentum_score: f64, volatility_score: f64, token: &EnhancedTokenMetadata, ) -> String { if trend_strength > 0.7 && momentum_score > 0.7 { if token.price_change_24h > 0.0 { "Strong Uptrend".to_string() } else { "Strong Downtrend".to_string() } } else if volatility_score > 0.8 { "High Volatility".to_string() } else if trend_strength < 0.3 { "Ranging".to_string() } else { "Mixed Signals".to_string() } } }
```

# agents/trader/src/twitter.rs

```rs
impl TwitterClient { pub fn new() -> Self { TwitterClient { api_key: std::env::var("TWITTER_API_KEY").unwrap(), api_secret: std::env::var("TWITTER_API_SECRET").unwrap(), access_token: std::env::var("TWITTER_ACCESS_TOKEN").unwrap(), access_secret: std::env::var("TWITTER_ACCESS_SECRET").unwrap(), } } }
```

# agents/trader/src/twitter/mod.rs

```rs
use anyhow::Result; use reqwest::{Client, header}; use serde_json::Value; use std::sync::Arc; use tokio::sync::Mutex; pub struct TwitterClient { client: Client, username: String, cookies: String, last_tweet_time: Arc<Mutex<i64>>, } impl TwitterClient { pub fn new(username: String, cookies: String) -> Result<Self> { let mut headers = header::HeaderMap::new(); headers.insert( header::COOKIE, header::HeaderValue::from_str(&cookies)?, ); let client = Client::builder() .default_headers(headers) .build()?; Ok(Self { client, username, cookies, last_tweet_time: Arc::new(Mutex::new(0)), }) } pub async fn post_tweet(&self, text: &str) -> Result<String> { let json = serde_json::json!({ "text": text, }); let response = self.client .post("https://api.twitter.com/2/tweets") .json(&json) .send() .await? .json::<Value>() .await?; Ok(response["data"]["id"].as_str() .ok_or_else(|| anyhow::anyhow!("Failed to get tweet ID"))? .to_string()) } pub async fn reply_to_tweet(&self, reply_to_id: &str, text: &str) -> Result<String> { let json = serde_json::json!({ "text": text, "reply": { "in_reply_to_tweet_id": reply_to_id } }); let response = self.client .post("https://api.twitter.com/2/tweets") .json(&json) .send() .await? .json::<Value>() .await?; Ok(response["data"]["id"].as_str() .ok_or_else(|| anyhow::anyhow!("Failed to get tweet ID"))? .to_string()) } }
```

# agents/trader/src/wallet.rs

```rs
use solana_sdk::{ pubkey::Pubkey, signature::{Keypair, ParseKeypairError}, }; use std::str::FromStr; pub fn load_wallet() -> Result<Keypair, ParseKeypairError> { let private_key = std::env::var("PRIVATE_KEY") .expect("PRIVATE_KEY must be set in .env"); Keypair::from_base58_string(&private_key) } pub fn get_public_key(keypair: &Keypair) -> Pubkey { keypair.pubkey() } pub fn load_keypair() -> Keypair { Keypair::new() // Use proper keypair loading in production }
```

# Cargo.toml

```toml
[package] name = "cainam-core" version = "0.1.0" edition = "2021" authors = ["Matt Gunnin <matt@cainamventures.com>"] repository = "https://github.com/cainamventures/cainam-core" readme = "README.md" keywords = ["ai", "solana", "rust", "cainam", "cainam-ventures"] description = "Core functionality for the Cainam project" [[bin]] name = "cainam-core" path = "src/main.rs" [[bin]] name = "setup_mongodb" path = "scripts/setup_mongodb.rs" [[bin]] name = "init_vector_store" path = "scripts/init_vector_store.rs" [[bin]] name = "test_vector_search" path = "scripts/test_vector_search.rs" [workspace] resolver = "2" members = [ # plugins "plugins/birdeye", "plugins/discord", "plugins/twitter", ] exclude = [ "examples", "memory-bank", "phases_output", ] [workspace.package] version = "0.1.0" edition = "2021" [profile.dev] opt-level = "z" [profile.release] codegen-units = 1 lto = "thin" opt-level = "z" strip = true [dependencies] anyhow = "1.0" async-trait = "0.1" bigdecimal = { version = "0.2", features = ["serde"] } bson = "2.0" chrono = "0.4" futures = "0.3" mockall = "0.11.0" mongodb = { version = "3.2.1", default-features = false, features = ["sync"] } reqwest = { version = "0.11", features = ["json"] } rig-core = "0.8.0" rig-mongodb = "0.2.4" serde = { version = "1.0", features = ["derive"] } serde_derive = "1.0" thiserror = "1.0" time = "0.3" tokio = { version = "1", features = ["full", "macros"] } tracing = "0.1" tracing-subscriber = "0.3" # Blockchain dependencies solana-account-decoder = "1.17" solana-client = "1.17" solana-sdk = "1.17" spl-associated-token-account = "2.2" spl-token = "4.0" # Additional utilities dotenvy = "0.15.7" serde_json = "1.0" uuid = { version = "1.6", features = ["v4", "serde"] }
```

# CONTRIBUTING.md

```md
# Contributing to Cainam Core Thank you for considering contributing to Cainam Core! Here are some guidelines to help you get started. ## Issues Before reporting an issue, please check existing or similar issues that are currently tracked. ## Pull Requests Contributions are always encouraged and welcome. Before creating a pull request, create a new issue that tracks that pull request describing the problem in more detail. Pull request descriptions should include information about it's implementation, especially if it makes changes to existing abstractions. PRs should be small and focused and should avoid interacting with multiple facets of the library. This may result in a larger PR being split into two or more smaller PRs. Commit messages should follow the [Conventional Commit](conventionalcommits.org/en/v1.0.0) format (prefixing with `feat`, `fix`, etc.) as this integrates into our auto-releases via a [release-plz](https://github.com/MarcoIeni/release-plz) Github action. **Working on your first Pull Request?** You can learn how from this *free* series [How to Contribute to an Open Source Project on GitHub](https://kcd.im/pull-request) ## Project Structure TBD ## Developing ### Setup \`\`\`bash git clone https://github.com/cainamventures/cainam-core cd cainam-core cargo test \`\`\` ### Clippy and Fmt We enforce both `clippy` and `fmt` for all pull requests. \`\`\`bash cargo clippy -- -D warnings \`\`\` \`\`\`bash cargo fmt \`\`\` ### Tests Make sure to test against the test suite before making a pull request. \`\`\`bash cargo test \`\`\`
```

# memory-bank/activeContext.md

```md
# Active Context ## Current Task Implementing and debugging MongoDB vector store integration for the Cainam Core Agent, specifically focusing on using the `rig-mongodb` crate correctly. ## Action Plan 1. ✅ MongoDB Atlas Integration - Set up MongoDB Atlas cluster - Configured connection string and authentication - Implemented connection pooling 2. ✅ Vector Store Implementation - Added MongoDB vector store support - Implemented token analytics collection - Created vector search index for embeddings 3. ✅ **Current Issues Resolved** - Fixed SearchParams configuration for vector search (removed unnecessary parameters) - Resolved vector store initialization errors - Corrected generic type usage with `rig-mongodb` (`MongoDbVectorIndex::<_, TokenAnalyticsData>::new`) - Fixed collection type mismatch (used `collection::<TokenAnalyticsData>`) 4. 🔄 **Current Focus** - Thoroughly testing the vector search functionality. - Ensuring the `test_vector_search.rs` script works correctly. ## Technical Context - Project uses MongoDB Atlas for vector store capabilities - Vector search implemented using MongoDB Atlas Search and the `rig-mongodb` crate. - Token analytics data stored with embeddings - Connection pooling configured for optimal performance ## Resolution Progress Current implementation includes: 1. ✅ MongoDB connection pool configuration 2. ✅ Token analytics data structure 3. ✅ Vector index creation 4. ✅ Search parameters configuration (simplified) 5. ✅ Document insertion functionality 6. ✅ `rig-mongodb` integration for vector search Current Issues: - None identified. Focus is on testing. Next steps: 1. Thoroughly test vector search functionality. 2. Implement proper error handling (ongoing). 3. Add comprehensive logging (ongoing). 4. Document MongoDB integration details (ongoing). Technical Notes: - Using MongoDB Atlas vector search capabilities - Embedding dimension: 1536 (OpenAI compatible) - Cosine similarity for vector search - Connection pooling configured with: - Min pool size: 5 - Max pool size: 10 - Connect timeout: 20 seconds - Vector index using IVFFlat algorithm (default for `rig-mongodb`) - Using `rig-mongodb` for simplified vector search implementation.
```

# memory-bank/codeReview.md

```md
# Code Review Guidelines Last Updated: 2025-02-11 ## Focus Areas ### 1. MongoDB Integration - Connection pooling configuration - Error handling and retry logic - Proper use of MongoDB Atlas features - Vector store implementation ### 2. Vector Search Implementation - Proper embedding handling - Search parameter configuration - Index creation and management - Query optimization ### 3. Error Handling \`\`\`rust // Good: Proper error context and handling pub async fn search_tokens(query: &str) -> Result<Vec<TokenAnalytics>> { let results = pool.top_n("token_analytics", model, query, 10) .await .context("Failed to perform vector search")?; process_results(results) .context("Failed to process search results") } // Bad: Missing error context pub async fn search_tokens(query: &str) -> Result<Vec<TokenAnalytics>> { let results = pool.top_n("token_analytics", model, query, 10).await?; process_results(results) } \`\`\` ### 4. Connection Management \`\`\`rust // Good: Proper connection pool configuration let pool_config = MongoPoolConfig { min_pool_size: 5, max_pool_size: 10, connect_timeout: Duration::from_secs(20), }; // Bad: Hardcoded values without configuration let client = Client::with_uri_str("mongodb://localhost").await?; \`\`\` ### 5. Vector Store Operations \`\`\`rust // Good: Proper search parameters let search_params = SearchParams::new() .exact(true) .num_candidates(100) .fields(vec!["embedding"]); // Bad: Missing required parameters let search_params = SearchParams::new() .exact(true) .num_candidates(100); \`\`\` ## Review Checklist ### MongoDB Integration - [ ] Proper connection pool configuration - [ ] Error handling with context - [ ] Retry logic for transient failures - [ ] Proper use of MongoDB Atlas features - [ ] Connection string security ### Vector Store Implementation - [ ] Proper embedding field configuration - [ ] Search parameter completeness - [ ] Index creation and management - [ ] Query optimization - [ ] Error handling for vector operations ### Code Quality - [ ] Error handling with proper context - [ ] Logging for important operations - [ ] Performance considerations - [ ] Type safety and null handling - [ ] Documentation completeness ### Testing - [ ] Unit tests for vector operations - [ ] Integration tests for MongoDB - [ ] Error case coverage - [ ] Performance benchmarks - [ ] Connection pool tests ## Common Issues to Watch 1. MongoDB Operations - Missing error context - Improper connection handling - Missing retry logic - Hardcoded configuration 2. Vector Store - Missing search parameters - Improper embedding handling - Missing index configuration - Inefficient queries 3. Error Handling - Generic error types - Missing error context - Improper error propagation - Missing logging 4. Performance - Connection pool misconfiguration - Missing indexes - Inefficient queries - Resource leaks ## Best Practices ### MongoDB Integration \`\`\`rust // Connection Pool impl MongoDbPool { pub async fn create_pool(config: MongoConfig) -> Result<Arc<MongoDbPool>> { let mut client_options = ClientOptions::parse(&config.uri).await?; config.pool_config.apply_to_options(&mut client_options); let client = Client::with_options(client_options)?; Ok(Arc::new(MongoDbPool { client, config })) } } // Error Handling pub async fn insert_documents(docs: Vec<Document>) -> Result<()> { let collection = self.get_collection()?; collection .insert_many(docs) .await .context("Failed to insert documents")?; Ok(()) } \`\`\` ### Vector Store Operations \`\`\`rust // Search Implementation pub async fn search_similar(query: &str, limit: usize) -> Result<Vec<Document>> { let search_params = SearchParams::new() .exact(true) .num_candidates(100) .fields(vec!["embedding"]); let index = MongoDbVectorIndex::new( collection, model, "vector_index", search_params ).await?; index.top_n(query, limit).await } \`\`\` ## Documentation Requirements 1. Function Documentation \`\`\`rust /// Performs a vector similarity search in the token analytics collection /// /// # Arguments /// * `query` - The search query string /// * `limit` - Maximum number of results to return /// /// # Returns /// * `Result<Vec<TokenAnalytics>>` - Search results or error with context pub async fn search_tokens(query: &str, limit: usize) -> Result<Vec<TokenAnalytics>> \`\`\` 2. Error Documentation \`\`\`rust /// Possible errors during vector store operations #[derive(Error, Debug)] pub enum VectorStoreError { #[error("MongoDB operation failed: {0}")] MongoError(#[from] mongodb::error::Error), #[error("Vector search failed: {0}")] SearchError(String), #[error("Invalid configuration: {0}")] ConfigError(String), } \`\`\`
```

# memory-bank/developmentWorkflow.md

```md
# Development Workflow Last Updated: 2025-02-11 ## Implementation Plan ### Phase 1: Core Infrastructure (Current Phase) #### Vector Store Implementation - [x] MongoDB Atlas Setup - [x] Configure connection pooling - [x] Set up authentication - [x] Create collections - [x] Vector Search Integration - [x] Create vector index - [x] Implement embedding storage - [x] Configure search parameters - [ ] Token Analytics System - [x] Implement data models - [x] Add document insertion - [ ] Complete search functionality - [ ] Add comprehensive error handling #### Next Steps: Agent System - [ ] Complete trader agent implementation - [ ] Vector store integration - [ ] Market signal processing - [ ] Decision making logic - [ ] Risk Management - [ ] Risk scoring system - [ ] Position monitoring - [ ] Portfolio analysis ### Current Focus 1. Vector Store Completion - Fix SearchParams configuration - Implement proper error handling - Add comprehensive logging - Complete testing suite 2. Agent Integration - Connect vector store to agent system - Implement market analysis - Add decision making logic ## Testing Strategy ### Unit Testing \`\`\`rust #[cfg(test)] mod tests { use super::*; #[tokio::test] async fn test_vector_search() -> Result<()> { let pool = setup_test_pool().await?; let result = pool.top_n("test_collection", model, "query", 10).await?; assert!(!result.is_empty()); Ok(()) } } \`\`\` ### Integration Testing 1. MongoDB Operations - Connection pool management - Document insertion - Vector search functionality - Error handling 2. Vector Store Integration - Embedding generation - Search accuracy - Performance metrics - Error scenarios ## Project Standards ### Code Organization \`\`\` src/ ├── config/ # Configuration (MongoDB, etc.) ├── models/ # Data models ├── services/ # Business logic ├── agent/ # Agent implementations └── trading/ # Trading logic \`\`\` ### Error Handling \`\`\`rust use anyhow::{Context, Result}; pub async fn search_tokens(query: &str) -> Result<Vec<TokenAnalytics>> { let results = pool.top_n("token_analytics", model, query, 10) .await .context("Failed to perform vector search")?; process_results(results) .context("Failed to process search results")?; Ok(results) } \`\`\` ### MongoDB Integration \`\`\`rust // Connection Pool Configuration let pool_config = MongoPoolConfig { min_pool_size: 5, max_pool_size: 10, connect_timeout: Duration::from_secs(20), }; // Vector Search Parameters let search_params = SearchParams::new() .exact(true) .num_candidates(100) .fields(vec!["embedding"]); \`\`\` ## Monitoring and Maintenance ### Health Checks - MongoDB connection status - Vector search performance - Error rates and types - System resource usage ### Performance Metrics - Search latency - Connection pool utilization - Document insertion rates - Memory usage ### Error Handling - Structured error logging - MongoDB operation retries - Connection error recovery - Alert thresholds ### Maintenance Tasks - Index optimization - Connection pool monitoring - Error log analysis - Performance tuning
```

# memory-bank/operationalContext.md

```md
# Operational Context Last Updated: 2025-01-30 ## System Operation ### Core Services 1. **Market Data Service** \`\`\`rust pub struct MarketDataService { birdeye_client: BirdeyeClient, db_pool: PgPool, cache: Cache, } \`\`\` - Real-time price and volume monitoring - Historical data aggregation - Market trend analysis - Data validation and cleaning 2. **Trading Service** \`\`\`rust pub struct TradingService { engine: TradingEngine, risk_manager: RiskManager, solana_client: SolanaClient, } \`\`\` - Trade execution - Position management - Risk validation - Transaction signing 3. **Agent Coordination Service** \`\`\`rust pub struct AgentCoordinator { agents: Vec<Box<dyn Agent>>, message_bus: MessageBus, state_manager: StateManager, } \`\`\` - Agent lifecycle management - Inter-agent communication - State synchronization - Performance monitoring ### Error Handling Patterns 1. **Database Errors** \`\`\`rust #[derive(Error, Debug)] pub enum DatabaseError { #[error("Connection failed: {0}")] ConnectionError(String), #[error("Query failed: {0}")] QueryError(String), #[error("Data validation failed: {0}")] ValidationError(String), } \`\`\` - Connection retry logic - Query timeout handling - Data integrity checks 2. **API Errors** \`\`\`rust #[derive(Error, Debug)] pub enum ApiError { #[error("Rate limit exceeded")] RateLimitError, #[error("Authentication failed: {0}")] AuthError(String), #[error("Request failed: {0}")] RequestError(String), } \`\`\` - Rate limiting - Authentication handling - Request retries 3. **Trading Errors** \`\`\`rust #[derive(Error, Debug)] pub enum TradingError { #[error("Insufficient funds: {0}")] InsufficientFunds(String), #[error("Invalid trade: {0}")] InvalidTrade(String), #[error("Execution failed: {0}")] ExecutionError(String), } \`\`\` - Position validation - Balance checks - Transaction verification ### Infrastructure Requirements 1. **Database** - PostgreSQL 15+ with TimescaleDB - Minimum 16GB RAM - SSD storage - Regular backups - Connection pooling 2. **Network** - Low latency connection - Redundant connectivity - DDoS protection - SSL/TLS encryption 3. **Compute** - Multi-core CPU - Minimum 32GB RAM - Load balancing - Auto-scaling ### Performance Requirements 1. **Latency Targets** \`\`\`rust pub struct PerformanceMetrics { trade_execution_ms: u64, // Target: < 500ms market_data_refresh_ms: u64, // Target: < 1000ms signal_processing_ms: u64, // Target: < 200ms db_query_ms: u64, // Target: < 100ms } \`\`\` 2. **Throughput Requirements** - 1000+ market signals/second - 100+ trades/minute - 10000+ database operations/second - 100+ concurrent agents 3. **Resource Utilization** - CPU: < 70% sustained - Memory: < 80% usage - Disk I/O: < 70% utilization - Network: < 50% capacity ## Monitoring and Alerting ### System Health Monitoring \`\`\`rust pub struct HealthCheck { pub service: String, pub status: Status, pub last_check: DateTime<Utc>, pub metrics: HashMap<String, f64>, } \`\`\` 1. **Service Health** - API availability - Database connectivity - Agent status - Memory usage 2. **Performance Metrics** - Trade execution latency - Market data freshness - Database query performance - Network latency 3. **Business Metrics** - Trade success rate - Agent performance - Portfolio returns - Risk exposure ### Alert Thresholds 1. **Critical Alerts** - Trade execution failures - Database connectivity issues - API authentication errors - Memory exhaustion 2. **Warning Alerts** - High latency - Elevated error rates - Resource utilization - Rate limit warnings 3. **Information Alerts** - Agent state changes - Database maintenance - Performance optimization - System updates ## Recovery Procedures ### 1. Database Recovery \`\`\`sql -- Point-in-time recovery SELECT * FROM market_signals WHERE timestamp >= '2025-01-30 00:00:00' AND timestamp < '2025-01-30 01:00:00'; -- Reprocess failed trades SELECT * FROM trade_executions WHERE status = 'FAILED' AND execution_time > now() - interval '1 hour'; \`\`\` ### 2. Service Recovery \`\`\`rust impl RecoveryManager { async fn recover_service(&self) -> Result<()> { // 1. Stop affected service // 2. Verify dependencies // 3. Restore state // 4. Restart service // 5. Verify operation } } \`\`\` ### 3. Data Integrity \`\`\`rust impl DataValidator { async fn validate_market_data(&self) -> Result<()> { // 1. Check data consistency // 2. Verify calculations // 3. Compare with backup sources // 4. Report discrepancies } } \`\`\` ## Maintenance Procedures ### 1. Database Maintenance - Daily backup verification - Weekly index optimization - Monthly data archival - Quarterly performance review ### 2. System Updates - Security patches - Dependency updates - Performance optimizations - Feature deployments ### 3. Monitoring Updates - Alert threshold adjustments - Metric collection tuning - Dashboard updates - Log rotation
```

# memory-bank/productContext.md

```md
# Product Context Last Updated: 2025-02-12 ## Core Problem Building a decentralized network of autonomous AI trading agents for the $CAINAM token platform on Solana requires efficient market data analysis, semantic search capabilities, and coordinated agent decision-making while ensuring reliability, security, and performance. We need a way to quickly find tokens based on semantic meaning, not just keywords. ## Key Components/Solutions ### 1. Vector Store & Market Analysis **Problem:** Need efficient storage and semantic search of market data and token analytics **Solution:** - MongoDB Atlas vector store implementation - Embedding-based similarity search using `rig-mongodb` - Token analytics data storage and retrieval - Efficient connection pooling and error handling ### 2. Agent Intelligence **Problem:** Agents need to make informed decisions based on historical and real-time data **Solution:** - Vector-based similarity search for market patterns - Semantic analysis of token characteristics - Efficient data retrieval through MongoDB Atlas - Scalable document storage and indexing ### 3. Data Management **Problem:** Need efficient storage and retrieval of market data and embeddings **Solution:** - MongoDB Atlas for document storage - Vector search capabilities for similarity matching - Efficient connection pooling - Proper error handling and retry logic ## Core Workflows ### 1. Token Analytics Processing 1. Token data collection and validation (from Birdeye, etc.) 2. Embedding generation for token characteristics (using OpenAI) 3. Storage in MongoDB with vector indexing 4. Efficient similarity search capabilities (using `rig-mongodb`) ### 2. Market Analysis 1. Real-time market data processing 2. Vector-based pattern recognition (future) 3. Similarity search for historical patterns 4. Decision making based on analysis (future) ### 3. Agent Operations 1. Continuous market monitoring 2. Vector-based similarity analysis 3. Pattern recognition and decision making (future) 4. Performance tracking and optimization ## Product Direction ### Phase 1: Vector Store Implementation (Current) - MongoDB Atlas integration - Vector search capabilities using `rig-mongodb` - Token analytics storage - Connection pooling and error handling ### Phase 2: Agent Intelligence (Next) - Enhanced market analysis - Pattern recognition - Decision making logic - Performance optimization ### Phase 3: Advanced Features (Future) - Advanced similarity search - Multi-dimensional analysis - Enhanced error handling - Performance monitoring ## Development Priorities 1. **Immediate Focus** - Complete MongoDB vector store implementation using `rig-mongodb` - Ensure correct generic type usage with `rig-mongodb` - Implement comprehensive error handling - Add proper logging and monitoring 2. **Short-term Goals** - Enhanced vector search capabilities - Agent integration with vector store - Performance optimization - Testing infrastructure 3. **Medium-term Goals** - Advanced pattern recognition - Enhanced decision making - System scalability - Advanced monitoring ## Success Metrics - Vector search accuracy and speed - System reliability and uptime - Query performance and latency - Error handling effectiveness - Connection pool efficiency
```

# memory-bank/projectBoundaries.md

```md
# Project Boundaries Last Updated: 2025-01-30 ## Technical Constraints ### 1. Performance Boundaries #### Latency Requirements - Trade execution: < 500ms end-to-end - Market data updates: < 1s refresh rate - Signal processing: < 200ms - Database queries: < 100ms response time #### Throughput Limits - Maximum 100 concurrent agents - Up to 1000 market signals per second - Maximum 100 trades per minute - Up to 10000 database operations per second #### Resource Constraints - Memory usage: < 32GB per instance - CPU utilization: < 70% sustained - Network bandwidth: < 1Gbps - Storage: < 1TB active data ### 2. API Limitations #### Birdeye API - Rate limit: 10 requests/second - Websocket connections: 5 max - Data freshness: 1s minimum - Historical data: 90 days #### Helius API - Webhook delivery: Best effort - Transaction history: 30 days - Rate limit: 100 requests/second - Concurrent connections: 10 max #### Solana RPC - Transaction confirmation: 2-4s - Rate limit: 40 requests/second - Connection limit: 20 per IP - Data size: 5MB max per request ### 3. Database Constraints #### TimescaleDB - Chunk interval: 1 day - Retention period: 1 year - Compression ratio: 10:1 target - Query complexity: < 1000 rows scan #### Qdrant - Vector dimensions: 1536 max - Index size: 1M vectors - Query time: < 50ms - Similarity threshold: 0.8 ## Scale Requirements ### 1. Data Volume \`\`\`rust pub struct DataVolume { market_signals_per_day: u64, // 86_400_000 trades_per_day: u64, // 144_000 token_analytics_per_day: u64, // 2_160_000 agent_metrics_per_day: u64, // 144_000 } \`\`\` ### 2. System Scale \`\`\`rust pub struct SystemScale { concurrent_agents: u32, // 100 active_markets: u32, // 1000 monitored_tokens: u32, // 10000 trading_pairs: u32, // 100 } \`\`\` ### 3. Storage Requirements \`\`\`rust pub struct StorageRequirements { market_data_per_day: u64, // 10GB trade_data_per_day: u64, // 1GB analytics_per_day: u64, // 5GB log_data_per_day: u64, // 2GB } \`\`\` ## Hard Limitations ### 1. Trading Restrictions \`\`\`rust pub struct TradingLimits { max_position_size: f64, // 5% of portfolio min_trade_size: f64, // $10 equivalent max_trades_per_minute: u32, // 100 max_slippage: f64, // 1% } \`\`\` ### 2. Risk Management \`\`\`rust pub struct RiskLimits { max_portfolio_exposure: f64, // 20% max_correlation: f64, // 0.7 min_confidence: f64, // 0.8 max_drawdown: f64, // 10% } \`\`\` ### 3. Technical Limits \`\`\`rust pub struct TechnicalLimits { max_concurrent_requests: u32, // 1000 max_websocket_connections: u32, // 100 max_database_connections: u32, // 500 max_memory_usage: u64, // 32GB } \`\`\` ## Non-Negotiables ### 1. Security Requirements - All private keys must be securely stored - All API communications must be encrypted - Rate limiting must be enforced - Access control for all operations ### 2. Data Integrity - All trades must be verified - Market data must be validated - Database consistency must be maintained - Audit trail for all operations ### 3. Reliability - No single point of failure - Automatic failover required - Data backup mandatory - Error recovery procedures required ## Future Considerations ### 1. Scalability - Horizontal scaling of agents - Distributed database deployment - Load balancing implementation - Cache layer addition ### 2. Feature Expansion - Cross-chain integration - Advanced analytics - Machine learning models - Social sentiment analysis ### 3. Performance Optimization - Query optimization - Caching strategies - Network optimization - Resource allocation ## Compliance Requirements ### 1. Data Retention - Trade records: 7 years - Market data: 1 year - System logs: 90 days - Error reports: 1 year ### 2. Audit Requirements - All trades must be traceable - Risk checks must be documented - System changes must be logged - Performance metrics must be stored ### 3. Reporting Requirements - Daily performance reports - Risk exposure analysis - System health metrics - Compliance verification
```

# memory-bank/projectbrief.md

```md
# Project Brief **Project Name:** Cainam Core Agent **Last Updated:** 2025-02-12 **Objective:** Develop a decentralized network of autonomous AI trading agents for the $CAINAM token platform on Solana, focusing on efficient market data analysis, semantic search, and coordinated decision-making. **Core Requirements:** 1. **Market Data Analysis:** * Real-time and historical market data processing. * Efficient semantic search capabilities for token analytics. * Market pattern recognition and signal generation. 2. **Autonomous Trading Agents:** * Intelligent decision-making based on market data and analysis. * Coordinated trading strategies. * Risk management and portfolio optimization. 3. **Decentralized Network:** * Agent communication and coordination. * Secure and reliable operation. * Scalability to support multiple agents and markets. 4. **Technical Foundation:** * Integration with Solana blockchain. * Utilization of MongoDB Atlas for vector storage and search. * Robust error handling and performance optimization. **Success Criteria:** * Accurate and efficient market data analysis. * Reliable vector search capabilities. * Functional autonomous trading agents. * Secure and scalable decentralized network. * Comprehensive error handling and monitoring.
```

# memory-bank/techContext.md

```md
# Technical Context ## Vector Store Implementation ### MongoDB Atlas Setup - Enabled Atlas Search for vector similarity search capabilities - Created token_analytics collection with document structure for embeddings - Implemented vector search index for efficient similarity search using cosine distance - Added vector store integration with proper connection pooling ### Database Schema The vector store implementation uses the following document structure: \`\`\`json { "_id": ObjectId, "token_address": String, "token_name": String, "token_symbol": String, "embedding": Array<float>, "created_at": ISODate } \`\`\` ### Search Configuration Implemented MongoDB vector search with: - Vector search index on embedding field - Cosine similarity for distance calculation - Configurable search parameters: - Exact matching option - Number of candidates - Field specification for embedding search ### Integration Notes - Using OpenAI's text-embedding-3-small model (1536 dimensions) - Configured with MongoDB Atlas Search for vector similarity - Supports batch document insertion - Includes proper connection pooling - Implements retry logic for operations ### Current Implementation 1. MongoDB Connection Pool - Configurable min/max pool size - Connection timeout settings - Error handling for connection issues 2. Vector Store Operations - Document insertion with embeddings - Vector similarity search - Top-N query support - Proper error handling 3. Data Models - TokenAnalyticsData structure - Proper serialization/deserialization - ObjectId handling - Embedding field management ### Error Handling - Comprehensive error types for MongoDB operations - Connection error handling - Vector store operation error handling - Proper error propagation - Logging integration with tracing ### Pending Improvements 1. SearchParams configuration refinement 2. Enhanced error context for vector operations 3. Additional logging for debugging 4. Performance optimization for batch operations 5. Connection pool monitoring
```

# migrations/01_initial_schema.sql

```sql
-- Enable required extensions CREATE EXTENSION IF NOT EXISTS "uuid-ossp"; CREATE EXTENSION IF NOT EXISTS timescaledb; -- Create enum types CREATE TYPE trade_status AS ENUM ('PENDING', 'EXECUTED', 'FAILED', 'CANCELLED'); CREATE TYPE signal_type AS ENUM ('BUY', 'SELL', 'HOLD', 'STRONG_BUY', 'STRONG_SELL', 'PRICE_SPIKE', 'PRICE_DROP', 'VOLUME_SURGE'); -- Market Signals CREATE TABLE market_signals ( id SERIAL, asset_address VARCHAR NOT NULL, signal_type VARCHAR NOT NULL, confidence DECIMAL NOT NULL, risk_score DECIMAL NOT NULL, sentiment_score DECIMAL, volume_change_24h DECIMAL, price_change_24h DECIMAL, timestamp TIMESTAMPTZ NOT NULL, metadata JSONB DEFAULT '{}', created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (id, timestamp) ); -- Trade Executions CREATE TABLE trade_executions ( id UUID DEFAULT gen_random_uuid(), signal_id INTEGER, signal_timestamp TIMESTAMPTZ, asset_address TEXT NOT NULL, size DECIMAL NOT NULL, entry_price DECIMAL NOT NULL, slippage DECIMAL NOT NULL, execution_time TIMESTAMPTZ NOT NULL, status TEXT NOT NULL, transaction_signature TEXT, fee_amount DECIMAL, metadata JSONB DEFAULT '{}', created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP, UNIQUE (id, execution_time) ); -- Agent Performance Metrics CREATE TABLE agent_performance ( id UUID PRIMARY KEY DEFAULT gen_random_uuid(), agent_type TEXT NOT NULL, accuracy DECIMAL NOT NULL, total_signals INTEGER NOT NULL, successful_trades INTEGER NOT NULL, evaluation_period TSTZRANGE NOT NULL, metadata JSONB DEFAULT '{}', created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP ); -- Token Analytics CREATE TABLE token_analytics ( id UUID DEFAULT gen_random_uuid(), token_address TEXT NOT NULL, token_name TEXT NOT NULL, token_symbol TEXT NOT NULL, price DECIMAL NOT NULL, volume_24h DECIMAL, market_cap DECIMAL, total_supply DECIMAL, holder_count INTEGER, metadata JSONB DEFAULT '{}', timestamp TIMESTAMPTZ NOT NULL, created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (id, timestamp) ); -- Create hypertables SELECT create_hypertable('market_signals', 'timestamp', chunk_time_interval => INTERVAL '1 day', if_not_exists => TRUE, migrate_data => TRUE ); SELECT create_hypertable('trade_executions', 'execution_time', chunk_time_interval => INTERVAL '1 day', if_not_exists => TRUE, migrate_data => TRUE ); SELECT create_hypertable('token_analytics', 'timestamp', chunk_time_interval => INTERVAL '1 hour', if_not_exists => TRUE, migrate_data => TRUE ); -- Create indexes CREATE INDEX idx_market_signals_asset_time ON market_signals(asset_address, timestamp); CREATE INDEX idx_trade_executions_asset_time ON trade_executions(asset_address, execution_time); CREATE INDEX idx_token_analytics_address_time ON token_analytics(token_address, timestamp); -- Enable compression for market signals ALTER TABLE market_signals SET ( timescaledb.compress, timescaledb.compress_segmentby = 'asset_address,signal_type', timescaledb.compress_orderby = 'timestamp' ); -- Enable compression for trade executions ALTER TABLE trade_executions SET ( timescaledb.compress, timescaledb.compress_segmentby = 'asset_address,status', timescaledb.compress_orderby = 'execution_time' ); -- Enable compression for token analytics ALTER TABLE token_analytics SET ( timescaledb.compress, timescaledb.compress_segmentby = 'token_address', timescaledb.compress_orderby = 'timestamp' ); -- Create compression policies SELECT add_compression_policy('market_signals', INTERVAL '7 days'); SELECT add_compression_policy('trade_executions', INTERVAL '7 days'); SELECT add_compression_policy('token_analytics', INTERVAL '7 days'); -- Add retention policies SELECT add_retention_policy('market_signals', INTERVAL '1 year'); SELECT add_retention_policy('trade_executions', INTERVAL '1 year'); SELECT add_retention_policy('token_analytics', INTERVAL '1 year');
```

# migrations/01_mongodb_setup.rs

```rs
use crate::config::mongodb::{MongoConfig, MongoDbPool}; use anyhow::Result; use rig_mongodb::{bson::doc, MongoDbPool}; use tracing::info; #[tokio::main] async fn main() -> Result<()> { dotenvy::dotenv().ok(); info!("Starting MongoDB migrations..."); // Use migration-specific configuration let config = MongoConfig { pool: MongoPoolConfig { min_pool_size: 1, max_pool_size: 2, connect_timeout: std::time::Duration::from_secs(30), }, ..MongoConfig::from_env() }; let pool = config.create_pool().await?; let db = pool.database(&config.database); info!("Creating collections and indexes..."); // Token analytics collection db.create_collection("token_analytics", None).await?; db.collection("token_analytics") .create_index( doc! { "token_address": 1, "timestamp": -1 }, None, ) .await?; // Market signals collection db.create_collection("market_signals", None).await?; db.collection("market_signals") .create_index( doc! { "asset_address": 1, "timestamp": -1 }, None, ) .await?; // Vector store collection with improved search configuration db.create_collection("vectors", None).await?; db.collection("vectors") .create_index( doc! { "vector": "2dsphere", "metadata.timestamp": -1, "weights": { "vector": 1, "metadata.timestamp": 1 }, "name": "vector_search_idx", "background": true }, None, ) .await?; // Trade history collection db.create_collection("trade_history", None).await?; db.collection("trade_history") .create_index( doc! { "trader_address": 1, "timestamp": -1, "status": 1 }, None, ) .await?; // Risk models collection db.create_collection("risk_models", None).await?; db.collection("risk_models") .create_index( doc! { "model_type": 1, "asset_address": 1, "timestamp": -1 }, None, ) .await?; // Portfolio allocations collection db.create_collection("portfolio_allocations", None).await?; db.collection("portfolio_allocations") .create_index( doc! { "wallet_address": 1, "token_address": 1, "timestamp": -1 }, None, ) .await?; info!("MongoDB migrations completed successfully!"); Ok(()) }
```

# migrations/02_mongodb_schema.rs

```rs
use crate::config::mongodb::MongoConfig; use anyhow::Result; use rig_mongodb::{bson::doc, MongoDbPool}; use tracing::info; #[tokio::main] async fn main() -> Result<()> { dotenvy::dotenv().ok(); info!("Running MongoDB schema migration..."); let config = MongoConfig::from_env(); let pool = config.create_pool().await?; let db = pool.database(&config.database); // Create market signals collection with timeseries optimization db.create_collection( "market_signals", Some(doc! { "timeseries": { "timeField": "timestamp", "metaField": "asset_address", "granularity": "minutes" }, "validator": { "$jsonSchema": { "bsonType": "object", "required": ["asset_address", "signal_type", "confidence", "timestamp"], "properties": { "asset_address": { "bsonType": "string" }, "signal_type": { "enum": ["BUY", "SELL", "HOLD", "STRONG_BUY", "STRONG_SELL", "PRICE_SPIKE", "PRICE_DROP", "VOLUME_SURGE"] }, "confidence": { "bsonType": "decimal128" }, "price_change_24h": { "bsonType": "decimal128" }, "volume_change_24h": { "bsonType": "decimal128" }, "risk_score": { "bsonType": "decimal128" }, "metadata": { "bsonType": "object" }, "timestamp": { "bsonType": "date" } } } } }), ) .await?; // Create trade executions collection with timeseries db.create_collection( "trade_executions", Some(doc! { "timeseries": { "timeField": "execution_time", "metaField": "asset_address", "granularity": "minutes" }, "validator": { "$jsonSchema": { "bsonType": "object", "required": ["asset_address", "amount", "price", "status", "execution_time"], "properties": { "asset_address": { "bsonType": "string" }, "amount": { "bsonType": "decimal128" }, "price": { "bsonType": "decimal128" }, "status": { "enum": ["PENDING", "EXECUTED", "FAILED", "CANCELLED"] }, "tx_signature": { "bsonType": "string" }, "metadata": { "bsonType": "object" }, "execution_time": { "bsonType": "date" } } } } }), ) .await?; // Create token analytics collection with timeseries db.create_collection( "token_analytics", Some(doc! { "timeseries": { "timeField": "timestamp", "metaField": "token_address", "granularity": "minutes" }, "validator": { "$jsonSchema": { "bsonType": "object", "required": ["token_address", "token_name", "price", "timestamp"], "properties": { "token_address": { "bsonType": "string" }, "token_name": { "bsonType": "string" }, "token_symbol": { "bsonType": "string" }, "price": { "bsonType": "decimal128" }, "volume_24h": { "bsonType": "decimal128" }, "market_cap": { "bsonType": "decimal128" }, "holder_count": { "bsonType": "int" }, "metadata": { "bsonType": "object" }, "timestamp": { "bsonType": "date" } } } } }), ) .await?; // Create agent performance collection db.create_collection( "agent_performance", Some(doc! { "validator": { "$jsonSchema": { "bsonType": "object", "required": ["period_start", "period_end", "total_trades", "success_rate"], "properties": { "period_start": { "bsonType": "date" }, "period_end": { "bsonType": "date" }, "total_trades": { "bsonType": "int" }, "success_rate": { "bsonType": "decimal128" }, "pnl": { "bsonType": "decimal128" }, "metadata": { "bsonType": "object" } } } } }), ) .await?; // Create test collections with timeseries optimization db.create_collection( "test_market_signals", Some(doc! { "timeseries": { "timeField": "timestamp", "metaField": "asset_address", "granularity": "minutes" }, "validator": { "$jsonSchema": { "bsonType": "object", "required": ["asset_address", "signal_type", "confidence", "timestamp"], "properties": { "asset_address": { "bsonType": "string" }, "signal_type": { "enum": ["BUY", "SELL", "HOLD", "STRONG_BUY", "STRONG_SELL", "PRICE_SPIKE", "PRICE_DROP", "VOLUME_SURGE"] }, "confidence": { "bsonType": "decimal128" }, "price_change_24h": { "bsonType": "decimal128" }, "volume_change_24h": { "bsonType": "decimal128" }, "risk_score": { "bsonType": "decimal128" }, "metadata": { "bsonType": "object" }, "timestamp": { "bsonType": "date" } } } } }), ) .await?; // Create trade executions collection db.create_collection( "test_trade_executions", Some(doc! { "timeseries": { "timeField": "execution_time", "metaField": "asset_address", "granularity": "minutes" } }), ) .await?; // Create time-based indexes for efficient querying db.collection("market_signals") .create_index( doc! { "asset_address": 1, "timestamp": -1 }, None, ) .await?; db.collection("trade_executions") .create_index( doc! { "asset_address": 1, "execution_time": -1 }, None, ) .await?; db.collection("token_analytics") .create_index( doc! { "token_address": 1, "timestamp": -1 }, None, ) .await?; info!("MongoDB schema migration completed successfully!"); Ok(()) }
```

# migrations/02_trade_status.sql

```sql
-- migrations/02_trade_status.sql -- Drop trade_status type if exists and create custom ENUM type for trade_status DROP TYPE IF EXISTS trade_status CASCADE; CREATE TYPE trade_status AS ENUM ( 'open', 'closed', 'pending', 'executed', 'cancelled' );
```

# migrations/03_mongodb_trade_status.rs

```rs
use crate::config::mongodb::MongoConfig; use anyhow::Result; use rig_mongodb::{bson::doc, MongoDbPool}; use tracing::info; #[tokio::main] async fn main() -> Result<()> { dotenvy::dotenv().ok(); info!("Running trade status migration..."); let config = MongoConfig::from_env(); let pool = config.create_pool().await?; let db = pool.database(&config.database); // Create trade_history collection with status validation db.create_collection( "trade_history", Some(doc! { "validator": { "$jsonSchema": { "bsonType": "object", "required": ["trade_id", "status", "updated_at"], "properties": { "trade_id": { "bsonType": "string" }, "status": { "enum": [ "initiated", "pending", "completed", "failed", "cancelled", "timeout" ] }, "updated_at": { "bsonType": "date" }, "error": { "bsonType": "string" } } } } }), ) .await?; // Create indexes for efficient status tracking db.collection("trade_history") .create_index( doc! { "trade_id": 1, "updated_at": -1 }, None, ) .await?; db.collection("trade_history") .create_index( doc! { "status": 1, "updated_at": -1 }, None, ) .await?; info!("Trade status migration completed successfully!"); Ok(()) }
```

# migrations/03_position_allocations.sql

```sql
CREATE TABLE IF NOT EXISTS position_allocations ( id SERIAL PRIMARY KEY, token_address TEXT NOT NULL, allocation NUMERIC NOT NULL DEFAULT 0.0, created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(), updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW() ); CREATE INDEX IF NOT EXISTS position_allocations_token_address_idx ON position_allocations(token_address);
```

# migrations/04_mongodb_allocations.rs

```rs
use crate::config::mongodb::MongoConfig; use anyhow::Result; use rig_mongodb::{bson::doc, MongoDbPool}; use tracing::info; #[tokio::main] async fn main() -> Result<()> { dotenvy::dotenv().ok(); info!("Running position allocations migration..."); let config = MongoConfig::from_env(); let pool = config.create_pool().await?; let db = pool.database(&config.database); // Create portfolio_allocations collection with validation db.create_collection("portfolio_allocations", Some(doc! { "validator": { "$jsonSchema": { "bsonType": "object", "required": ["wallet_address", "token_address", "allocation_weight", "timestamp"], "properties": { "wallet_address": { "bsonType": "string" }, "token_address": { "bsonType": "string" }, "allocation_weight": { "bsonType": "decimal128" }, "target_weight": { "bsonType": "decimal128" }, "min_weight": { "bsonType": "decimal128" }, "max_weight": { "bsonType": "decimal128" }, "last_rebalance": { "bsonType": "date" }, "timestamp": { "bsonType": "date" } } } } })).await?; // Create indexes for efficient allocation lookups db.collection("portfolio_allocations") .create_index( doc! { "wallet_address": 1, "token_address": 1, "timestamp": -1 }, None, ) .await?; // Create rebalance_history collection db.create_collection("rebalance_history", Some(doc! { "validator": { "$jsonSchema": { "bsonType": "object", "required": ["wallet_address", "token_address", "old_weight", "new_weight", "timestamp"], "properties": { "wallet_address": { "bsonType": "string" }, "token_address": { "bsonType": "string" }, "old_weight": { "bsonType": "decimal128" }, "new_weight": { "bsonType": "decimal128" }, "reason": { "bsonType": "string" }, "timestamp": { "bsonType": "date" } } } } })).await?; // Create indexes for rebalance history db.collection("rebalance_history") .create_index( doc! { "wallet_address": 1, "timestamp": -1 }, None, ) .await?; info!("Position allocations migration completed successfully!"); Ok(()) }
```

# migrations/04_vector_store.sql

```sql
-- Enable pgvector extension CREATE EXTENSION IF NOT EXISTS vector; -- Create documents table for vector store CREATE TABLE IF NOT EXISTS documents ( id UUID PRIMARY KEY DEFAULT gen_random_uuid(), content TEXT NOT NULL, metadata JSONB DEFAULT '{}', embedding vector(1536), -- Using 1536 dimensions for OpenAI embeddings created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP ); -- Create index for vector similarity search CREATE INDEX IF NOT EXISTS documents_embedding_idx ON documents USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100); -- Create function to perform vector similarity search CREATE OR REPLACE FUNCTION vector_similarity_search( query_embedding vector, match_threshold float, match_count int ) RETURNS TABLE ( id UUID, content TEXT, metadata JSONB, similarity float ) LANGUAGE plpgsql AS $$ BEGIN RETURN QUERY SELECT d.id, d.content, d.metadata, 1 - (d.embedding <=> query_embedding) as similarity FROM documents d WHERE 1 - (d.embedding <=> query_embedding) > match_threshold ORDER BY d.embedding <=> query_embedding LIMIT match_count; END; $$;
```

# migrations/05_init_vector_store.sql

```sql
-- ensure PgVector extension is installed CREATE EXTENSION IF NOT EXISTS vector; -- Create table for market data CREATE TABLE market_data ( id uuid DEFAULT gen_random_uuid(), document jsonb NOT NULL, embedded_text text NOT NULL, embedding vector(1536) ); -- Create table for trade history CREATE TABLE trade_history ( id uuid DEFAULT gen_random_uuid(), document jsonb NOT NULL, embedded_text text NOT NULL, embedding vector(1536) ); -- Create table for risk models CREATE TABLE risk_models ( id uuid DEFAULT gen_random_uuid(), document jsonb NOT NULL, embedded_text text NOT NULL, embedding vector(1536) ); -- Create table for sentiment analysis CREATE TABLE sentiment_analysis ( id uuid DEFAULT gen_random_uuid(), document jsonb NOT NULL, embedded_text text NOT NULL, embedding vector(1536) ); -- Create HNSW indexes for cosine similarity search CREATE INDEX IF NOT EXISTS market_data_embeddings_idx ON market_data USING hnsw(embedding vector_cosine_ops); CREATE INDEX IF NOT EXISTS trade_history_embeddings_idx ON trade_history USING hnsw(embedding vector_cosine_ops); CREATE INDEX IF NOT EXISTS risk_models_embeddings_idx ON risk_models USING hnsw(embedding vector_cosine_ops); CREATE INDEX IF NOT EXISTS sentiment_analysis_embeddings_idx ON sentiment_analysis USING hnsw(embedding vector_cosine_ops);
```

# migrations/05_mongodb_vector_store.rs

```rs
use crate::config::mongodb::MongoConfig; use anyhow::Result; use rig_mongodb::{bson::doc, MongoDbPool}; use tracing::info; #[tokio::main] async fn main() -> Result<()> { dotenvy::dotenv().ok(); info!("Running vector store migration..."); let config = MongoConfig::from_env(); let pool = config.create_pool().await?; let db = pool.database(&config.database); // Create vector collections with proper schemas for different embedding types // Market data vectors db.create_collection( "market_data_vectors", Some(doc! { "validator": { "$jsonSchema": { "bsonType": "object", "required": ["document", "embedding", "timestamp"], "properties": { "document": { "bsonType": "object" }, "embedding": { "bsonType": "array" }, "metadata": { "bsonType": "object" }, "timestamp": { "bsonType": "date" } } } } }), ) .await?; // Trade history vectors db.create_collection( "trade_history_vectors", Some(doc! { "validator": { "$jsonSchema": { "bsonType": "object", "required": ["document", "embedding", "timestamp"], "properties": { "document": { "bsonType": "object" }, "embedding": { "bsonType": "array" }, "metadata": { "bsonType": "object" }, "timestamp": { "bsonType": "date" } } } } }), ) .await?; // Risk model vectors db.create_collection( "risk_model_vectors", Some(doc! { "validator": { "$jsonSchema": { "bsonType": "object", "required": ["document", "embedding", "timestamp"], "properties": { "document": { "bsonType": "object" }, "embedding": { "bsonType": "array" }, "metadata": { "bsonType": "object" }, "timestamp": { "bsonType": "date" } } } } }), ) .await?; // Sentiment analysis vectors db.create_collection( "sentiment_vectors", Some(doc! { "validator": { "$jsonSchema": { "bsonType": "object", "required": ["document", "embedding", "timestamp"], "properties": { "document": { "bsonType": "object" }, "embedding": { "bsonType": "array" }, "metadata": { "bsonType": "object" }, "timestamp": { "bsonType": "date" } } } } }), ) .await?; // Create vector search indexes for each collection let vector_search_options = doc! { "numDimensions": 1536, // OpenAI embedding dimensions "similarity": "cosine" }; // Market data vectors index db.run_command( doc! { "createSearchIndex": "market_data_vectors", "definition": { "mappings": { "dynamic": true, "fields": { "embedding": { "type": "knnVector", "dimensions": 1536, "similarity": "cosine" }, "timestamp": { "type": "date" } } } } }, None, ) .await?; // Trade history vectors index db.run_command( doc! { "createSearchIndex": "trade_history_vectors", "definition": { "mappings": { "dynamic": true, "fields": { "embedding": { "type": "knnVector", "dimensions": 1536, "similarity": "cosine" }, "timestamp": { "type": "date" } } } } }, None, ) .await?; // Risk model vectors index db.run_command( doc! { "createSearchIndex": "risk_model_vectors", "definition": { "mappings": { "dynamic": true, "fields": { "embedding": { "type": "knnVector", "dimensions": 1536, "similarity": "cosine" }, "timestamp": { "type": "date" } } } } }, None, ) .await?; // Sentiment vectors index db.run_command( doc! { "createSearchIndex": "sentiment_vectors", "definition": { "mappings": { "dynamic": true, "fields": { "embedding": { "type": "knnVector", "dimensions": 1536, "similarity": "cosine" }, "timestamp": { "type": "date" } } } } }, None, ) .await?; // Create regular indexes for metadata filtering for collection in [ "market_data_vectors", "trade_history_vectors", "risk_model_vectors", "sentiment_vectors", ] .iter() { db.collection(collection) .create_index( doc! { "metadata.token_address": 1, "timestamp": -1 }, None, ) .await?; } info!("Vector store migration completed successfully!"); Ok(()) }
```

# playground-1.mongodb.js

```js
/* global use, db */ // MongoDB Playground // To disable this template go to Settings | MongoDB | Use Default Template For Playground. // Make sure you are connected to enable completions and to be able to run a playground. // Use Ctrl+Space inside a snippet or a string literal to trigger completions. // The result of the last command run in a playground is shown on the results panel. // By default the first 20 documents will be returned with a cursor. // Use 'console.log()' to print to the debug output. // For more documentation on playgrounds please refer to // https://www.mongodb.com/docs/mongodb-vscode/playgrounds/ // Select the database to use. use("mongodbVSCodePlaygroundDB") // Insert a few documents into the sales collection. db.getCollection("sales").insertMany([ { item: "abc", price: 10, quantity: 2, date: new Date("2014-03-01T08:00:00Z"), }, { item: "jkl", price: 20, quantity: 1, date: new Date("2014-03-01T09:00:00Z"), }, { item: "xyz", price: 5, quantity: 10, date: new Date("2014-03-15T09:00:00Z"), }, { item: "xyz", price: 5, quantity: 20, date: new Date("2014-04-04T11:21:39.736Z"), }, { item: "abc", price: 10, quantity: 10, date: new Date("2014-04-04T21:23:13.331Z"), }, { item: "def", price: 7.5, quantity: 5, date: new Date("2015-06-04T05:08:13Z"), }, { item: "def", price: 7.5, quantity: 10, date: new Date("2015-09-10T08:43:00Z"), }, { item: "abc", price: 10, quantity: 5, date: new Date("2016-02-06T20:20:13Z"), }, ]) // Run a find command to view items sold on April 4th, 2014. const salesOnApril4th = db .getCollection("sales") .find({ date: { $gte: new Date("2014-04-04"), $lt: new Date("2014-04-05") }, }) .count() // Print a message to the output window. console.log(`${salesOnApril4th} sales occurred in 2014.`) // Here we run an aggregation and open a cursor to the results. // Use '.toArray()' to exhaust the cursor to return the whole result set. // You can use '.hasNext()/.next()' to iterate through the cursor page by page. db.getCollection("sales").aggregate([ // Find all of the sales that occurred in 2014. { $match: { date: { $gte: new Date("2014-01-01"), $lt: new Date("2015-01-01") }, }, }, // Group the total sales for each product. { $group: { _id: "$item", totalSaleAmount: { $sum: { $multiply: ["$price", "$quantity"] } }, }, }, ]) // Replace with your actual database and collection names use cainam; db.token_analytics.aggregate([ { $searchIndex: { name: "vector_index", // The name of your index definition: { mappings: { dynamic: false, // Good practice for vector search fields: { embedding: { type: "vector", dimensions: 1536, similarity: "cosine", }, }, }, }, }, }, { $limit: 1, // Add a limit stage to avoid processing all documents }, ]); // To actually CREATE the index, you need to run this command: db.runCommand({ createSearchIndexes: "token_analytics", indexes: [ { name: "vector_index", definition: { mappings: { dynamic: false, fields: { embedding: { type: "vector", dimensions: 1536, similarity: "cosine", }, }, }, }, }, ], });
```

# plugins/birdeye/Cargo.toml

```toml
[package] name = "cainam-birdeye" version = "0.1.0" edition = "2021" description = "Birdeye client for DeFi and token analytics" license = "MIT" [dependencies] rig-core = "0.7.0" tokio = { version = "1.0", features = ["full"] } serde = { version = "1.0", features = ["derive"] } serde_json = "1.0" reqwest = { version = "0.11", features = ["json"] } async-trait = "0.1" anyhow = "1.0" thiserror = "1.0" tracing = "0.1" chrono = "0.4" base64 = "0.21" bs58 = "0.5" url = "2.4" hex = "0.4" tokio-tungstenite = { version = "0.20", features = ["native-tls"] } futures-util = "0.3" [dev-dependencies] tokio = { version = "1.0", features = ["full", "test-util", "macros"] } tracing-subscriber = { version = "0.3", features = ["env-filter"] } pretty_assertions = "1.0" # Environment variables dotenvy = "0.15.7" mockall = "0.11" [lib] name = "cainam_birdeye" path = "src/lib.rs"
```

# plugins/birdeye/README.md

```md
# cainam-birdeye A Birdeye plugin for rig-core that provides token and wallet analytics on Solana. ## Features - Token search with sorting and filtering options - Wallet portfolio analysis - Token price history and market data - Token security analysis - Support for multiple time intervals ## Installation Add this to your `Cargo.toml`: \`\`\`toml [dependencies] cainam-birdeye = "0.1.0" \`\`\` ## Configuration The plugin requires a Birdeye API key. You can obtain one from [Birdeye](https://birdeye.so). Set the API key in your environment: \`\`\`bash export BIRDEYE_API_KEY=your_api_key_here \`\`\` Or provide it through your rig-core configuration. ## Usage ### Token Search \`\`\`rust use rig_birdeye::{TokenSearchAction, TokenSortBy, SortType}; let action = TokenSearchAction { keyword: "SOL".to_string(), sort_by: Some(TokenSortBy::volume_24h), sort_type: Some(SortType::Desc), limit: Some(10), }; let result = agent.execute(action).await?; \`\`\` ### Wallet Portfolio \`\`\`rust use rig_birdeye::WalletSearchAction; let action = WalletSearchAction { wallet: "wallet_address_here".to_string(), }; let portfolio = agent.execute(action).await?; \`\`\` ## Actions - `TokenSearchAction`: Search for tokens with various sorting options - `WalletSearchAction`: Get wallet portfolio information ## Types ### TokenMarketData \`\`\`rust pub struct TokenMarketData { pub address: String, pub symbol: String, pub name: String, pub decimals: u8, pub price: f64, pub price_change_24h: f64, pub volume_24h: f64, pub market_cap: Option<f64>, } \`\`\` ### WalletPortfolio \`\`\`rust pub struct WalletPortfolio { pub wallet: String, pub total_usd: f64, pub items: Vec<WalletToken>, } \`\`\` ## Error Handling The plugin uses custom error types that implement `std::error::Error`: \`\`\`rust pub enum BirdeyeError { RequestError(reqwest::Error), ApiError { status_code: u16, message: String }, RateLimitExceeded, InvalidApiKey, // ... } \`\`\` ## License This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
```

# plugins/birdeye/src/actions/token_search.rs

```rs
use async_trait::async_trait; use rig::{Action, ActionContext, ActionResult}; use serde::{Deserialize, Serialize}; use crate::{providers::BirdeyeProvider, types::*}; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TokenSearchAction { pub keyword: String, #[serde(skip_serializing_if = "Option::is_none")] pub sort_by: Option<TokenSortBy>, #[serde(skip_serializing_if = "Option::is_none")] pub sort_type: Option<SortType>, #[serde(skip_serializing_if = "Option::is_none")] pub limit: Option<u32>, } #[async_trait] impl Action for TokenSearchAction { type Output = Vec<TokenMarketData>; type Error = BirdeyeError; fn name(&self) -> &'static str { "token_search" } fn description(&self) -> &'static str { "Search for tokens on Solana using Birdeye" } async fn execute(&self, ctx: &ActionContext) -> ActionResult<Self::Output, Self::Error> { let api_key = ctx.get_secret("BIRDEYE_API_KEY") .ok_or_else(|| BirdeyeError::InvalidApiKey)?; let provider = BirdeyeProvider::new(api_key); let params = TokenSearchParams { keyword: self.keyword.clone(), sort_by: self.sort_by, sort_type: self.sort_type, offset: None, limit: self.limit, }; let tokens = provider.search_tokens(params).await?; Ok(tokens) } }
```

# plugins/birdeye/src/actions/wallet_search.rs

```rs
use async_trait::async_trait; use rig::{Action, ActionContext, ActionResult}; use serde::{Deserialize, Serialize}; use crate::{providers::BirdeyeProvider, types::*}; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct WalletSearchAction { pub wallet: String, } #[async_trait] impl Action for WalletSearchAction { type Output = WalletPortfolio; type Error = BirdeyeError; fn name(&self) -> &'static str { "wallet_search" } fn description(&self) -> &'static str { "Get wallet portfolio information from Birdeye" } async fn execute(&self, ctx: &ActionContext) -> ActionResult<Self::Output, Self::Error> { let api_key = ctx.get_secret("BIRDEYE_API_KEY") .ok_or_else(|| BirdeyeError::InvalidApiKey)?; let provider = BirdeyeProvider::new(api_key); let portfolio = provider.get_wallet_portfolio(&self.wallet).await?; Ok(portfolio) } }
```

# plugins/birdeye/src/lib.rs

```rs
pub mod providers; pub mod types; use crate::providers::{birdeye::BirdeyeProvider, cache::CachedClient}; use crate::types::{ api::{ LiquidityAnalysis, MarketImpact, PricePoint, TokenInfo, TokenOverview, TokenSearchParams, WalletPortfolio, }, error::BirdeyeError, TimeInterval, }; use anyhow::Result; use std::sync::Arc; pub struct BirdeyeClient { provider: Arc<CachedClient<BirdeyeProvider>>, } impl BirdeyeClient { pub fn new(api_key: String) -> Self { let provider = BirdeyeProvider::new(api_key); Self { provider: Arc::new(CachedClient::new(provider)), } } pub async fn search_tokens( &self, params: TokenSearchParams, ) -> Result<Vec<TokenInfo>, BirdeyeError> { let cache_key = format!("search_tokens_{:?}", params); let params = params.clone(); self.provider .execute( &cache_key, move |provider| async move { provider.search_tokens(params).await }, 60, ) .await } pub async fn get_token_overview(&self, address: String) -> Result<TokenOverview, BirdeyeError> { let cache_key = format!("overview:{}", address); let address = address.clone(); self.provider .execute( &cache_key, move |provider| async move { provider.get_token_overview(address).await }, 60, ) .await } pub async fn analyze_liquidity( &self, address: String, ) -> Result<LiquidityAnalysis, BirdeyeError> { let cache_key = format!("liquidity:{}", address); let address = address.clone(); self.provider .execute( &cache_key, move |provider| async move { provider.analyze_liquidity(&address).await }, 60, ) .await } pub async fn get_market_impact( &self, address: String, size_usd: f64, ) -> Result<MarketImpact, BirdeyeError> { let cache_key = format!("impact:{}:{}", address, size_usd); let address = address.clone(); self.provider .execute( &cache_key, move |provider| async move { provider.get_market_impact(&address, size_usd).await }, 60, ) .await } pub async fn get_price_history( &self, address: String, interval: TimeInterval, ) -> Result<Vec<PricePoint>, BirdeyeError> { let cache_key = format!("history:{}:{}", address, interval); let address = address.clone(); self.provider .execute( &cache_key, move |provider| async move { provider.get_price_history(&address, interval).await }, 60, ) .await } pub async fn get_wallet_portfolio( &self, wallet_address: String, ) -> Result<WalletPortfolio, BirdeyeError> { let cache_key = format!("portfolio:{}", wallet_address); let address = wallet_address.clone(); self.provider .execute( &cache_key, move |provider| async move { provider.get_wallet_portfolio(&address).await }, 60, ) .await } }
```

# plugins/birdeye/src/providers/birdeye.rs

```rs
use crate::types::{ api::{ LiquidityAnalysis, MarketImpact, PricePoint, TokenInfo, TokenOverview, TokenSearchParams, WalletPortfolio, }, error::BirdeyeError, TimeInterval, }; use reqwest::Client; use serde::Deserialize; use std::time::Duration; const API_BASE_URL: &str = "https://public-api.birdeye.so"; const DEFAULT_TIMEOUT: Duration = Duration::from_secs(30); #[derive(Debug, Deserialize)] struct TokenSearchResponse { data: Vec<TokenInfo>, } #[derive(Debug, Clone)] pub struct BirdeyeProvider { client: Client, api_key: String, } impl BirdeyeProvider { pub fn new(api_key: String) -> Self { Self { client: Client::builder() .timeout(DEFAULT_TIMEOUT) .build() .unwrap_or_default(), api_key, } } pub async fn search_tokens( &self, params: TokenSearchParams, ) -> Result<Vec<TokenInfo>, BirdeyeError> { let url = format!("{}/search/tokens", API_BASE_URL); let response = self .client .get(&url) .header("X-API-KEY", &self.api_key) .query(&[("query", &params.query)]) .send() .await .map_err(|e| BirdeyeError::RequestError(e.to_string()))?; let data = response .json::<TokenSearchResponse>() .await .map_err(|e| BirdeyeError::SerializationError(e.to_string()))?; Ok(data.data) } pub async fn get_token_overview(&self, address: String) -> Result<TokenOverview, BirdeyeError> { let url = format!("{}/token/{}/overview", API_BASE_URL, address); let response = self .client .get(&url) .header("X-API-KEY", &self.api_key) .send() .await .map_err(|e| BirdeyeError::RequestError(e.to_string()))?; response .json() .await .map_err(|e| BirdeyeError::SerializationError(e.to_string())) } pub async fn analyze_liquidity( &self, address: &str, ) -> Result<LiquidityAnalysis, BirdeyeError> { let url = format!("{}/token/{}/liquidity", API_BASE_URL, address); let response = self .client .get(&url) .header("X-API-KEY", &self.api_key) .send() .await .map_err(|e| BirdeyeError::RequestError(e.to_string()))?; response .json() .await .map_err(|e| BirdeyeError::SerializationError(e.to_string())) } pub async fn get_market_impact( &self, address: &str, size_usd: f64, ) -> Result<MarketImpact, BirdeyeError> { let url = format!("{}/token/{}/impact", API_BASE_URL, address); let response = self .client .get(&url) .header("X-API-KEY", &self.api_key) .query(&[("size_usd", size_usd.to_string())]) .send() .await .map_err(|e| BirdeyeError::RequestError(e.to_string()))?; response .json() .await .map_err(|e| BirdeyeError::SerializationError(e.to_string())) } pub async fn get_price_history( &self, address: &str, interval: TimeInterval, ) -> Result<Vec<PricePoint>, BirdeyeError> { let url = format!("{}/token/{}/price", API_BASE_URL, address); let response = self .client .get(&url) .header("X-API-KEY", &self.api_key) .query(&[("interval", interval.to_string())]) .send() .await .map_err(|e| BirdeyeError::RequestError(e.to_string()))?; response .json() .await .map_err(|e| BirdeyeError::SerializationError(e.to_string())) } pub async fn get_wallet_portfolio( &self, wallet_address: &str, ) -> Result<WalletPortfolio, BirdeyeError> { let url = format!("{}/wallet/{}/portfolio", API_BASE_URL, wallet_address); let response = self .client .get(&url) .header("X-API-KEY", &self.api_key) .send() .await .map_err(|e| BirdeyeError::RequestError(e.to_string()))?; response .json() .await .map_err(|e| BirdeyeError::SerializationError(e.to_string())) } } // Implement Send and Sync for BirdeyeProvider since reqwest::Client is already Send + Sync unsafe impl Send for BirdeyeProvider {} unsafe impl Sync for BirdeyeProvider {} #[cfg(test)] mod tests { use super::*; use std::env; #[tokio::test] async fn test_search_tokens() -> Result<(), BirdeyeError> { let api_key = env::var("BIRDEYE_API_KEY").expect("BIRDEYE_API_KEY must be set"); let provider = BirdeyeProvider::new(api_key); let params = TokenSearchParams { query: "SOL".to_string(), offset: None, limit: Some(10), }; let response = provider.search_tokens(params).await?; assert!(!response.is_empty()); Ok(()) } #[tokio::test] async fn test_get_token_overview() -> Result<(), BirdeyeError> { let api_key = env::var("BIRDEYE_API_KEY").expect("BIRDEYE_API_KEY must be set"); let provider = BirdeyeProvider::new(api_key); let overview = provider .get_token_overview("So11111111111111111111111111111111111111112".to_string()) .await?; assert!(overview.price > 0.0); Ok(()) } }
```

# plugins/birdeye/src/providers/cache.rs

```rs
use serde::{de::DeserializeOwned, Serialize}; use std::future::Future; use std::{ collections::HashMap, sync::Arc, time::{Duration, Instant}, }; use tokio::sync::RwLock; #[derive(Debug)] struct CacheEntry<T> { data: T, expires_at: Instant, } impl<T> CacheEntry<T> { fn new(data: T, ttl: Duration) -> Self { Self { data, expires_at: Instant::now() + ttl, } } fn is_expired(&self) -> bool { Instant::now() > self.expires_at } } #[derive(Debug)] struct CacheState<T> { inner: Arc<T>, cache: HashMap<String, CacheEntry<Vec<u8>>>, } #[derive(Debug, Clone)] pub struct CachedClient<T> { state: Arc<RwLock<CacheState<T>>>, } impl<T: Send + Sync + 'static> CachedClient<T> { pub fn new(inner: T) -> Self { Self { state: Arc::new(RwLock::new(CacheState { inner: Arc::new(inner), cache: HashMap::new(), })), } } pub async fn execute<F, Fut, R>(&self, key: &str, f: F, ttl: u64) -> R where F: FnOnce(Arc<T>) -> Fut + Send + 'static, Fut: Future<Output = R> + Send, R: DeserializeOwned + Serialize + Send + 'static, { // First try to get from cache with a read lock { let state = self.state.read().await; if let Some(entry) = state.cache.get(key) { if !entry.is_expired() { if let Ok(data) = serde_json::from_slice(&entry.data) { return data; } } } } // Get the inner reference let inner = Arc::clone(&self.state.read().await.inner); // Execute the function let result = f(inner).await; // Cache the result with a write lock if let Ok(data) = serde_json::to_vec(&result) { let mut state = self.state.write().await; state.cache.insert( key.to_string(), CacheEntry::new(data, Duration::from_secs(ttl)), ); } result } }
```

# plugins/birdeye/src/providers/mock.rs

```rs
use crate::{ types::api::TokenInfo, LiquidityAnalysis, MarketImpact, PricePoint, TokenOverview, WalletPortfolio, }; use serde_json::json; use std::{collections::HashMap, sync::Arc, time::Duration}; use tokio::sync::RwLock; /// Mock response data for testing #[derive(Clone)] pub struct MockResponse { pub status: u16, pub body: serde_json::Value, pub delay: Duration, } /// Mock HTTP client for testing pub struct MockHttpClient { responses: Arc<RwLock<HashMap<String, MockResponse>>>, } impl MockHttpClient { pub fn new() -> Self { let mut responses = HashMap::new(); // Add default mock responses responses.insert( "/token/search".to_string(), MockResponse { status: 200, body: json!({ "data": [ { "address": "So11111111111111111111111111111111111111112", "symbol": "SOL", "name": "Wrapped SOL", "decimals": 9, "price_usd": 100.0, "volume_24h": 1000000.0, "market_cap": 10000000000.0 } ], "pagination": { "offset": 0, "limit": 10, "total": 1 } }), delay: Duration::from_millis(100), }, ); responses.insert( "/token/overview".to_string(), MockResponse { status: 200, body: json!({ "address": "So11111111111111111111111111111111111111112", "decimals": 9, "symbol": "SOL", "name": "Wrapped SOL", "price": 100.0, "volume_24h": 1000000.0, "price_change_24h": 5.0, "liquidity": 500000.0, "holders": 1000000 }), delay: Duration::from_millis(50), }, ); Self { responses: Arc::new(RwLock::new(responses)), } } /// Add or update a mock response pub async fn set_response(&self, endpoint: &str, response: MockResponse) { let mut responses = self.responses.write().await; responses.insert(endpoint.to_string(), response); } /// Simulate a rate limit error pub async fn simulate_rate_limit(&self, endpoint: &str) { self.set_response( endpoint, MockResponse { status: 429, body: json!({"error": "Rate limit exceeded"}), delay: Duration::from_millis(50), }, ) .await; } /// Simulate a network error pub async fn simulate_network_error(&self, endpoint: &str) { self.set_response( endpoint, MockResponse { status: 500, body: json!({"error": "Internal server error"}), delay: Duration::from_millis(50), }, ) .await; } /// Get mock response for an endpoint pub async fn get(&self, endpoint: &str) -> Option<MockResponse> { let responses = self.responses.read().await; responses.get(endpoint).cloned() } } /// Create test data for various response types pub fn create_test_token_info() -> TokenInfo { TokenInfo { address: "So11111111111111111111111111111111111111112".to_string(), symbol: "SOL".to_string(), name: "Wrapped SOL".to_string(), decimals: 9, price: Some(100.0), volume_24h: Some(1000000.0), market_cap: Some(10000000000.0), } } pub fn create_test_token_overview() -> TokenOverview { TokenOverview { address: "So11111111111111111111111111111111111111112".to_string(), symbol: "SOL".to_string(), name: "Wrapped SOL".to_string(), decimals: 9, price: 100.0, volume_24h: 1000000.0, market_cap: 10000000000.0, fully_diluted_market_cap: Some(12000000000.0), total_supply: 100000000.0, circulating_supply: Some(80000000.0), } } pub fn create_test_wallet_portfolio() -> WalletPortfolio { WalletPortfolio { wallet_address: "DYw8jCTfwHNRJhhmFcbXvVDTqWMEVFBX6ZKUmG5CNSKK".to_string(), total_value_usd: 1000000.0, tokens: vec![], } } pub fn create_test_liquidity_analysis() -> LiquidityAnalysis { LiquidityAnalysis { total_bid_liquidity: 250000.0, total_ask_liquidity: 250000.0, bid_ask_ratio: 1.0, depth_quality: 0.8, } } pub fn create_test_market_impact() -> MarketImpact { MarketImpact { price_impact: 0.01, executed_price: 101.0, size_usd: 10000.0, size_tokens: 100.0, } } pub fn create_test_price_history() -> Vec<PricePoint> { vec![ PricePoint { timestamp: chrono::Utc::now().timestamp(), price: 100.0, volume: 100000.0, }, PricePoint { timestamp: chrono::Utc::now().timestamp() - 3600, price: 99.0, volume: 95000.0, }, ] } #[cfg(test)] mod tests { use super::*; use tokio::time::Instant; #[tokio::test] async fn test_mock_client_basic() { let client = MockHttpClient::new(); let response = client.get("/token/search").await.unwrap(); assert_eq!(response.status, 200); } #[tokio::test] async fn test_mock_client_rate_limit() { let client = MockHttpClient::new(); client.simulate_rate_limit("/token/search").await; let response = client.get("/token/search").await.unwrap(); assert_eq!(response.status, 429); } #[tokio::test] async fn test_mock_client_delay() { let client = MockHttpClient::new(); let start = Instant::now(); let response = client.get("/token/search").await.unwrap(); assert!(start.elapsed() >= response.delay); } #[tokio::test] async fn test_mock_client_custom_response() { let client = MockHttpClient::new(); client .set_response( "/custom", MockResponse { status: 200, body: json!({"test": true}), delay: Duration::from_millis(0), }, ) .await; let response = client.get("/custom").await.unwrap(); assert_eq!(response.status, 200); assert_eq!(response.body, json!({"test": true})); } }
```

# plugins/birdeye/src/providers/mod.rs

```rs
pub mod birdeye; pub mod cache; pub mod mock; pub mod pagination; pub mod rate_limiter; pub mod websocket; pub use birdeye::*; pub use cache::*; pub use mock::*; pub use pagination::*; pub use rate_limiter::*; pub use websocket::{MarketUpdate, TradeUpdate, WebSocketProvider}; // Re-export types from the types module pub use crate::types::{ api::{LiquidityAnalysis, MarketImpact, PricePoint, TokenInfo, TokenOverview}, TimeInterval, };
```

# plugins/birdeye/src/providers/pagination.rs

```rs
use crate::types::error::BirdeyeError; use std::future::Future; use std::marker::PhantomData; use std::pin::Pin; #[derive(Debug, Clone)] pub struct PaginationParams { pub offset: Option<u32>, pub limit: Option<u32>, } impl PaginationParams { pub fn new(offset: Option<u32>, limit: Option<u32>) -> Self { Self { offset, limit } } } impl Default for PaginationParams { fn default() -> Self { Self { offset: None, limit: Some(10), } } } pub trait PaginatedRequest<T>: Send + Sync { fn execute( &self, offset: u32, limit: u32, ) -> Pin<Box<dyn Future<Output = Result<Vec<T>, BirdeyeError>> + Send>>; } impl<F, Fut, T> PaginatedRequest<T> for F where F: Fn(u32, u32) -> Fut + Send + Sync, Fut: Future<Output = Result<Vec<T>, BirdeyeError>> + Send + 'static, { fn execute( &self, offset: u32, limit: u32, ) -> Pin<Box<dyn Future<Output = Result<Vec<T>, BirdeyeError>> + Send>> { Box::pin(self(offset, limit)) } } pub struct PaginatedIterator<T, R> where T: Send, R: PaginatedRequest<T>, { request: R, page_size: u32, current_offset: u32, total_items: Option<u32>, _phantom: PhantomData<T>, } impl<T, R> PaginatedIterator<T, R> where T: Send, R: PaginatedRequest<T>, { pub fn new(request: R, page_size: u32) -> Self { Self { request, page_size, current_offset: 0, total_items: None, _phantom: PhantomData, } } pub async fn next_page(&mut self) -> Option<Result<Vec<T>, BirdeyeError>> { if let Some(total) = self.total_items { if self.current_offset >= total { return None; } } let result = self .request .execute(self.current_offset, self.page_size) .await; match result { Ok(items) => { if items.is_empty() { None } else { self.current_offset += items.len() as u32; Some(Ok(items)) } } Err(e) => Some(Err(e)), } } pub async fn collect_all(mut self) -> Result<Vec<T>, BirdeyeError> { let mut all_items = Vec::new(); while let Some(result) = self.next_page().await { all_items.extend(result?); } Ok(all_items) } }
```

# plugins/birdeye/src/providers/rate_limiter.rs

```rs
use std::time::{Duration, Instant}; use tokio::sync::Mutex; #[derive(Debug)] struct RateLimiterState { tokens: f64, last_update: Instant, rate: f64, burst: f64, } #[derive(Debug)] pub struct RateLimiter { state: Mutex<RateLimiterState>, } impl RateLimiter { pub fn new(rate: f64, burst: f64) -> Self { Self { state: Mutex::new(RateLimiterState { tokens: burst, last_update: Instant::now(), rate, burst, }), } } pub async fn acquire(&self) -> Duration { let mut state = self.state.lock().await; let now = Instant::now(); let elapsed = now.duration_since(state.last_update).as_secs_f64(); // Add new tokens based on elapsed time state.tokens = (state.tokens + elapsed * state.rate).min(state.burst); state.last_update = now; if state.tokens >= 1.0 { state.tokens -= 1.0; Duration::from_secs(0) } else { let wait_time = (1.0 - state.tokens) / state.rate; state.tokens = 0.0; Duration::from_secs_f64(wait_time) } } }
```

# plugins/birdeye/src/providers/websocket.rs

```rs
use crate::types::error::BirdeyeError; use futures_util::{SinkExt, StreamExt}; use serde::{Deserialize, Serialize}; use tokio::sync::broadcast; use tokio_tungstenite::{connect_async, tungstenite::Message}; const WEBSOCKET_URL: &str = "wss://public-api.birdeye.so/socket"; #[derive(Debug, Clone, Serialize, Deserialize)] #[serde(rename_all = "camelCase")] pub struct MarketUpdate { pub address: String, pub price: f64, pub volume_24h: f64, pub price_change_24h: f64, pub timestamp: i64, } #[derive(Debug, Clone, Serialize, Deserialize)] #[serde(rename_all = "camelCase")] pub struct TradeUpdate { pub address: String, pub price: f64, pub size: f64, pub side: TradeSide, pub timestamp: i64, } #[derive(Debug, Clone, Serialize, Deserialize)] #[serde(rename_all = "lowercase")] pub enum TradeSide { Buy, Sell, } #[derive(Debug, Clone, Serialize)] #[serde(rename_all = "camelCase")] struct SubscribeMessage { action: String, token: String, api_key: String, } #[derive(Debug)] pub struct WebSocketProvider { api_key: String, market_sender: broadcast::Sender<MarketUpdate>, trade_sender: broadcast::Sender<TradeUpdate>, } impl WebSocketProvider { pub fn new(api_key: &str) -> Self { let (market_sender, _) = broadcast::channel(1000); let (trade_sender, _) = broadcast::channel(1000); Self { api_key: api_key.to_string(), market_sender, trade_sender, } } pub fn subscribe_market_updates(&self) -> broadcast::Receiver<MarketUpdate> { self.market_sender.subscribe() } pub fn subscribe_trade_updates(&self) -> broadcast::Receiver<TradeUpdate> { self.trade_sender.subscribe() } pub async fn connect_and_stream(&self, tokens: Vec<String>) -> Result<(), BirdeyeError> { let url = format!("{}?apiKey={}", WEBSOCKET_URL, self.api_key); let (ws_stream, _) = connect_async(&url) .await .map_err(|e| BirdeyeError::WebSocketError(e.to_string()))?; let (mut write, mut read) = ws_stream.split(); // Subscribe to tokens for token in tokens { let subscribe_msg = SubscribeMessage { action: "subscribe".to_string(), token, api_key: self.api_key.clone(), }; let msg = serde_json::to_string(&subscribe_msg) .map_err(|e| BirdeyeError::SerializationError(e.to_string()))?; write .send(Message::Text(msg)) .await .map_err(|e| BirdeyeError::WebSocketError(e.to_string()))?; } let market_sender = self.market_sender.clone(); let trade_sender = self.trade_sender.clone(); // Spawn message handling task tokio::spawn(async move { while let Some(msg) = read.next().await { match msg { Ok(Message::Text(text)) => { if let Ok(market_update) = serde_json::from_str::<MarketUpdate>(&text) { let _ = market_sender.send(market_update); } else if let Ok(trade_update) = serde_json::from_str::<TradeUpdate>(&text) { let _ = trade_sender.send(trade_update); } } Ok(Message::Close(_)) => break, Err(e) => { eprintln!("WebSocket error: {}", e); break; } _ => {} } } }); Ok(()) } } // Example usage: // // let ws_provider = WebSocketProvider::new("your-api-key"); // // // Subscribe to market updates // let mut market_rx = ws_provider.subscribe_market_updates(); // tokio::spawn(async move { // while let Ok(update) = market_rx.recv().await { // println!("Market update: {:?}", update); // } // }); // // // Subscribe to trade updates // let mut trade_rx = ws_provider.subscribe_trade_updates(); // tokio::spawn(async move { // while let Ok(update) = trade_rx.recv().await { // println!("Trade update: {:?}", update); // } // }); // // // Connect and start streaming // ws_provider.connect_and_stream(vec!["token-address-1".to_string(), "token-address-2".to_string()]).await?;
```

# plugins/birdeye/src/types/api.rs

```rs
use super::TimeInterval; use serde::{Deserialize, Serialize}; use std::fmt; // Token Search Types #[derive(Debug, Serialize, Deserialize, Clone)] pub struct TokenSearchParams { pub query: String, pub limit: Option<u32>, pub offset: Option<u32>, } impl TokenSearchParams { pub fn new(keyword: String) -> Self { Self { query: keyword, limit: None, offset: None, } } pub fn with_limit(mut self, limit: u32) -> Self { self.limit = Some(limit); self } pub fn with_offset(mut self, offset: u32) -> Self { self.offset = Some(offset); self } pub fn offset(&self) -> Option<u32> { self.offset } pub fn limit(&self) -> Option<u32> { self.limit } } #[derive(Debug, Clone, Copy, Serialize, Deserialize)] #[serde(rename_all = "lowercase")] pub enum TokenSortBy { #[serde(rename = "price")] Price, #[serde(rename = "volume")] Volume, #[serde(rename = "liquidity")] Liquidity, #[serde(rename = "price_change")] PriceChange, } impl fmt::Display for TokenSortBy { fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result { match self { TokenSortBy::Price => write!(f, "price"), TokenSortBy::Volume => write!(f, "volume"), TokenSortBy::Liquidity => write!(f, "liquidity"), TokenSortBy::PriceChange => write!(f, "price_change"), } } } #[derive(Debug, Clone, Copy, Serialize, Deserialize)] #[serde(rename_all = "lowercase")] pub enum SortType { #[serde(rename = "asc")] Ascending, #[serde(rename = "desc")] Descending, } impl fmt::Display for SortType { fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result { match self { SortType::Ascending => write!(f, "asc"), SortType::Descending => write!(f, "desc"), } } } // Token Market Data Types #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TokenMarketData { pub address: String, pub symbol: String, pub name: String, pub decimals: u8, pub price: f64, pub price_change_24h: f64, pub volume_24h: f64, pub market_cap: Option<f64>, pub fully_diluted_valuation: Option<f64>, pub circulating_supply: Option<f64>, pub total_supply: Option<f64>, } // Wallet Portfolio Types #[derive(Debug, Serialize, Deserialize, Clone)] pub struct WalletPortfolio { pub wallet_address: String, pub total_value_usd: f64, pub tokens: Vec<TokenBalance>, } #[derive(Debug, Serialize, Deserialize, Clone)] pub struct TokenBalance { pub token_address: String, pub symbol: String, pub amount: f64, pub value_usd: f64, } // Price History Types #[derive(Debug, Clone, Serialize, Deserialize)] pub struct PriceHistoryParams { pub address: String, pub interval: TimeInterval, pub time_from: Option<i64>, pub time_to: Option<i64>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct PriceHistoryPoint { pub timestamp: i64, pub price: f64, pub volume: f64, } // Token Security Types #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TokenSecurity { pub address: String, pub total_supply: f64, pub mintable: bool, pub proxied: bool, pub owner_address: String, pub creator_address: String, pub security_checks: SecurityChecks, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct SecurityChecks { pub honeypot: bool, pub trading_cooldown: bool, pub transfer_pausable: bool, pub is_blacklisted: bool, pub is_whitelisted: bool, pub is_proxy: bool, pub is_mintable: bool, pub can_take_back_ownership: bool, pub hidden_owner: bool, pub anti_whale_modifiable: bool, pub is_anti_whale: bool, pub trading_pausable: bool, pub can_be_blacklisted: bool, pub is_true_token: bool, pub is_airdrop_scam: bool, pub slippage_modifiable: bool, pub is_honeypot: bool, pub transfer_pausable_time: bool, pub is_wrapped: bool, } // Token Overview Types #[derive(Debug, Serialize, Deserialize, Clone)] pub struct TokenOverview { pub address: String, pub symbol: String, pub name: String, pub decimals: u8, pub price: f64, pub volume_24h: f64, pub market_cap: f64, pub fully_diluted_market_cap: Option<f64>, pub total_supply: f64, pub circulating_supply: Option<f64>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TokenInfo { pub address: String, pub symbol: String, pub name: String, pub decimals: u8, pub price: Option<f64>, pub volume_24h: Option<f64>, pub market_cap: Option<f64>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct LiquidityAnalysis { pub total_bid_liquidity: f64, pub total_ask_liquidity: f64, pub bid_ask_ratio: f64, pub depth_quality: f64, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MarketImpact { pub price_impact: f64, pub executed_price: f64, pub size_usd: f64, pub size_tokens: f64, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct PricePoint { pub timestamp: i64, pub price: f64, pub volume: f64, } #[derive(Debug, Serialize, Deserialize)] pub struct PaginatedResponse<T> { pub data: Vec<T>, pub total: u32, pub offset: u32, pub limit: u32, }
```

# plugins/birdeye/src/types/error.rs

```rs
use serde::{Deserialize, Serialize}; use thiserror::Error; #[derive(Error, Debug, Serialize, Deserialize)] pub enum BirdeyeError { #[error("HTTP request failed: {0}")] RequestError(String), #[error("Serialization error: {0}")] SerializationError(String), #[error("WebSocket error: {0}")] WebSocketError(String), #[error("Rate limit exceeded")] RateLimitExceeded, #[error("Invalid response: {0}")] InvalidResponse(String), #[error("Cache error: {0}")] CacheError(String), #[error("Invalid time interval: {0}")] InvalidTimeInterval(String), #[error("Invalid parameters: {0}")] InvalidParameters(String), } impl From<reqwest::Error> for BirdeyeError { fn from(err: reqwest::Error) -> Self { BirdeyeError::RequestError(err.to_string()) } } impl From<serde_json::Error> for BirdeyeError { fn from(err: serde_json::Error) -> Self { BirdeyeError::SerializationError(err.to_string()) } }
```

# plugins/birdeye/src/types/mod.rs

```rs
pub mod api; pub mod error; use serde::{Deserialize, Serialize}; use std::fmt; use std::str::FromStr; #[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)] pub enum TimeInterval { OneMinute, ThreeMinutes, FiveMinutes, FifteenMinutes, OneHour, FourHours, OneDay, OneWeek, OneMonth, } impl fmt::Display for TimeInterval { fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result { let s = match self { TimeInterval::OneMinute => "1m", TimeInterval::ThreeMinutes => "3m", TimeInterval::FiveMinutes => "5m", TimeInterval::FifteenMinutes => "15m", TimeInterval::OneHour => "1h", TimeInterval::FourHours => "4h", TimeInterval::OneDay => "1d", TimeInterval::OneWeek => "1w", TimeInterval::OneMonth => "1M", }; write!(f, "{}", s) } } impl FromStr for TimeInterval { type Err = error::BirdeyeError; fn from_str(s: &str) -> Result<Self, Self::Err> { match s { "1m" => Ok(TimeInterval::OneMinute), "3m" => Ok(TimeInterval::ThreeMinutes), "5m" => Ok(TimeInterval::FiveMinutes), "15m" => Ok(TimeInterval::FifteenMinutes), "1h" => Ok(TimeInterval::OneHour), "4h" => Ok(TimeInterval::FourHours), "1d" => Ok(TimeInterval::OneDay), "1w" => Ok(TimeInterval::OneWeek), "1M" => Ok(TimeInterval::OneMonth), _ => Err(error::BirdeyeError::InvalidTimeInterval(s.to_string())), } } }
```

# plugins/birdeye/src/types/shared.rs

```rs
use serde::{Deserialize, Serialize}; #[derive(Debug, Clone, Copy, Serialize, Deserialize)] #[serde(rename_all = "lowercase")] pub enum Chain { Solana, Ethereum, Arbitrum, Avalanche, Bsc, Optimism, Polygon, Base, Zksync, Sui, } impl std::fmt::Display for Chain { fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result { match self { Chain::Solana => write!(f, "solana"), Chain::Ethereum => write!(f, "ethereum"), Chain::Arbitrum => write!(f, "arbitrum"), Chain::Avalanche => write!(f, "avalanche"), Chain::Bsc => write!(f, "bsc"), Chain::Optimism => write!(f, "optimism"), Chain::Polygon => write!(f, "polygon"), Chain::Base => write!(f, "base"), Chain::Zksync => write!(f, "zksync"), Chain::Sui => write!(f, "sui"), } } } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct Address { pub address: String, pub chain: Chain, #[serde(rename = "type")] pub address_type: AddressType, } #[derive(Debug, Clone, Copy, Serialize, Deserialize)] #[serde(rename_all = "lowercase")] pub enum AddressType { Wallet, Token, Contract, } // Helper functions for address validation and chain detection impl Address { pub fn detect_chain(address: &str) -> Chain { if address.starts_with("0x") { if address.len() == 66 { Chain::Sui // Sui addresses are 0x + 64 hex chars } else { Chain::Ethereum // Default to Ethereum for other 0x addresses } } else { Chain::Solana // Default to Solana for base58 addresses } } pub fn is_valid_address(address: &str) -> bool { match Self::detect_chain(address) { Chain::Solana => bs58::decode(address).into_vec().is_ok(), Chain::Sui => address.starts_with("0x") && address.len() == 66 && hex::decode(&address[2..]).is_ok(), _ => address.starts_with("0x") && address.len() == 42 && hex::decode(&address[2..]).is_ok(), } } } // Constants for API configuration pub const API_BASE_URL: &str = "https://public-api.birdeye.so"; pub const DEFAULT_MAX_RETRIES: u32 = 3; pub const RETRY_DELAY_MS: u64 = 2000;
```

# plugins/birdeye/tests/integration_test.rs

```rs
use rig_birdeye::{ actions::{TokenSearchAction, WalletSearchAction}, providers::birdeye::BirdeyeProvider, types::{ api::{SortType, TokenSearchParams, TokenSortBy}, error::BirdeyeError, }, }; use std::env; fn setup() -> BirdeyeProvider { // Load environment variables from .env file dotenvy::dotenv().ok(); // Initialize logging tracing_subscriber::fmt::init(); // Create Birdeye provider let api_key = env::var("BIRDEYE_API_KEY").expect("BIRDEYE_API_KEY must be set"); BirdeyeProvider::new(&api_key) } #[tokio::test] async fn test_token_search() -> Result<(), Box<dyn std::error::Error>> { let provider = setup(); let params = TokenSearchParams { keyword: "SOL".to_string(), sort_by: Some(TokenSortBy::Volume), sort_type: Some(SortType::Descending), offset: None, limit: Some(10), }; let tokens = provider.search_tokens(params).await?; assert!(!tokens.is_empty(), "No tokens found"); // Validate first token let token = &tokens[0]; assert!(!token.address.is_empty(), "Token address is empty"); assert!(!token.symbol.is_empty(), "Token symbol is empty"); assert!(token.price_usd > 0.0, "Token price should be positive"); assert!(token.volume_24h > 0.0, "Token volume should be positive"); Ok(()) } #[tokio::test] async fn test_wallet_search() -> Result<(), Box<dyn std::error::Error>> { let provider = setup(); // Use a known Solana wallet address for testing let wallet_address = "DYw8jCTfwHNRJhhmFcbXvVDTqWMEVFBX6ZKUmG5CNSKK".to_string(); let portfolio = provider.search_wallet(&wallet_address).await?; assert_eq!(portfolio.wallet, wallet_address, "Wallet address mismatch"); assert!( portfolio.total_usd >= 0.0, "Total USD should be non-negative" ); assert!( !portfolio.items.is_empty(), "Portfolio should contain tokens" ); Ok(()) } #[tokio::test] async fn test_invalid_api_key() { let provider = BirdeyeProvider::new("invalid_key"); let params = TokenSearchParams { keyword: "SOL".to_string(), sort_by: None, sort_type: None, offset: None, limit: None, }; let result = provider.search_tokens(params).await; assert!(result.is_err(), "Expected error with invalid API key"); match result.unwrap_err() { BirdeyeError::InvalidApiKey => (), err => panic!("Expected InvalidApiKey error, got: {:?}", err), } } #[tokio::test] async fn test_token_overview() -> Result<(), Box<dyn std::error::Error>> { let provider = setup(); // Use SOL token address for testing let token_address = "So11111111111111111111111111111111111111112"; let overview = provider.get_token_overview(token_address).await?; assert_eq!(overview.address, token_address); assert_eq!(overview.symbol, "SOL"); assert!(overview.price > 0.0); assert!(overview.volume_24h > 0.0); assert!(overview.liquidity > 0.0); Ok(()) }
```

# plugins/cookie/Cargo.toml

```toml
[package] name = "solagent-plugin-cookie" version = "0.1.0" edition = "2021" authors = ["zTgx <beautifularea@gmail.com>"] repository = "https://github.com/zTgx/solagent.rs" keywords = ["solagent", "plugin", "cookie"] license = "Apache-2.0" description = "solagent plugin cookie" [dependencies] solagent-core = "0.1.3" serde = { version = "1.0", features = ["derive"] } base64 = "0.22.1" reqwest = { version = "0.12", features = ["json"] }
```

# plugins/cookie/src/get_agent_by_ca.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use solagent_core::{serde_json::Value, SolanaAgentKit}; use std::error::Error; /// Retrieve agent details in specified interval by one of its tokens contract address. /// /// # Parameters /// /// - `agent`: An instance of `SolanaAgentKit`. /// - `contract_address`: Contract address of one of the tokens contracts (matches case insensitive) /// - `interval`: An optional Interval for twitter stats and deltas (_3Days, _7Days). If not provided, returns the _7Days. /// /// # Returns /// /// A `Result` that agent details pub async fn get_agent_by_ca( agent: &SolanaAgentKit, contract_address: &str, interval: Option<u32>, ) -> Result<Value, Box<dyn Error>> { // Get the Cookie API key from the agent's configuration let api_key = match agent.config.cookie_api_key.as_ref() { Some(key) => key, None => return Err("Missing Cookie API key in agent.config.cookie_api_key".into()), }; let api_url = "https://api.cookie.fun/v2/agents/contractAddress"; let url = format!("{}/{}?interval=_{}Days", api_url, contract_address, interval.unwrap_or(7)); let client = reqwest::Client::new(); let response = client.get(&url).header("x-api-key", api_key).send().await?; let json: Value = response.json().await?; Ok(json) }
```

# plugins/cookie/src/get_agent_by_name.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use solagent_core::{serde_json::Value, SolanaAgentKit}; use std::error::Error; /// Retrieve agent details in specified interval by twitter username. /// /// # Parameters /// /// - `agent`: An instance of `SolanaAgentKit`. /// - `twitter_name`: Twitter username of agent (matches case insensitive) /// - `interval`: An optional Interval for twitter stats and deltas (_3Days, _7Days). If not provided, returns the _7Days. /// /// # Returns /// /// A `Result` that agent details pub async fn get_agent_by_name( agent: &SolanaAgentKit, twitter_name: &str, interval: Option<u32>, ) -> Result<Value, Box<dyn Error>> { // Get the Cookie API key from the agent's configuration let api_key = match agent.config.cookie_api_key.as_ref() { Some(key) => key, None => return Err("Missing Cookie API key in agent.config.cookie_api_key".into()), }; let api_url = "https://api.cookie.fun/v2/agents/twitterUsername"; let url = format!("{}/{}?interval=_{}Days", api_url, twitter_name, interval.unwrap_or(7)); let client = reqwest::Client::new(); let response = client.get(&url).header("x-api-key", api_key).send().await?; let json: Value = response.json().await?; Ok(json) }
```

# plugins/cookie/src/lib.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. mod get_agent_by_name; pub use get_agent_by_name::get_agent_by_name; mod get_agent_by_ca; pub use get_agent_by_ca::get_agent_by_ca; mod search_tweets; pub use search_tweets::search_tweets;
```

# plugins/cookie/src/search_tweets.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use solagent_core::{serde_json::Value, SolanaAgentKit}; use std::error::Error; /// Retrieve popular content matching search query, created in time range {from} - {to} (YYYY-MM-DD dates). /// /// # Parameters /// /// - `agent`: An instance of `SolanaAgentKit`. /// - `search_query`: Word or phrase to be searched for in text /// - `from`: Only consider content created after given date, eg. 2025-01-01 /// - `to`: Only consider content created before given date, eg. 2025-01-20 /// /// # Returns /// /// A `Result` that tweets details pub async fn search_tweets( agent: &SolanaAgentKit, tweets: &str, from: &str, to: &str, ) -> Result<Value, Box<dyn Error>> { // Get the Cookie API key from the agent's configuration let api_key = match agent.config.cookie_api_key.as_ref() { Some(key) => key, None => return Err("Missing Cookie API key in agent.config.cookie_api_key".into()), }; let api_url = "https://api.cookie.fun/v1/hackathon/search"; let url = format!("{}/{}?from={}&to={}", api_url, tweets, from, to); let client = reqwest::Client::new(); let response = client.get(&url).header("x-api-key", api_key).send().await?; let json: Value = response.json().await?; Ok(json) }
```

# plugins/dexscreener/Cargo.toml

```toml
[package] name = "solagent-plugin-dexscreener" version = "0.1.1" edition = "2021" authors = ["zTgx <beautifularea@gmail.com>"] repository = "https://github.com/zTgx/solagent.rs" keywords = ["solagent", "plugin", "dexscreener"] license = "Apache-2.0" description = "solagent plugin dexscreener" [dependencies] serde_json = "1.0" serde = { version = "1.0", features = ["derive"] } reqwest = { version = "0.12", features = ["json"] }
```

# plugins/dexscreener/src/get_token_data_by_ticker.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use serde::{Deserialize, Serialize}; use std::error::Error; // Define the Txns struct to represent transaction information #[derive(Debug, Serialize, Deserialize, Clone)] struct Txns { #[serde(rename = "m5")] m5: BuysSells, #[serde(rename = "h1")] h1: BuysSells, #[serde(rename = "h6")] h6: BuysSells, #[serde(rename = "h24")] h24: BuysSells, } // Define the BuysSells struct to represent the number of buys and sells #[derive(Debug, Serialize, Deserialize, Clone)] struct BuysSells { #[serde(rename = "buys")] buys: u64, #[serde(rename = "sells")] sells: u64, } // Define the Volume struct to represent trading volume #[derive(Debug, Serialize, Deserialize, Clone)] struct Volume { #[serde(rename = "h24")] h24: f64, #[serde(rename = "h6")] h6: f64, #[serde(rename = "h1")] h1: f64, #[serde(rename = "m5")] m5: f64, } // Define the PriceChange struct to represent price changes #[derive(Debug, Serialize, Deserialize, Clone)] struct PriceChange { #[serde(rename = "h6")] h6: f64, } // Define the Liquidity struct to represent liquidity #[derive(Debug, Serialize, Deserialize, Clone)] struct Liquidity { #[serde(rename = "usd")] usd: f64, #[serde(rename = "base")] base: u64, #[serde(rename = "quote")] quote: f64, } // Define the Token struct to represent token information #[derive(Debug, Serialize, Deserialize, Clone)] struct Token { #[serde(rename = "address")] address: String, #[serde(rename = "name")] name: String, #[serde(rename = "symbol")] symbol: String, } // Define the Pair struct to represent trading pair information #[derive(Debug, Serialize, Deserialize, Clone)] struct Pair { #[serde(rename = "chainId")] chain_id: String, #[serde(rename = "dexId")] dex_id: String, #[serde(rename = "url")] url: String, #[serde(rename = "pairAddress")] pair_address: String, #[serde(rename = "labels")] labels: Option<Vec<String>>, #[serde(rename = "baseToken")] base_token: Token, #[serde(rename = "quoteToken")] quote_token: Token, #[serde(rename = "priceNative")] price_native: String, #[serde(rename = "priceUsd")] price_usd: String, #[serde(rename = "txns")] txns: Txns, #[serde(rename = "volume")] volume: Volume, #[serde(rename = "priceChange")] price_change: PriceChange, #[serde(rename = "liquidity")] liquidity: Liquidity, #[serde(rename = "fdv")] fdv: u64, #[serde(rename = "marketCap")] market_cap: u64, #[serde(rename = "pairCreatedAt")] pair_created_at: Option<u64>, } // Define the Root struct to represent the root structure of the JSON #[derive(Debug, Serialize, Deserialize, Clone)] struct SearchTokenData { #[serde(rename = "schemaVersion")] schema_version: String, #[serde(rename = "pairs")] pairs: Vec<Pair>, } #[derive(Debug, Serialize, Deserialize)] pub struct JupiterTokenData { pub address: String, pub name: String, pub symbol: String, pub decimals: u8, pub tags: Vec<String>, pub logo_uri: String, pub daily_volume: f64, pub freeze_authority: Option<String>, pub mint_authority: Option<String>, pub permanent_delegate: Option<String>, pub extensions: Extensions, } #[derive(Debug, Serialize, Deserialize)] pub struct Extensions { #[serde(skip_serializing_if = "Option::is_none")] pub coingecko_id: Option<String>, } /// Get the token data for a given token ticker. /// /// # Parameters /// /// - `ticker`: Ticker of the token, e.g. 'USDC' /// /// # Returns /// /// A `Result` pub async fn get_token_data_by_ticker(ticker: &str) -> Result<JupiterTokenData, Box<dyn Error>> { let client = reqwest::Client::new(); let address = get_token_address_from_ticker(&client, ticker).await?; get_token_data_by_address(&client, &address).await } async fn get_token_address_from_ticker(client: &reqwest::Client, ticker: &str) -> Result<String, Box<dyn Error>> { let url = format!("https://api.dexscreener.com/latest/dex/search?q=${}", ticker); let response = client.get(&url).send().await?; if !response.status().is_success() { return Err(format!("Failed to get token address from ticker: {}", response.status()).into()); } let data: SearchTokenData = response.json().await?; let mut solana_pairs: Vec<Pair> = data.pairs.iter().filter(|&pair| pair.chain_id == "solana").cloned().collect(); solana_pairs.sort_by_key(|pair| std::cmp::Reverse(pair.fdv)); let filtered_pairs: Vec<Pair> = solana_pairs .into_iter() .filter(|pair| pair.base_token.symbol.to_lowercase() == ticker.to_lowercase()) .collect(); filtered_pairs.into_iter().next().map(|pair| pair.base_token.address).ok_or("get address error".into()) } async fn get_token_data_by_address( client: &reqwest::Client, address: &str, ) -> Result<JupiterTokenData, Box<dyn Error>> { let url = format!("https://tokens.jup.ag/token/${}", address); let response = client.get(&url).send().await?; if !response.status().is_success() { return Err(format!("Failed to get token address from ticker: {}", response.status()).into()); } let data: JupiterTokenData = response.json().await?; Ok(data) }
```

# plugins/dexscreener/src/lib.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. mod get_token_data_by_ticker; pub use get_token_data_by_ticker::{get_token_data_by_ticker, JupiterTokenData};
```

# plugins/discord/Cargo.toml

```toml
[package] name = "cainam-discord" version = "0.1.0" edition = "2021" [dependencies] rig-core = "0.7.0" tokio = { version = "1.34.0", features = ["full"] } serenity = { version = "0.11", default-features = false, features = ["client", "gateway", "rustls_backend", "cache", "model", "http"] } # Environment variables dotenvy = "0.15.7" anyhow = "1.0.75" tracing = "0.1" tracing-subscriber = "0.3" reqwest = { version = "0.11", features = ["json"] } serde = { version = "1.0", features = ["derive"] } serde_json = "1.0" schemars = "0.8" async-trait = "0.1.83"
```

# plugins/discord/documents/backup.rs

```rs
use anyhow::{Context, Result}; use rig::providers::openai; use rig::vector_store::in_memory_store::InMemoryVectorStore; use rig::vector_store::VectorStore; use rig::embeddings::EmbeddingsBuilder; use rig::rag::RagAgent; use rig::vector_store::in_memory_store::InMemoryVectorIndex; use rig::completion::Prompt; use std::path::Path; use std::fs; use std::sync::Arc; pub struct RigAgent { rag_agent: Arc<RagAgent<openai::CompletionModel, InMemoryVectorIndex<openai::EmbeddingModel>, rig::vector_store::NoIndex>>, } impl RigAgent { pub async fn new() -> Result<Self> { // Initialize OpenAI client let openai_client = openai::Client::from_env(); let embedding_model = openai_client.embedding_model("text-embedding-ada-002"); // Create vector store let mut vector_store = InMemoryVectorStore::default(); // Get the current directory and construct paths to markdown files let current_dir = std::env::current_dir()?; let documents_dir = current_dir.join("documents"); let md1_path = documents_dir.join("Rig_guide.md"); let md2_path = documents_dir.join("Rig_faq.md"); let md3_path = documents_dir.join("Rig_examples.md"); let md4_path = documents_dir.join("Rig_code_samples.md"); // Load markdown documents let md1_content = Self::load_md_content(&md1_path)?; let md2_content = Self::load_md_content(&md2_path)?; let md3_content = Self::load_md_content(&md3_path)?; let md4_content = Self::load_md_content(&md4_path)?; // Create embeddings and add to vector store let embeddings = EmbeddingsBuilder::new(embedding_model.clone()) .simple_document("Rig_guide", &md1_content) .simple_document("Rig_faq", &md2_content) .simple_document("Rig_examples", &md3_content) .simple_document("Rig_code_samples", &md4_content) .build() .await?; vector_store.add_documents(embeddings).await?; // Create index let context_index = vector_store.index(embedding_model); // Create RAG agent let rag_agent = Arc::new(openai_client.context_rag_agent("gpt-4o") .preamble(" Your name is Rig Agent, you are an advanced AI assistant powered by Rig, a Rust library for building LLM applications. Your primary function is to provide accurate, helpful, and context-aware responses by leveraging both your general knowledge and specific information retrieved from a curated knowledge base. Key responsibilities and behaviors: 1. Information Retrieval: You have access to a vast knowledge base. When answering questions, always consider the context provided by the retrieved information. 2. Accuracy and Honesty: Strive for accuracy in your responses. If you're unsure about something or if the retrieved information is incomplete, clearly state this. Never invent or assume information. 3. Clarity and Conciseness: Provide clear and concise answers. Use bullet points or numbered lists for complex information when appropriate. 4. Source Attribution: When using information from the knowledge base, indicate this by saying something like 'Based on the retrieved information...' or 'According to the knowledge base...'. 5. Follow-up Encouragement: If a topic requires more depth than can be provided in a single response, encourage the user to ask follow-up questions. 6. Technical Proficiency: You have deep knowledge about Rig and its capabilities. When discussing Rig or answering related questions, provide detailed and technically accurate information. 7. Code Examples: When appropriate, provide Rust code examples to illustrate concepts, especially when discussing Rig's functionalities. Always format code examples for proper rendering in Discord by wrapping them in triple backticks and specifying the language as 'rust'. For example: \`\`\`rust let example_code = \"This is how you format Rust code for Discord\"; println!(\"{}\", example_code); \`\`\` 8. Adaptability: Be prepared to handle a wide range of topics. If a question falls outside your knowledge base, focus on providing general guidance or suggesting ways to rephrase the query. 9. Ethical Considerations: Be mindful of ethical implications in your responses. Avoid generating harmful, illegal, or biased content. 10. Continuous Learning: While you can't actually learn or update your knowledge, simulate a learning attitude by showing interest in new information provided by users. Remember, your goal is to be a helpful, accurate, and insightful assistant, leveraging both your general capabilities and the specific information available to you through the RAG system.") .dynamic_context(2, context_index) .build()); Ok(Self { rag_agent }) } fn load_md_content<P: AsRef<Path>>(file_path: P) -> Result<String> { fs::read_to_string(file_path.as_ref()) .with_context(|| format!("Failed to read markdown file: {:?}", file_path.as_ref())) } pub async fn process_message(&self, message: &str) -> Result<String> { self.rag_agent.prompt(message).await.map_err(anyhow::Error::from) } }
```

# plugins/discord/documents/Rig_code_samples.md

```md
# Rig code samples 1. Building a simple agent with Rig: \`\`\`rust use std::env; use rig::{completion::Prompt, providers}; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { // Create OpenAI client let client = providers::openai::Client::new( &env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY not set"), ); // Create agent with a single context prompt let comedian_agent = client .agent("gpt-4o") .preamble("You are a comedian here to entertain the user using humour and jokes.") .build(); // Prompt the agent and print the response let response = comedian_agent.prompt("Entertain me!").await?; println!("{}", response); Ok(()) } \`\`\` 2. Building an agent with context with Rig: \`\`\`rust use std::env; use rig::{agent::AgentBuilder, completion::Prompt, providers::cohere}; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { // Create OpenAI and Cohere clients // let openai_client = openai::Client::new(&env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY not set")); let cohere_client = cohere::Client::new(&env::var("COHERE_API_KEY").expect("COHERE_API_KEY not set")); // let model = openai_client.completion_model("gpt-4o"); let model = cohere_client.completion_model("command-r"); // Create an agent with multiple context documents let agent = AgentBuilder::new(model) .context("Definition of a *flurbo*: A flurbo is a green alien that lives on cold planets") .context("Definition of a *glarb-glarb*: A glarb-glarb is a ancient tool used by the ancestors of the inhabitants of planet Jiro to farm the land.") .context("Definition of a *linglingdong*: A term used by inhabitants of the far side of the moon to describe humans.") .build(); // Prompt the agent and print the response let response = agent.prompt("What does \"glarb-glarb\" mean?").await?; println!("{}", response); Ok(()) } \`\`\` 3. Building an agent with tools with Rig: \`\`\`rust use anyhow::Result; use rig::{ completion::{Prompt, ToolDefinition}, providers, tool::Tool, }; use serde::{Deserialize, Serialize}; use serde_json::json; use std::env; #[derive(Deserialize)] struct OperationArgs { x: i32, y: i32, } #[derive(Debug, thiserror::Error)] #[error("Math error")] struct MathError; #[derive(Deserialize, Serialize)] struct Adder; impl Tool for Adder { const NAME: &'static str = "add"; type Error = MathError; type Args = OperationArgs; type Output = i32; async fn definition(&self, _prompt: String) -> ToolDefinition { ToolDefinition { name: "add".to_string(), description: "Add x and y together".to_string(), parameters: json!({ "type": "object", "properties": { "x": { "type": "number", "description": "The first number to add" }, "y": { "type": "number", "description": "The second number to add" } } }), } } async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> { let result = args.x + args.y; Ok(result) } } #[derive(Deserialize, Serialize)] struct Subtract; impl Tool for Subtract { const NAME: &'static str = "subtract"; type Error = MathError; type Args = OperationArgs; type Output = i32; async fn definition(&self, _prompt: String) -> ToolDefinition { serde_json::from_value(json!({ "name": "subtract", "description": "Subtract y from x (i.e.: x - y)", "parameters": { "type": "object", "properties": { "x": { "type": "number", "description": "The number to substract from" }, "y": { "type": "number", "description": "The number to substract" } } } })) .expect("Tool Definition") } async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> { let result = args.x - args.y; Ok(result) } } #[tokio::main] async fn main() -> Result<(), anyhow::Error> { // Create OpenAI client let openai_api_key = env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY not set"); let openai_client = providers::openai::Client::new(&openai_api_key); // Create agent with a single context prompt and two tools let gpt4_calculator_agent = openai_client .agent("gpt-4o") .context("You are a calculator here to help the user perform arithmetic operations.") .tool(Adder) .tool(Subtract) .build(); // Create OpenAI client let cohere_api_key = env::var("COHERE_API_KEY").expect("COHERE_API_KEY not set"); let cohere_client = providers::cohere::Client::new(&cohere_api_key); // Create agent with a single context prompt and two tools let coral_calculator_agent = cohere_client .agent("command-r") .preamble("You are a calculator here to help the user perform arithmetic operations.") .tool(Adder) .tool(Subtract) .build(); // Prompt the agent and print the response println!("Calculate 2 - 5"); println!( "GPT-4: {}", gpt4_calculator_agent.prompt("Calculate 2 - 5").await? ); println!( "Coral: {}", coral_calculator_agent.prompt("Calculate 2 - 5").await? ); Ok(()) } \`\`\` 4. Building an Anthropic agent with Rig: \`\`\`rust use std::env; use rig::{ completion::Prompt, providers::anthropic::{self, CLAUDE_3_5_SONNET}, }; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { // Create OpenAI client let client = anthropic::ClientBuilder::new( &env::var("ANTHROPIC_API_KEY").expect("ANTHROPIC_API_KEY not set"), ) .build(); // Create agent with a single context prompt let agent = client .agent(CLAUDE_3_5_SONNET) .preamble("Be precise and concise.") .temperature(0.5) .max_tokens(8192) .build(); // Prompt the agent and print the response let response = agent .prompt("When and where and what type is the next solar eclipse?") .await?; println!("{}", response); Ok(()) } \`\`\` 5. Building a calculator chatbot with Rig: \`\`\`rust use anyhow::Result; use rig::{ cli_chatbot::cli_chatbot, completion::ToolDefinition, embeddings::EmbeddingsBuilder, providers::openai::Client, tool::{Tool, ToolEmbedding, ToolSet}, vector_store::{in_memory_store::InMemoryVectorStore, VectorStore}, }; use serde::{Deserialize, Serialize}; use serde_json::json; use std::env; #[derive(Deserialize)] struct OperationArgs { x: i32, y: i32, } #[derive(Debug, thiserror::Error)] #[error("Math error")] struct MathError; #[derive(Debug, thiserror::Error)] #[error("Init error")] struct InitError; #[derive(Deserialize, Serialize)] struct Add; impl Tool for Add { const NAME: &'static str = "add"; type Error = MathError; type Args = OperationArgs; type Output = i32; async fn definition(&self, _prompt: String) -> ToolDefinition { serde_json::from_value(json!({ "name": "add", "description": "Add x and y together", "parameters": { "type": "object", "properties": { "x": { "type": "number", "description": "The first number to add" }, "y": { "type": "number", "description": "The second number to add" } } } })) .expect("Tool Definition") } async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> { let result = args.x + args.y; Ok(result) } } impl ToolEmbedding for Add { type InitError = InitError; type Context = (); type State = (); fn init(_state: Self::State, _context: Self::Context) -> Result<Self, Self::InitError> { Ok(Add) } fn embedding_docs(&self) -> Vec<String> { vec!["Add x and y together".into()] } fn context(&self) -> Self::Context {} } #[derive(Deserialize, Serialize)] struct Subtract; impl Tool for Subtract { const NAME: &'static str = "subtract"; type Error = MathError; type Args = OperationArgs; type Output = i32; async fn definition(&self, _prompt: String) -> ToolDefinition { serde_json::from_value(json!({ "name": "subtract", "description": "Subtract y from x (i.e.: x - y)", "parameters": { "type": "object", "properties": { "x": { "type": "number", "description": "The number to substract from" }, "y": { "type": "number", "description": "The number to substract" } } } })) .expect("Tool Definition") } async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> { let result = args.x - args.y; Ok(result) } } impl ToolEmbedding for Subtract { type InitError = InitError; type Context = (); type State = (); fn init(_state: Self::State, _context: Self::Context) -> Result<Self, Self::InitError> { Ok(Subtract) } fn embedding_docs(&self) -> Vec<String> { vec!["Subtract y from x (i.e.: x - y)".into()] } fn context(&self) -> Self::Context {} } struct Multiply; impl Tool for Multiply { const NAME: &'static str = "multiply"; type Error = MathError; type Args = OperationArgs; type Output = i32; async fn definition(&self, _prompt: String) -> ToolDefinition { serde_json::from_value(json!({ "name": "multiply", "description": "Compute the product of x and y (i.e.: x * y)", "parameters": { "type": "object", "properties": { "x": { "type": "number", "description": "The first factor in the product" }, "y": { "type": "number", "description": "The second factor in the product" } } } })) .expect("Tool Definition") } async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> { let result = args.x * args.y; Ok(result) } } impl ToolEmbedding for Multiply { type InitError = InitError; type Context = (); type State = (); fn init(_state: Self::State, _context: Self::Context) -> Result<Self, Self::InitError> { Ok(Multiply) } fn embedding_docs(&self) -> Vec<String> { vec!["Compute the product of x and y (i.e.: x * y)".into()] } fn context(&self) -> Self::Context {} } struct Divide; impl Tool for Divide { const NAME: &'static str = "divide"; type Error = MathError; type Args = OperationArgs; type Output = i32; async fn definition(&self, _prompt: String) -> ToolDefinition { serde_json::from_value(json!({ "name": "divide", "description": "Compute the Quotient of x and y (i.e.: x / y). Useful for ratios.", "parameters": { "type": "object", "properties": { "x": { "type": "number", "description": "The Dividend of the division. The number being divided" }, "y": { "type": "number", "description": "The Divisor of the division. The number by which the dividend is being divided" } } } })) .expect("Tool Definition") } async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> { let result = args.x / args.y; Ok(result) } } impl ToolEmbedding for Divide { type InitError = InitError; type Context = (); type State = (); fn init(_state: Self::State, _context: Self::Context) -> Result<Self, Self::InitError> { Ok(Divide) } fn embedding_docs(&self) -> Vec<String> { vec!["Compute the Quotient of x and y (i.e.: x / y). Useful for ratios.".into()] } fn context(&self) -> Self::Context {} } #[tokio::main] async fn main() -> Result<(), anyhow::Error> { // Create OpenAI client let openai_api_key = env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY not set"); let openai_client = Client::new(&openai_api_key); // Create dynamic tools embeddings let toolset = ToolSet::builder() .dynamic_tool(Add) .dynamic_tool(Subtract) .dynamic_tool(Multiply) .dynamic_tool(Divide) .build(); let embedding_model = openai_client.embedding_model("text-embedding-ada-002"); let embeddings = EmbeddingsBuilder::new(embedding_model.clone()) .tools(&toolset)? .build() .await?; let mut store = InMemoryVectorStore::default(); store.add_documents(embeddings).await?; let index = store.index(embedding_model); // Create RAG agent with a single context prompt and a dynamic tool source let calculator_rag = openai_client .agent("gpt-4o") .preamble( "You are an assistant here to help the user select which tool is most appropriate to perform arithmetic operations. Follow these instructions closely. 1. Consider the user's request carefully and identify the core elements of the request. 2. Select which tool among those made available to you is appropriate given the context. 3. This is very important: never perform the operation yourself and never give me the direct result. Always respond with the name of the tool that should be used and the appropriate inputs in the following format: Tool: <tool name> Inputs: <list of inputs> " ) // Add a dynamic tool source with a sample rate of 1 (i.e.: only // 1 additional tool will be added to prompts) .dynamic_tools(4, index, toolset) .build(); // Prompt the agent and print the response cli_chatbot(calculator_rag).await?; Ok(()) } \`\`\` 6. Building a cohere connector with Rig: \`\`\`rust use std::env; use rig::{ completion::{Completion, Prompt}, providers::cohere::Client as CohereClient, }; use serde_json::json; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { // Create Cohere client let cohere_api_key = env::var("COHERE_API_KEY").expect("COHERE_API_KEY not set"); let cohere_client = CohereClient::new(&cohere_api_key); let klimadao_agent = cohere_client .agent("command-r") .temperature(0.0) .additional_params(json!({ "connectors": [{"id":"web-search", "options":{"site": "https://docs.klimadao.finance"}}] })) .build(); // Prompt the model and print the response // We use `prompt` to get a simple response from the model as a String let response = klimadao_agent.prompt("Tell me about BCT tokens?").await?; println!("\n\nCoral: {:?}", response); // Prompt the model and get the citations // We use `completion` to allow use to customize the request further and // get a more detailed response from the model. // Here the response is of type CompletionResponse<cohere::CompletionResponse> // which contains `choice` (Message or ToolCall) as well as `raw_response`, // the underlying providers' raw response. let response = klimadao_agent .completion("Tell me about BCT tokens?", vec![]) .await? .additional_params(json!({ "connectors": [{"id":"web-search", "options":{"site": "https://docs.klimadao.finance"}}] })) .send() .await?; println!( "\n\nCoral: {:?}\n\nCitations:\n{:?}", response.choice, response.raw_response.citations ); Ok(()) } \`\`\` 7. Building debate agents with Rig: \`\`\`rust use std::env; use anyhow::Result; use rig::{ agent::Agent, completion::{Chat, Message}, providers::{cohere, openai}, }; struct Debater { gpt_4o: Agent<openai::CompletionModel>, coral: Agent<cohere::CompletionModel>, } impl Debater { fn new(position_a: &str, position_b: &str) -> Self { let openai_client = openai::Client::new(&env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY not set")); let cohere_client = cohere::Client::new(&env::var("COHERE_API_KEY").expect("COHERE_API_KEY not set")); Self { o: openai_client.agent("gpt-4o").preamble(position_a).build(), coral: cohere_client .agent("command-r") .preamble(position_b) .build(), } } async fn rounds(&self, n: usize) -> Result<()> { let mut history_a: Vec<Message> = vec![]; let mut history_b: Vec<Message> = vec![]; let mut last_resp_b: Option<String> = None; for _ in 0..n { let prompt_a = if let Some(msg_b) = &last_resp_b { msg_b.clone() } else { "Plead your case!".into() }; let resp_a = self.gpt_4o.chat(&prompt_a, history_a.clone()).await?; println!("GPT-4:\n{}", resp_a); history_a.push(Message { role: "user".into(), content: prompt_a.clone(), }); history_a.push(Message { role: "assistant".into(), content: resp_a.clone(), }); println!("================================================================"); let resp_b = self.coral.chat(&resp_a, history_b.clone()).await?; println!("Coral:\n{}", resp_b); println!("================================================================"); history_b.push(Message { role: "user".into(), content: resp_a.clone(), }); history_b.push(Message { role: "assistant".into(), content: resp_b.clone(), }); last_resp_b = Some(resp_b) } Ok(()) } } #[tokio::main] async fn main() -> Result<(), anyhow::Error> { // Create model let debator = Debater::new( "\ You believe that religion is a useful concept. \ This could be for security, financial, ethical, philosophical, metaphysical, religious or any kind of other reason. \ You choose what your arguments are. \ I will argue against you and you must rebuke me and try to convince me that I am wrong. \ Make your statements short and concise. \ ", "\ You believe that religion is a harmful concept. \ This could be for security, financial, ethical, philosophical, metaphysical, religious or any kind of other reason. \ You choose what your arguments are. \ I will argue against you and you must rebuke me and try to convince me that I am wrong. \ Make your statements short and concise. \ ", ); // Run the debate for 4 rounds debator.rounds(4).await?; Ok(()) } \`\`\` 8. Building extractor with Rig: \`\`\`rust use rig::providers::openai; use schemars::JsonSchema; use serde::{Deserialize, Serialize}; #[derive(Debug, Deserialize, JsonSchema, Serialize)] /// A record representing a person struct Person { /// The person's first name, if provided (null otherwise) pub first_name: Option<String>, /// The person's last name, if provided (null otherwise) pub last_name: Option<String>, /// The person's job, if provided (null otherwise) pub job: Option<String>, } #[tokio::main] async fn main() -> Result<(), anyhow::Error> { // Create OpenAI client let openai_client = openai::Client::from_env(); // Create extractor let data_extractor = openai_client.extractor::<Person>("gpt-4o").build(); let person = data_extractor .extract("Hello my name is John Doe! I am a software engineer.") .await?; println!("GPT-4: {}", serde_json::to_string_pretty(&person).unwrap()); Ok(()) } \`\`\` 9. Building multi agents with Rig: \`\`\`rust use std::env; use rig::{ agent::{Agent, AgentBuilder}, cli_chatbot::cli_chatbot, completion::{Chat, CompletionModel, Message, PromptError}, providers::openai::Client as OpenAIClient, }; /// Represents a multi agent application that consists of two components: /// an agent specialized in translating prompt into english and a simple GPT-4 model. /// When prompted, the application will use the translator agent to translate the /// prompt in english, before answering it with GPT-4. The answer in english is returned. struct EnglishTranslator<M: CompletionModel> { translator_agent: Agent<M>, gpt4: Agent<M>, } impl<M: CompletionModel> EnglishTranslator<M> { fn new(model: M) -> Self { Self { // Create the translator agent translator_agent: AgentBuilder::new(model.clone()) .preamble("\ You are a translator assistant that will translate any input text into english. \ If the text is already in english, simply respond with the original text but fix any mistakes (grammar, syntax, etc.). \ ") .build(), // Create the GPT4 model gpt4: AgentBuilder::new(model).build() } } } impl<M: CompletionModel> Chat for EnglishTranslator<M> { async fn chat(&self, prompt: &str, chat_history: Vec<Message>) -> Result<String, PromptError> { // Translate the prompt using the translator agent let translated_prompt = self .translator_agent .chat(prompt, chat_history.clone()) .await?; println!("Translated prompt: {}", translated_prompt); // Answer the prompt using gpt4 self.gpt4.chat(&translated_prompt, chat_history).await } } #[tokio::main] async fn main() -> Result<(), anyhow::Error> { // Create OpenAI client let openai_api_key = env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY not set"); let openai_client = OpenAIClient::new(&openai_api_key); let model = openai_client.completion_model("gpt-4o"); // Create OpenAI client // let cohere_api_key = env::var("COHERE_API_KEY").expect("COHERE_API_KEY not set"); // let cohere_client = CohereClient::new(&cohere_api_key); // let model = cohere_client.completion_model("command-r"); // Create model let translator = EnglishTranslator::new(model); // Spin up a chatbot using the agent cli_chatbot(translator).await?; Ok(()) } \`\`\` 10. Building perplexity agent with Rig: \`\`\`rust use std::env; use rig::{ completion::Prompt, providers::{self, perplexity::LLAMA_3_1_70B_INSTRUCT}, }; use serde_json::json; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { // Create OpenAI client let client = providers::perplexity::Client::new( &env::var("PERPLEXITY_API_KEY").expect("PERPLEXITY_API_KEY not set"), ); // Create agent with a single context prompt let agent = client .agent(LLAMA_3_1_70B_INSTRUCT) .preamble("Be precise and concise.") .temperature(0.5) .additional_params(json!({ "return_related_questions": true, "return_images": true })) .build(); // Prompt the agent and print the response let response = agent .prompt("When and where and what type is the next solar eclipse?") .await?; println!("{}", response); Ok(()) } \`\`\` 11. Building RAG Agent with Rig: \`\`\`rust use std::env; use rig::{ completion::Prompt, embeddings::EmbeddingsBuilder, providers::openai::{Client, TEXT_EMBEDDING_ADA_002}, vector_store::{in_memory_store::InMemoryVectorStore, VectorStore}, }; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { // Create OpenAI client let openai_api_key = env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY not set"); let openai_client = Client::new(&openai_api_key); let embedding_model = openai_client.embedding_model(TEXT_EMBEDDING_ADA_002); // Create vector store, compute embeddings and load them in the store let mut vector_store = InMemoryVectorStore::default(); let embeddings = EmbeddingsBuilder::new(embedding_model.clone()) .simple_document("doc0", "Definition of a *flurbo*: A flurbo is a green alien that lives on cold planets") .simple_document("doc1", "Definition of a *glarb-glarb*: A glarb-glarb is a ancient tool used by the ancestors of the inhabitants of planet Jiro to farm the land.") .simple_document("doc2", "Definition of a *linglingdong*: A term used by inhabitants of the far side of the moon to describe humans.") .build() .await?; vector_store.add_documents(embeddings).await?; // Create vector store index let index = vector_store.index(embedding_model); let rag_agent = openai_client.agent("gpt-4o") .preamble(" You are a dictionary assistant here to assist the user in understanding the meaning of words. You will find additional non-standard word definitions that could be useful below. ") .dynamic_context(1, index) .build(); // Prompt the agent and print the response let response = rag_agent.prompt("What does \"glarb-glarb\" mean?").await?; println!("{}", response); Ok(()) } \`\`\` 12. Building RAG agent with dynamics tools with Rig: \`\`\`rust use anyhow::Result; use rig::{ completion::{Prompt, ToolDefinition}, embeddings::EmbeddingsBuilder, providers::openai::Client, tool::{Tool, ToolEmbedding, ToolSet}, vector_store::{in_memory_store::InMemoryVectorStore, VectorStore}, }; use serde::{Deserialize, Serialize}; use serde_json::json; use std::env; #[derive(Deserialize)] struct OperationArgs { x: i32, y: i32, } #[derive(Debug, thiserror::Error)] #[error("Math error")] struct MathError; #[derive(Debug, thiserror::Error)] #[error("Math error")] struct InitError; #[derive(Deserialize, Serialize)] struct Add; impl Tool for Add { const NAME: &'static str = "add"; type Error = MathError; type Args = OperationArgs; type Output = i32; async fn definition(&self, _prompt: String) -> ToolDefinition { serde_json::from_value(json!({ "name": "add", "description": "Add x and y together", "parameters": { "type": "object", "properties": { "x": { "type": "number", "description": "The first number to add" }, "y": { "type": "number", "description": "The second number to add" } } } })) .expect("Tool Definition") } async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> { let result = args.x + args.y; Ok(result) } } impl ToolEmbedding for Add { type InitError = InitError; type Context = (); type State = (); fn init(_state: Self::State, _context: Self::Context) -> Result<Self, Self::InitError> { Ok(Add) } fn embedding_docs(&self) -> Vec<String> { vec!["Add x and y together".into()] } fn context(&self) -> Self::Context {} } #[derive(Deserialize, Serialize)] struct Subtract; impl Tool for Subtract { const NAME: &'static str = "subtract"; type Error = MathError; type Args = OperationArgs; type Output = i32; async fn definition(&self, _prompt: String) -> ToolDefinition { serde_json::from_value(json!({ "name": "subtract", "description": "Subtract y from x (i.e.: x - y)", "parameters": { "type": "object", "properties": { "x": { "type": "number", "description": "The number to substract from" }, "y": { "type": "number", "description": "The number to substract" } } } })) .expect("Tool Definition") } async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> { let result = args.x - args.y; Ok(result) } } impl ToolEmbedding for Subtract { type InitError = InitError; type Context = (); type State = (); fn init(_state: Self::State, _context: Self::Context) -> Result<Self, Self::InitError> { Ok(Subtract) } fn context(&self) -> Self::Context {} fn embedding_docs(&self) -> Vec<String> { vec!["Subtract y from x (i.e.: x - y)".into()] } } #[tokio::main] async fn main() -> Result<(), anyhow::Error> { // required to enable CloudWatch error logging by the runtime tracing_subscriber::fmt() .with_max_level(tracing::Level::INFO) // disable printing the name of the module in every log line. .with_target(false) // this needs to be set to false, otherwise ANSI color codes will // show up in a confusing manner in CloudWatch logs. .with_ansi(false) // disabling time is handy because CloudWatch will add the ingestion time. .without_time() .init(); // Create OpenAI client let openai_api_key = env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY not set"); let openai_client = Client::new(&openai_api_key); let embedding_model = openai_client.embedding_model("text-embedding-ada-002"); // Create vector store, compute tool embeddings and load them in the store let mut vector_store = InMemoryVectorStore::default(); let toolset = ToolSet::builder() .dynamic_tool(Add) .dynamic_tool(Subtract) .build(); let embeddings = EmbeddingsBuilder::new(embedding_model.clone()) .tools(&toolset)? .build() .await?; vector_store.add_documents(embeddings).await?; // Create vector store index let index = vector_store.index(embedding_model); // Create RAG agent with a single context prompt and a dynamic tool source let calculator_rag = openai_client .agent("gpt-4o") .preamble("You are a calculator here to help the user perform arithmetic operations.") // Add a dynamic tool source with a sample rate of 1 (i.e.: only // 1 additional tool will be added to prompts) .dynamic_tools(1, index, toolset) .build(); // Prompt the agent and print the response let response = calculator_rag.prompt("Calculate 3 - 7").await?; println!("{}", response); Ok(()) } \`\`\` 13. Building sentiment classifiers with Rig: \`\`\`rust use rig::providers::openai; use schemars::JsonSchema; use serde::{Deserialize, Serialize}; #[derive(Debug, Deserialize, JsonSchema, Serialize)] /// An enum representing the sentiment of a document enum Sentiment { Positive, Negative, Neutral, } #[derive(Debug, Deserialize, JsonSchema, Serialize)] struct DocumentSentiment { /// The sentiment of the document sentiment: Sentiment, } #[tokio::main] async fn main() { // Create OpenAI client let openai_client = openai::Client::from_env(); // Create extractor let data_extractor = openai_client .extractor::<DocumentSentiment>("gpt-4o") .build(); let sentiment = data_extractor .extract("I am happy") .await .expect("Failed to extract sentiment"); println!("GPT-4: {:?}", sentiment); } \`\`\` 14. Simple vector search with Rig: \`\`\`rust use std::env; use rig::{ embeddings::{DocumentEmbeddings, EmbeddingsBuilder}, providers::openai::Client, vector_store::{in_memory_store::InMemoryVectorIndex, VectorStoreIndex}, }; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { // Create OpenAI client let openai_api_key = env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY not set"); let openai_client = Client::new(&openai_api_key); let model = openai_client.embedding_model("text-embedding-ada-002"); let embeddings = EmbeddingsBuilder::new(model.clone()) .simple_document("doc0", "Definition of a *flurbo*: A flurbo is a green alien that lives on cold planets") .simple_document("doc1", "Definition of a *glarb-glarb*: A glarb-glarb is a ancient tool used by the ancestors of the inhabitants of planet Jiro to farm the land.") .simple_document("doc2", "Definition of a *linglingdong*: A term used by inhabitants of the far side of the moon to describe humans.") .build() .await?; let index = InMemoryVectorIndex::from_embeddings(model, embeddings).await?; let results = index .top_n::<DocumentEmbeddings>("What is a linglingdong?", 1) .await? .into_iter() .map(|(score, id, doc)| (score, id, doc.document)) .collect::<Vec<_>>(); println!("Results: {:?}", results); let id_results = index .top_n_ids("What is a linglingdong?", 1) .await? .into_iter() .map(|(score, id)| (score, id)) .collect::<Vec<_>>(); println!("ID results: {:?}", id_results); Ok(()) } \`\`\` 15. Building cohere vector search with Rig: \`\`\`rust use std::env; use rig::{ embeddings::{DocumentEmbeddings, EmbeddingsBuilder}, providers::cohere::Client, vector_store::{in_memory_store::InMemoryVectorStore, VectorStore, VectorStoreIndex}, }; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { // Create Cohere client let cohere_api_key = env::var("COHERE_API_KEY").expect("COHERE_API_KEY not set"); let cohere_client = Client::new(&cohere_api_key); let document_model = cohere_client.embedding_model("embed-english-v3.0", "search_document"); let search_model = cohere_client.embedding_model("embed-english-v3.0", "search_query"); let mut vector_store = InMemoryVectorStore::default(); let embeddings = EmbeddingsBuilder::new(document_model) .simple_document("doc0", "Definition of a *flurbo*: A flurbo is a green alien that lives on cold planets") .simple_document("doc1", "Definition of a *glarb-glarb*: A glarb-glarb is a ancient tool used by the ancestors of the inhabitants of planet Jiro to farm the land.") .simple_document("doc2", "Definition of a *linglingdong*: A term used by inhabitants of the far side of the moon to describe humans.") .build() .await?; vector_store.add_documents(embeddings).await?; let index = vector_store.index(search_model); let results = index .top_n::<DocumentEmbeddings>("What is a linglingdong?", 1) .await? .into_iter() .map(|(score, id, doc)| (score, id, doc.document)) .collect::<Vec<_>>(); println!("Results: {:?}", results); Ok(()) } \`\`\`
```

# plugins/discord/documents/Rig_examples.md

```md
# Rig Examples This document provides a collection of examples demonstrating various features and use cases of the Rig library for building LLM-powered applications in Rust. ## 1. Building a Simple Agent \`\`\`rust use rig::{completion::Prompt, providers::openai}; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { let openai_client = openai::Client::from_env(); let comedian_agent = openai_client .agent("gpt-4o") .preamble("You are a comedian here to entertain the user using humor and jokes.") .build(); let response = comedian_agent.prompt("Tell me a joke about programming.").await?; println!("{}", response); Ok(()) } \`\`\` ## 2. Creating a Custom Tool \`\`\`rust use rig::{completion::ToolDefinition, tool::Tool}; use serde::{Deserialize, Serialize}; use serde_json::json; #[derive(Deserialize)] struct WeatherArgs { city: String, } #[derive(Debug, thiserror::Error)] #[error("Weather API error")] struct WeatherError; #[derive(Serialize)] struct WeatherInfo { temperature: f32, condition: String, } struct WeatherTool; impl Tool for WeatherTool { const NAME: &'static str = "get_weather"; type Error = WeatherError; type Args = WeatherArgs; type Output = WeatherInfo; async fn definition(&self, _prompt: String) -> ToolDefinition { ToolDefinition { name: Self::NAME.to_string(), description: "Get current weather for a city".to_string(), parameters: json!({ "type": "object", "properties": { "city": { "type": "string", "description": "The city to get weather for" } }, "required": ["city"] }), } } async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> { // In a real implementation, you would call a weather API here Ok(WeatherInfo { temperature: 22.5, condition: "Sunny".to_string(), }) } } \`\`\` ## 3. Using Different Models (OpenAI and Cohere) \`\`\`rust use rig::{completion::Prompt, providers::{openai, cohere}}; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { let openai_client = openai::Client::from_env(); let cohere_client = cohere::Client::new(&std::env::var("COHERE_API_KEY")?); let gpt4 = openai_client.agent("gpt-4o").build(); let command = cohere_client.agent("command").build(); let gpt4_response = gpt4.prompt("Explain quantum computing").await?; let command_response = command.prompt("Explain quantum computing").await?; println!("GPT-4: {}", gpt4_response); println!("Cohere Command: {}", command_response); Ok(()) } \`\`\` ## 4. Chaining Agents \`\`\`rust use rig::{completion::{Chat, Message}, providers::openai, agent::Agent}; struct TranslatorAgent { translator: Agent<openai::CompletionModel>, responder: Agent<openai::CompletionModel>, } impl TranslatorAgent { fn new(openai_client: &openai::Client) -> Self { Self { translator: openai_client.agent("gpt-4o") .preamble("You are a translator. Translate the input to English.") .build(), responder: openai_client.agent("gpt-4o") .preamble("You are a helpful assistant. Respond to the user's question.") .build(), } } } impl Chat for TranslatorAgent { async fn chat(&self, prompt: &str, chat_history: Vec<Message>) -> Result<String, rig::completion::PromptError> { let translated = self.translator.chat(prompt, vec![]).await?; self.responder.chat(&translated, chat_history).await } } #[tokio::main] async fn main() -> Result<(), anyhow::Error> { let openai_client = openai::Client::from_env(); let agent = TranslatorAgent::new(&openai_client); let response = agent.chat("Bonjour, comment ça va?", vec![]).await?; println!("Response: {}", response); Ok(()) } \`\`\` ## 5. RAG Agent with Dynamic Tools \`\`\`rust use rig::{ providers::openai, embeddings::EmbeddingsBuilder, vector_store::{in_memory_store::InMemoryVectorStore, VectorStore}, tool::{Tool, ToolSet}, }; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { let openai_client = openai::Client::from_env(); let embedding_model = openai_client.embedding_model(openai::TEXT_EMBEDDING_ADA_002); // Create vector store and add documents let mut vector_store = InMemoryVectorStore::default(); let embeddings = EmbeddingsBuilder::new(embedding_model.clone()) .simple_document("doc1", "Rig is a Rust library for building LLM applications.") .simple_document("doc2", "Rig supports OpenAI and Cohere as LLM providers.") .build() .await?; vector_store.add_documents(embeddings).await?; // Create dynamic tools let toolset = ToolSet::builder() .dynamic_tool(WeatherTool) // Add more dynamic tools here .build(); // Create RAG agent with dynamic tools let rag_agent = openai_client.agent("gpt-4o") .preamble("You are an assistant that can answer questions about Rig and check the weather.") .dynamic_context(2, vector_store.index(embedding_model.clone())) .dynamic_tools(1, vector_store.index(embedding_model), toolset) .build(); let response = rag_agent.prompt("What is Rig and what's the weather like in New York?").await?; println!("RAG Agent: {}", response); Ok(()) } \`\`\` ## 6. Using Extractors \`\`\`rust use rig::providers::openai; use schemars::JsonSchema; use serde::{Deserialize, Serialize}; #[derive(Debug, Deserialize, JsonSchema, Serialize)] struct Person { name: String, age: u8, occupation: String, } #[tokio::main] async fn main() -> Result<(), anyhow::Error> { let openai_client = openai::Client::from_env(); let extractor = openai_client.extractor::<Person>("gpt-4o").build(); let text = "John Doe is a 30-year-old software engineer."; let person = extractor.extract(text).await?; println!("Extracted person: {:?}", person); Ok(()) } \`\`\` ## 7. Text Classification System \`\`\`rust use rig::providers::openai; use schemars::JsonSchema; use serde::{Deserialize, Serialize}; #[derive(Debug, Deserialize, JsonSchema, Serialize)] enum Sentiment { Positive, Negative, Neutral, } #[derive(Debug, Deserialize, JsonSchema, Serialize)] struct SentimentClassification { sentiment: Sentiment, confidence: f32, } #[tokio::main] async fn main() -> Result<(), anyhow::Error> { let openai_client = openai::Client::from_env(); let classifier = openai_client .extractor::<SentimentClassification>("gpt-4o") .preamble("Classify the sentiment of the given text as Positive, Negative, or Neutral.") .build(); let text = "I love using Rig for building LLM applications!"; let classification = classifier.extract(text).await?; println!("Sentiment: {:?}, Confidence: {}", classification.sentiment, classification.confidence); Ok(()) } \`\`\` ## 8. Multi-Agent System \`\`\`rust use rig::{completion::{Chat, Message}, providers::openai, agent::Agent}; struct DebateAgents { agent_a: Agent<openai::CompletionModel>, agent_b: Agent<openai::CompletionModel>, } impl DebateAgents { fn new(openai_client: &openai::Client) -> Self { Self { agent_a: openai_client.agent("gpt-4o") .preamble("You are debating in favor of renewable energy.") .build(), agent_b: openai_client.agent("gpt-4o") .preamble("You are debating in favor of nuclear energy.") .build(), } } async fn debate(&self, rounds: usize) -> Result<(), anyhow::Error> { let mut history_a = vec![]; let mut history_b = vec![]; for i in 0..rounds { println!("Round {}:", i + 1); let response_a = self.agent_a.chat("Present your argument", history_a.clone()).await?; println!("Agent A: {}", response_a); history_b.push(Message { role: "user".into(), content: response_a }); let response_b = self.agent_b.chat("Respond to the argument", history_b.clone()).await?; println!("Agent B: {}", response_b); history_a.push(Message { role: "user".into(), content: response_b }); } Ok(()) } } #[tokio::main] async fn main() -> Result<(), anyhow::Error> { let openai_client = openai::Client::from_env(); let debate = DebateAgents::new(&openai_client); debate.debate(3).await?; Ok(()) } \`\`\` ## 9. Vector Search with Cohere \`\`\`rust use rig::{ providers::cohere, embeddings::EmbeddingsBuilder, vector_store::{in_memory_store::InMemoryVectorStore, VectorStore, VectorStoreIndex}, }; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { let cohere_client = cohere::Client::new(&std::env::var("COHERE_API_KEY")?); let document_model = cohere_client.embedding_model(cohere::EMBED_ENGLISH_V3, "search_document"); let search_model = cohere_client.embedding_model(cohere::EMBED_ENGLISH_V3, "search_query"); let mut vector_store = InMemoryVectorStore::default(); let embeddings = EmbeddingsBuilder::new(document_model) .simple_document("doc1", "Rig is a Rust library for building LLM applications.") .simple_document("doc2", "Rig supports various LLM providers and vector stores.") .build() .await?; vector_store.add_documents(embeddings).await?; let index = vector_store.index(search_model); let results = index.top_n::<String>("What is Rig?", 1).await?; for (score, id, doc) in results { println!("Score: {}, ID: {}, Document: {}", score, id, doc); } Ok(()) } \`\`\` ## 10. Cohere Connectors \`\`\`rust use rig::{completion::Completion, providers::cohere::Client as CohereClient}; use serde_json::json; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { let cohere_client = CohereClient::new(&std::env::var("COHERE_API_KEY")?); let agent = cohere_client .agent("command-r") .temperature(0.0) .additional_params(json!({ "connectors": [{"id":"web-search", "options":{"site": "https://docs.rs/rig-core"}}] })) .build(); let response = agent .completion("What are the main features of Rig?", vec![]) .await? .additional_params(json!({ "connectors": [{"id":"web-search", "options":{"site": "https://docs.rs/rig-core"}}] })) .send() .await?; println!("Response: {:?}", response.choice); println!("Citations: {:?}", response.raw_response.citations); Ok(()) } \`\`\` ## 11. Calculator Chatbot \`\`\`rust use rig::{ cli_chatbot::cli_chatbot, completion::ToolDefinition, providers::openai::Client, tool::Tool, }; use serde::{Deserialize, Serialize}; use serde_json::json; #[derive(Deserialize)] struct CalculatorArgs { x: f64, y: f64, operation: String, } #[derive(Debug, thiserror::Error)] #[error("Math error")] struct MathError; #[derive(Deserialize, Serialize)] struct Calculator; impl Tool for Calculator { const NAME: &'static str = "calculate"; type Error = MathError; type Args = CalculatorArgs; type Output = f64; async fn definition(&self, _prompt: String) -> ToolDefinition { ToolDefinition { name: Self::NAME.to_string(), description: "Perform basic arithmetic operations".to_string(), parameters: json!({ "type": "object", "properties": { "x": { "type": "number", "description": "First number" }, "y": { "type": "number", "description": "Second number" }, "operation": { "type": "string", "enum": ["add", "subtract", "multiply", "divide"], "description": "Arithmetic operation to perform" } }, "required": ["x", "y", "operation"] }), } } async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> { match args.operation.as_str() { "add" => Ok(args.x + args.y), "subtract" => Ok(args.x - args.y), "multiply" => Ok(args.x * args.y), "divide" => { if args.y == 0.0 { Err(MathError) } else { Ok(args.x / args.y) } }, _ => Err(MathError), } } } #[tokio::main] async fn main() -> Result<(), anyhow::Error> { let openai_client = Client::from_env(); let calculator_agent = openai_client .agent("gpt-4o") .preamble("You are a calculator assistant. Use the calculate tool to perform arithmetic operations.") .tool(Calculator) .build(); cli_chatbot(calculator_agent).await?; Ok(()) } \`\`\` ## 12. Using Anthropic's Claude Models Rig also supports Anthropic's Claude models. Here's an example of how to use them: \`\`\`rust use rig::{ completion::Prompt, providers::anthropic::{self, ClientBuilder, CLAUDE_3_5_SONNET}, }; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { let anthropic_client = ClientBuilder::new(&std::env::var("ANTHROPIC_API_KEY")?) .anthropic_version(anthropic::ANTHROPIC_VERSION_LATEST) .build(); let agent = anthropic_client .agent(CLAUDE_3_5_SONNET) .preamble("Be precise and concise.") .temperature(0.5) .max_tokens(8192) .build(); let response = agent .prompt("Explain the key features of the Rig library for Rust.") .await?; println!("Claude: {}", response); Ok(()) } \`\`\` ## 13. Using Perplexity Models Rig also supports Perplexity AI models. Here's an example: \`\`\`rust use rig::{ completion::Prompt, providers::perplexity::{self, Client, LLAMA_3_1_70B_INSTRUCT}, }; use serde_json::json; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { let perplexity_client = Client::new(&std::env::var("PERPLEXITY_API_KEY")?); let agent = perplexity_client .agent(LLAMA_3_1_70B_INSTRUCT) .preamble("Be precise and concise.") .temperature(0.5) .additional_params(json!({ "return_related_questions": true, "return_images": true })) .build(); let response = agent .prompt("What are the main benefits of using Rig for LLM applications?") .await?; println!("Perplexity: {}", response); Ok(()) } \`\`\` ## 14. Using LanceDB for Vector Storage Rig supports LanceDB for efficient vector storage. Here's an example of how to use it: \`\`\`rust use std::sync::Arc; use arrow_array::RecordBatchIterator; use rig::{ embeddings::{EmbeddingModel, EmbeddingsBuilder}, providers::openai::{Client, TEXT_EMBEDDING_ADA_002}, vector_store::VectorStoreIndex, }; use rig_lancedb::{LanceDbVectorStore, SearchParams}; use serde::Deserialize; #[derive(Deserialize, Debug)] struct VectorSearchResult { id: String, content: String, } #[tokio::main] async fn main() -> Result<(), anyhow::Error> { let openai_client = Client::from_env(); let model = openai_client.embedding_model(TEXT_EMBEDDING_ADA_002); // Initialize LanceDB locally let db = lancedb::connect("data/lancedb-store").execute().await?; // Create embeddings let embeddings = EmbeddingsBuilder::new(model.clone()) .simple_document("doc1", "Rig is a Rust library for building LLM applications.") .simple_document("doc2", "Rig supports various LLM providers and vector stores.") .build() .await?; // Create table with embeddings let record_batch = rig_lancedb::as_record_batch(embeddings, model.ndims()); let table = db .create_table( "rig_docs", RecordBatchIterator::new(vec![record_batch], Arc::new(rig_lancedb::schema(model.ndims()))), ) .execute() .await?; // Create vector store let search_params = SearchParams::default(); let vector_store = LanceDbVectorStore::new(table, model, "id", search_params).await?; // Query the index let results = vector_store .top_n::<VectorSearchResult>("What is Rig?", 1) .await?; for (score, id, result) in results { println!("Score: {}, ID: {}, Content: {}", score, id, result.content); } Ok(()) } \`\`\` ## Key Features of Rig 1. **Multiple LLM Providers**: Rig supports various LLM providers, including OpenAI, Anthropic (Claude), Cohere, and Perplexity AI. 2. **Flexible Agent System**: Easy creation of AI agents with customizable preambles, tools, and dynamic context. 3. **Vector Stores**: Support for different vector stores, including in-memory and LanceDB, for efficient similarity search. 4. **Embeddings**: Built-in support for generating and managing embeddings from various models. 5. **Tools and Function Calling**: Ability to define custom tools and use function calling with LLMs. 6. **RAG (Retrieval-Augmented Generation)**: Easy implementation of RAG systems with dynamic context and tools. 7. **Extractors**: Simplifies the process of extracting structured data from text using LLMs. 8. **Multi-Agent Systems**: Facilitates the creation of systems with multiple interacting AI agents. 9. **Connectors**: Support for external data sources, like Cohere's web connectors. 10. **CLI Chatbots**: Utility functions for creating command-line interface chatbots. 11. **Async/Await**: Built with Rust's async/await paradigm for efficient concurrent operations. 12. **Type Safety**: Leverages Rust's strong type system for robust and safe LLM application development. ## Best Practices When Using Rig 1. **Environment Variables**: Always use environment variables for API keys instead of hardcoding them. 2. **Error Handling**: Make use of Rust's robust error handling with `Result` types and the `?` operator. 3. **Model Selection**: Choose the appropriate model for your task. More powerful models like GPT-4 or Claude 3 Opus are better for complex reasoning, while smaller models may be sufficient for simpler tasks. 4. **Prompt Engineering**: Craft clear and specific prompts. Use the `preamble` method to set the overall context and behavior of your agents. 5. **Tools**: Implement tools for specific functionalities to extend the capabilities of your agents. 6. **Vector Stores**: Use vector stores for efficient similarity search when working with large amounts of data. 7. **Embeddings**: Generate embeddings once and store them, rather than regenerating them for each query. 8. **Rate Limiting**: Be aware of rate limits for different LLM providers and implement appropriate waiting or retrying mechanisms. 9. **Testing**: Write unit and integration tests for your Rig-based applications to ensure reliability. 10. **Modularity**: Design your application with modularity in mind, separating concerns like model initialization, agent creation, and business logic. By following these best practices and leveraging Rig's features, you can build powerful, efficient, and maintainable LLM-powered applications in Rust.
```

# plugins/discord/documents/Rig_faq.md

```md
Certainly! Here's a long list of question-answer pairs in the style of synthetic data for fine-tuning a model, specifically focused on Rig and its capabilities: 1. Q: What is Rig? A: Rig is an open-source Rust library designed to simplify the development of applications powered by Large Language Models (LLMs). It provides a unified API for working with different LLM providers, supports advanced AI workflows, and offers flexible abstractions for building complex AI systems. 2. Q: Which LLM providers does Rig support? A: Rig currently supports OpenAI and Cohere as LLM providers. It offers a unified API that allows developers to easily switch between these providers or use multiple providers in the same project. 3. Q: How do I create a simple agent using Rig? A: To create a simple agent using Rig, you can use the following code: \`\`\`rust let agent = openai_client.agent("gpt-4o") .preamble("You are a helpful assistant.") .build(); \`\`\` 4. Q: What is the purpose of the `preamble` in Rig agents? A: The `preamble` in Rig agents serves as a system prompt or context for the agent. It defines the agent's role, behavior, and any specific instructions or knowledge it should have. 5. Q: How can I implement a custom tool in Rig? A: To implement a custom tool in Rig, you need to create a struct that implements the `Tool` trait. This involves defining methods like `definition` for describing the tool and `call` for executing the tool's functionality. 6. Q: What is a RAG system in Rig? A: A RAG (Retrieval-Augmented Generation) system in Rig combines an LLM with a vector store for context retrieval. It allows the agent to access relevant information from a knowledge base when generating responses. 7. Q: How do I set up a vector store in Rig? A: You can set up a vector store in Rig using the `InMemoryVectorStore` or by implementing the `VectorStore` trait for a custom storage solution. Here's a basic example: \`\`\`rust let mut vector_store = InMemoryVectorStore::default(); vector_store.add_documents(embeddings).await?; \`\`\` 8. Q: What is the purpose of the `EmbeddingsBuilder` in Rig? A: The `EmbeddingsBuilder` in Rig is used to create embeddings for documents efficiently. It allows you to batch multiple documents for embedding generation, which is more efficient than processing them individually. 9. Q: How can I use different models within the same Rig application? A: Rig allows you to create multiple model instances, even from different providers. For example: \`\`\`rust let gpt4 = openai_client.model("gpt-4o").build(); let command = cohere_client.model("command").build(); \`\`\` 10. Q: What is the difference between `Agent` and `Model` in Rig? A: In Rig, a `Model` represents a raw LLM model, while an `Agent` combines a model with additional context (preamble) and potentially tools. Agents provide a higher-level abstraction for building AI assistants. 11. Q: How does Rig handle errors in LLM interactions? A: Rig provides custom error types like `CompletionError` and `EmbeddingError` for handling errors in LLM interactions. These allow for more specific error handling and propagation in your application. 12. Q: Can I use Rig for streaming responses from LLMs? A: Yes, Rig supports streaming responses for long-running tasks. You can use the `completion_stream` method to receive chunks of the response as they are generated by the LLM. 13. Q: What is the purpose of the `Tool` trait in Rig? A: The `Tool` trait in Rig defines the interface for custom functionalities that can be used by agents. It allows you to extend the capabilities of your AI assistants with specific actions or integrations. 14. Q: How can I implement a multi-agent system using Rig? A: You can implement a multi-agent system in Rig by creating multiple agent instances and orchestrating their interactions in your application logic. Each agent can have its own role and capabilities. 15. Q: What is the `Extractor` in Rig used for? A: The `Extractor` in Rig is used for structured data extraction from text. It allows you to define a schema for the data you want to extract and uses an LLM to parse the information into that structure. 16. Q: How does Rig support text classification tasks? A: Rig supports text classification tasks through its `Extractor` functionality. You can define an enum or struct representing your classification categories and use an LLM to classify text into these categories. 17. Q: Can I use Rig with my own custom vector store implementation? A: Yes, you can use Rig with a custom vector store implementation by implementing the `VectorStore` trait for your storage solution. This allows you to integrate Rig with various database systems or specialized vector stores. 18. Q: How does Rig handle API rate limiting? A: Rig itself doesn't directly handle API rate limiting, but it's designed to work well with rate limiting strategies. You can implement retries with exponential backoff in your application logic when using Rig's API calls. 19. Q: What is the purpose of the `additional_params` in Rig's completion requests? A: The `additional_params` in Rig's completion requests allow you to pass provider-specific parameters to the LLM. This enables fine-tuning of the request beyond Rig's standard parameters. 20. Q: How can I use Rig with Cohere's web connectors? A: You can use Rig with Cohere's web connectors by adding the connector information to the `additional_params` when creating an agent or sending a completion request. For example: \`\`\`rust .additional_params(json!({ "connectors": [{"id":"web-search", "options":{"site": "https://docs.rs/rig-core"}}] })) \`\`\` 21. Q: What is the difference between static and dynamic tools in Rig? A: Static tools in Rig are always available to an agent, while dynamic tools are retrieved from a vector store based on the current context. Dynamic tools allow for more flexible and context-aware tool usage. 22. Q: How does Rig handle context management in conversations? A: Rig allows you to manage conversation context through the `chat` method, which accepts a vector of previous messages. You can accumulate and pass the conversation history to maintain context across multiple interactions. 23. Q: Can I use Rig for fine-tuning LLMs? A: Rig currently doesn't provide direct support for fine-tuning LLMs. Its primary focus is on using pre-trained models efficiently. However, you can use Rig in conjunction with provider-specific fine-tuning processes. 24. Q: How does Rig ensure type safety when working with LLMs? A: Rig leverages Rust's strong type system to ensure type safety. It uses traits like `CompletionModel` and `EmbeddingModel` to define clear interfaces, and employs generics and type parameters to maintain type safety across different operations. 25. Q: What is the role of the `VectorStoreIndex` in Rig? A: The `VectorStoreIndex` in Rig provides methods for efficient similarity search within a vector store. It's used in RAG systems to retrieve relevant context based on the similarity between the query and stored documents. 26. Q: How can I implement a chatbot using Rig? A: Rig provides a `cli_chatbot` utility that you can use to quickly implement a command-line chatbot. Alternatively, you can create your own chatbot logic using Rig's `Chat` trait and agent functionality. 27. Q: What is the purpose of the `JsonSchema` derive macro often used with Rig? A: The `JsonSchema` derive macro is used in conjunction with Rig's `Extractor` functionality. It allows Rig to generate a JSON schema for your Rust types, which is then used to guide the LLM in extracting structured data. 28. Q: How does Rig handle asynchronous operations? A: Rig is designed to work with Rust's async ecosystem. It uses `async` functions throughout its API, allowing for efficient handling of I/O-bound operations like API calls to LLM providers. 29. Q: Can I use Rig in a web application? A: Yes, Rig can be used in web applications. While it doesn't provide web-specific functionality, its async design makes it suitable for use with web frameworks like Actix or Rocket. 30. Q: How does Rig compare to other LLM libraries? A: Rig differentiates itself by providing a unified API across different LLM providers, offering high-level abstractions like agents and RAG systems, and leveraging Rust's performance and safety features. It's designed to be extensible and integrate well with the Rust ecosystem. 31. Q: How does Rig handle token limits for LLM providers? A: Rig doesn't automatically handle token limits, but it allows you to set `max_tokens` when creating completion requests. It's up to the developer to manage token usage within the provider's limits. 32. Q: Can I use Rig with local LLM models? A: While Rig primarily supports cloud-based LLM providers, you could potentially implement the `CompletionModel` trait for a local model. However, this would require significant custom implementation. 33. Q: How does Rig support prompt engineering? A: Rig supports prompt engineering through its `preamble` feature in agents and the ability to customize completion requests. You can craft and refine prompts to guide the LLM's behavior effectively. 34. Q: What's the difference between `prompt` and `chat` methods in Rig? A: The `prompt` method is for single-turn interactions, while `chat` is for multi-turn conversations. `chat` allows you to pass in conversation history for context. 35. Q: How can I implement a custom embedding model in Rig? A: You can implement a custom embedding model by creating a struct that implements the `EmbeddingModel` trait. This would involve defining methods for embedding generation and specifying the maximum number of documents that can be processed at once. 36. Q: Does Rig support function calling features of LLMs? A: Yes, Rig supports function calling through its tool system. You can define tools that the LLM can "call" to perform specific actions or retrieve information. 37. Q: How does Rig handle concurrent requests to LLM providers? A: Rig is built on Rust's async ecosystem, which allows for efficient handling of concurrent requests. However, actual concurrency limits would depend on the specific LLM provider's API constraints. 38. Q: Can I use Rig for document summarization tasks? A: Yes, you can use Rig for document summarization. You could create an agent with a custom prompt designed for summarization, potentially using RAG for longer documents. 39. Q: How does Rig support semantic search? A: Rig supports semantic search through its vector store and embedding functionalities. You can embed documents and queries, then use vector similarity to find semantically related content. 40. Q: Can I use Rig with multiple LLM providers in the same application? A: Yes, Rig's design allows you to use multiple LLM providers in the same application. You can create different clients for each provider and use them as needed. 41. Q: How does Rig handle versioning of LLM models? A: Rig allows you to specify the model version when creating a completion model. It's up to the developer to manage and update model versions as needed. 42. Q: Can I use Rig for few-shot learning tasks? A: Yes, you can implement few-shot learning with Rig by including examples in your prompt or preamble when creating an agent or sending a completion request. 43. Q: How does Rig support debugging of LLM interactions? A: Rig doesn't provide built-in debugging tools, but its error types and the ability to inspect raw responses can aid in debugging. You can also implement your own logging or debugging mechanisms around Rig's API calls. 44. Q: Can I use Rig with Azure OpenAI services? A: While Rig doesn't have built-in support for Azure OpenAI, you could potentially implement a custom client that uses Azure OpenAI's API while conforming to Rig's traits and interfaces. 45. Q: How does Rig handle retries for failed API calls? A: Rig doesn't automatically handle retries. Implementing retry logic would be the responsibility of the application using Rig, possibly using a crate like `tokio-retry`. 46. Q: Can I use Rig for implementing a question-answering system? A: Yes, Rig is well-suited for building question-answering systems. You could use a RAG agent to retrieve relevant context and generate answers based on that context. 47. Q: How does Rig support prompt templating? A: Rig doesn't have a built-in prompt templating system, but you can implement your own templating logic when constructing prompts or preambles for agents. 48. Q: Can I use Rig for implementing a chatbot with personality? A: Yes, you can create a chatbot with a specific personality using Rig. You would define the personality in the agent's preamble and potentially through carefully crafted prompts. 49. Q: How does Rig handle API authentication for different providers? A: Rig typically uses API keys for authentication, which are provided when creating a client for a specific provider. The authentication process is abstracted away from the user once the client is set up. 50. Q: Can I use Rig for implementing a code generation tool? A: Yes, you can use Rig to implement a code generation tool. You would create an agent with appropriate prompts and potentially use tools to handle specific coding tasks or language features. 51. Q: How does Rig support working with multiple languages? A: Rig itself is language-agnostic when it comes to the text it processes. Support for multiple languages would primarily depend on the capabilities of the underlying LLM models being used. 52. Q: Can I use Rig for implementing a text-to-SQL tool? A: Yes, you could implement a text-to-SQL tool using Rig. You'd create an agent with appropriate prompts for SQL generation, and potentially use tools to validate or execute the generated SQL. 53. Q: How does Rig handle long documents that exceed token limits? A: Rig doesn't automatically handle document chunking. For long documents, you would need to implement your own logic to split the document into appropriate chunks, possibly using a sliding window approach with overlap. 54. Q: Can I use Rig with custom tokenizers? A: Rig uses the tokenizers provided by the LLM providers. If you need to use a custom tokenizer, you would need to implement that at the application level, outside of Rig's direct functionality. 55. Q: How does Rig support A/B testing of different prompts or models? A: Rig doesn't have built-in A/B testing functionality, but its flexible design allows you to implement A/B testing at the application level, creating different agents or completion requests for comparison. 56. Q: Can I use Rig for implementing a sentiment analysis tool? A: Yes, you can implement a sentiment analysis tool using Rig. You could use the `Extractor` functionality to classify text into sentiment categories, or create a custom tool for sentiment analysis. 57. Q: How does Rig handle caching of LLM responses? A: Rig doesn't provide built-in caching. If you need to cache LLM responses, you would implement this at the application level, possibly using a crate like `cached` or a database for persistence. 58. Q: Can I use Rig with quantized models? A: Rig's support for quantized models would depend on the LLM provider's API. As long as the provider exposes quantized models through their standard API, you should be able to use them with Rig. 59. Q: How does Rig support content moderation? A: Rig doesn't have built-in content moderation features. You would need to implement content moderation either by creating a custom tool, using provider-specific moderation APIs, or post-processing LLM outputs. 60. Q: Can I use Rig for implementing a text classification pipeline? A: Yes, you can implement a text classification pipeline using Rig. You could use the `Extractor` functionality or create a custom agent designed for classification tasks. 61. Q: How does Rig handle context window management for long conversations? A: Rig doesn't automatically manage context windows. For long conversations, you'd need to implement a custom solution, potentially using a sliding window approach or summarizing previous context. You could create a wrapper around Rig's `Chat` trait to handle this. 62. Q: Can Rig be used for implementing a federated learning system with LLMs? A: While Rig doesn't have built-in support for federated learning, you could potentially use it as part of a federated system. You'd need to implement the federated learning logic separately, using Rig to interact with LLMs for the learning process. 63. Q: How can I implement custom attention mechanisms using Rig? A: Rig doesn't provide direct access to model internals like attention mechanisms. However, you could simulate custom attention by carefully constructing prompts or by implementing a custom `CompletionModel` that incorporates your attention mechanism before calling the LLM. 64. Q: Can Rig be used for implementing a meta-learning system? A: Yes, you could use Rig as part of a meta-learning system. You'd likely create multiple agents with different configurations, use them to solve tasks, and then have a meta-agent that learns to select or combine these agents effectively. 65. Q: How does Rig support multi-modal AI systems? A: Rig is primarily designed for text-based LLMs. For multi-modal systems, you'd need to handle other modalities (like images or audio) separately and then integrate that with Rig's text capabilities, possibly using custom tools to bridge the modalities. 66. Q: Can Rig be used for implementing a hierarchical planning system? A: Yes, you could implement a hierarchical planning system using Rig. You might create multiple agents for different levels of planning, using tools to decompose high-level plans into more detailed sub-plans. 67. Q: How can I implement a system for detecting and mitigating LLM hallucinations using Rig? A: You could create a pipeline of agents: one to generate responses, another to fact-check or critique those responses, and a third to synthesize or correct based on the critique. You'd also likely use RAG to ground the responses in factual information. 68. Q: Can Rig be used for implementing a system that combines symbolic AI with neural approaches? A: Yes, Rig can be part of a neuro-symbolic system. You could use Rig's LLM capabilities for the neural part, and implement symbolic reasoning as custom tools. The agent would then serve as the interface between these two paradigms. 69. Q: How can I implement dynamic prompt generation using Rig? A: You could create a meta-agent responsible for generating prompts. This agent would take high-level instructions and generate specific prompts, which are then passed to other agents or used in completion requests. 70. Q: Can Rig be used for implementing a system that performs multi-hop reasoning? A: Yes, you can implement multi-hop reasoning with Rig. You'd create an agent that breaks down complex queries into a series of simpler questions, potentially using tools to store intermediate results, and then synthesizes the final answer. 71. Q: How can I implement a system for detecting and mitigating biases in LLM outputs using Rig? A: You could create a pipeline with multiple agents: one to generate content, another trained to detect various types of biases, and a third to revise the content to mitigate detected biases. You might also implement custom tools for specific bias detection algorithms. 72. Q: Can Rig be used for implementing a system that performs counterfactual reasoning? A: Yes, you can implement counterfactual reasoning with Rig. You'd create prompts that explicitly ask the LLM to consider alternative scenarios. You might also implement custom tools to help generate and track counterfactual scenarios. 73. Q: How can I implement a system for automatic prompt optimization using Rig? A: You could create a meta-agent that generates and tests multiple prompts for a given task. Implement a custom tool to evaluate the performance of each prompt, and use another agent to iteratively refine the prompts based on these evaluations. 74. Q: Can Rig be used for implementing a system that performs analogical reasoning? A: Yes, Rig can be used for analogical reasoning. You'd create prompts that explicitly ask the LLM to draw analogies. You might also implement custom tools to store and retrieve known analogies, or to evaluate the strength of proposed analogies. 75. Q: How can I implement a system for automatic error correction in LLM outputs using Rig? A: You could create a pipeline with one agent to generate content, another agent trained to detect errors (factual, grammatical, logical, etc.), and a third agent to correct these errors. You might also implement custom tools for specific types of error checking. 76. Q: Can Rig be used for implementing a system that performs causal reasoning? A: Yes, you can implement causal reasoning with Rig. You'd create prompts that explicitly ask about cause-and-effect relationships. You might also implement custom tools to represent and manipulate causal graphs. 77. Q: How can I implement a system for automatic code review using Rig? A: You could create an agent with a prompt engineered for code review tasks. Implement custom tools for static code analysis, and use the agent to synthesize human-readable reviews based on the tool outputs and its own analysis. 78. Q: Can Rig be used for implementing a system that performs temporal reasoning? A: Yes, Rig can be used for temporal reasoning. You'd create prompts that explicitly handle temporal concepts. You might also implement custom tools to represent and manipulate timelines or temporal logic statements. 79. Q: How can I implement a system for automatic data augmentation using Rig? A: You could create an agent that takes existing data examples and generates variations or new examples. Implement custom tools to validate the generated examples and ensure they meet specific criteria for your augmentation needs. 80. Q: Can Rig be used for implementing a system that performs abductive reasoning? A: Yes, you can implement abductive reasoning with Rig. Create prompts that ask the LLM to generate the best explanations for given observations. You might implement custom tools to evaluate the plausibility of different explanations. 81. Q: How can I implement a system for automatic ontology construction using Rig? A: Create an agent that extracts concepts and relationships from text. Implement custom tools to represent and manipulate ontological structures. Use another agent to refine and validate the constructed ontology. 82. Q: Can Rig be used for implementing a system that performs meta-cognition? A: Yes, you can implement meta-cognitive capabilities using Rig. Create agents that not only perform tasks but also reflect on their own performance, generating explanations for their reasoning and identifying areas of uncertainty. 83. Q: How can I implement a system for automatic theorem proving using Rig? A: While Rig isn't designed for formal theorem proving, you could create an agent that generates proof strategies. Implement custom tools for formal logic manipulation, and use the agent to guide the proof process, possibly in conjunction with a dedicated theorem prover. 84. Q: Can Rig be used for implementing a system that performs conceptual blending? A: Yes, you can implement conceptual blending with Rig. Create an agent that takes two or more concepts as input and generates novel combinations. Implement custom tools to evaluate the coherence and novelty of the blended concepts. 85. Q: How can I implement a system for automatic curriculum learning using Rig? A: Create a meta-agent that generates increasingly complex tasks. Implement custom tools to evaluate the performance of a learning agent on these tasks. Use another agent to adjust the curriculum based on the learning progress. 86. Q: Can Rig be used for implementing a system that performs non-monotonic reasoning? A: Yes, you can implement non-monotonic reasoning with Rig. Create prompts that allow for the retraction or modification of previous conclusions. Implement custom tools to manage a dynamic knowledge base that can be updated as new information arrives. 87. Q: How can I implement a system for automatic story generation using Rig? A: Create an agent with a prompt engineered for storytelling. Implement custom tools for managing plot structures, character development, and narrative coherence. Use multiple agents for different aspects of the story (e.g., plot, dialogue, descriptions). 88. Q: Can Rig be used for implementing a system that performs ethical reasoning? A: Yes, you can implement ethical reasoning with Rig. Create prompts that explicitly consider ethical principles and dilemmas. Implement custom tools to represent and reason about ethical frameworks. Use multiple agents to represent different ethical perspectives. 89. Q: How can I implement a system for automatic paraphrasing using Rig? A: Create an agent with a prompt designed for paraphrasing tasks. Implement custom tools to evaluate the semantic similarity between the original text and the paraphrase. Use another agent to iteratively refine the paraphrase based on similarity scores and other criteria. 90. Q: Can Rig be used for implementing a system that performs commonsense reasoning? A: Yes, you can implement commonsense reasoning with Rig. Create prompts that explicitly ask for commonsense inferences. Implement custom tools to access and query commonsense knowledge bases. Use RAG to ground the reasoning in a large body of general knowledge. 91. Q: How can I implement a system for automatic question generation using Rig? A: Create an agent with a prompt designed for question generation tasks. Implement custom tools to evaluate the quality and relevance of generated questions. Use another agent to refine the questions based on specific criteria (e.g., difficulty level, question type). 92. Q: Can Rig be used for implementing a system that performs defeasible reasoning? A: Yes, you can implement defeasible reasoning with Rig. Create prompts that allow for tentative conclusions that can be defeated by new information. Implement custom tools to manage a knowledge base of defeasible rules and exceptions. 93. Q: How can I implement a system for automatic text style transfer using Rig? A: Create multiple agents trained on different writing styles. Implement custom tools to analyze the stylistic features of text. Use one agent to decompose the content, another to transfer the style, and a third to ensure the transferred text maintains the original meaning. 94. Q: Can Rig be used for implementing a system that performs analogical problem-solving? A: Yes, you can implement analogical problem-solving with Rig. Create an agent that identifies structural similarities between a source problem and a target problem. Implement custom tools to map solutions from the source to the target domain. 95. Q: How can I implement a system for automatic metadata generation using Rig? A: Create an agent with a prompt designed to extract key information from content. Implement custom tools to validate and format the extracted metadata. Use another agent to enhance the metadata with additional relevant information from external sources. 96. Q: Can Rig be used for implementing a system that performs counterfactual explanation generation? A: Yes, you can implement counterfactual explanation generation with Rig. Create prompts that ask the LLM to identify minimal changes that would alter a prediction or outcome. Implement custom tools to validate the logical consistency of the generated counterfactuals. 97. Q: How can I implement a system for automatic text summarization with controllable attributes using Rig? A: Create an agent with a prompt designed for summarization tasks. Implement custom tools to measure various attributes of the summary (e.g., length, readability, focus on specific topics). Use another agent to iteratively refine the summary based on desired attribute values. 98. Q: Can Rig be used for implementing a system that performs multi-document synthesis? A: Yes, you can implement multi-document synthesis with Rig. Create an agent that extracts key information from multiple documents. Implement custom tools to detect and resolve conflicts between sources. Use another agent to synthesize a coherent output from the extracted information. 99. Q: How can I implement a system for automatic generation of explanations for black-box model predictions using Rig? A: Create an agent that generates human-readable explanations for model outputs. Implement custom tools to interface with the black-box model and extract relevant features. Use another agent to validate the explanations against the model's behavior. 100. Q: Can Rig be used for implementing a system that performs incremental learning? A: While Rig doesn't directly support model fine-tuning, you could implement a form of incremental learning. Create an agent that maintains a dynamic knowledge base, updating it with new information. Use this knowledge base in conjunction with RAG to inform the LLM's responses, effectively allowing it to "learn" new information over time. # more questions and answers 1. Q: How do I set the `max_tokens` parameter when using Rig? A: You can set the `max_tokens` parameter when building an agent or creating a completion request. For example: \`\`\`rust let agent = openai_client.agent("gpt-4o") .preamble("You are a helpful assistant.") .max_tokens(150) // Set max_tokens here .build(); \`\`\` Or when creating a completion request: \`\`\`rust let response = model.completion_request("Your prompt here") .max_tokens(100) .send() .await?; \`\`\` 2. Q: How can I adjust the temperature setting in Rig? A: You can set the temperature when building an agent or in a completion request: \`\`\`rust let agent = openai_client.agent("gpt-4o") .temperature(0.7) // Set temperature here .build(); \`\`\` 3. Q: Can I use Rig with streaming responses from LLMs? A: Yes, Rig supports streaming responses. You can use the `stream()` method on a completion request: \`\`\`rust let mut stream = model.completion_request("Your prompt") .stream() .await?; while let Some(chunk) = stream.next().await { println!("Chunk: {}", chunk?); } \`\`\` 4. Q: How do I handle rate limiting with Rig? A: Rig doesn't handle rate limiting internally. You should implement rate limiting in your application, possibly using a crate like `governor`: \`\`\`rust use governor::{Quota, RateLimiter}; use std::num::NonZeroU32; let limiter = RateLimiter::direct(Quota::per_minute(NonZeroU32::new(60).unwrap())); limiter.until_ready().await; // Then make your Rig API call \`\`\` 5. Q: How can I use Rig with a custom LLM provider? A: To use Rig with a custom LLM provider, you need to implement the `CompletionModel` trait for your provider: \`\`\`rust struct MyCustomModel; impl CompletionModel for MyCustomModel { type Response = MyCustomResponse; async fn completion(&self, request: CompletionRequest) -> Result<CompletionResponse<Self::Response>, CompletionError> { // Implement your custom logic here } } \`\`\` 6. Q: How do I use Rig's `Extractor` for structured data extraction? A: To use the `Extractor`, define a struct that represents your data structure and use the `extractor` method: \`\`\`rust #[derive(Deserialize, JsonSchema)] struct PersonInfo { name: String, age: u8, } let extractor = openai_client.extractor::<PersonInfo>("gpt-4o").build(); let result = extractor.extract("John Doe is 30 years old").await?; \`\`\` 7. Q: Can I use Rig with Azure OpenAI services? A: Rig doesn't have built-in support for Azure OpenAI, but you can create a custom client: \`\`\`rust struct AzureOpenAIClient { // fields for Azure-specific configuration } impl CompletionModel for AzureOpenAIClient { // Implement the trait methods to work with Azure OpenAI } \`\`\` 8. Q: How do I implement custom error handling with Rig? A: You can create custom error types and use them in your implementations: \`\`\`rust #[derive(Debug, thiserror::Error)] enum MyCustomError { #[error("API error: {0}")] ApiError(String), // other error variants } impl From<MyCustomError> for CompletionError { fn from(error: MyCustomError) -> Self { CompletionError::ProviderError(error.to_string()) } } \`\`\` 9. Q: How can I use Rig with a vector database like Pinecone? A: Implement the `VectorStore` trait for Pinecone: \`\`\`rust struct PineconeStore { // Pinecone client fields } impl VectorStore for PineconeStore { // Implement the required methods } \`\`\` 10. Q: How do I implement a custom `Tool` in Rig? A: Create a struct and implement the `Tool` trait: \`\`\`rust struct MyCustomTool; impl Tool for MyCustomTool { const NAME: &'static str = "my_custom_tool"; type Error = MyToolError; type Args = MyToolArgs; type Output = MyToolOutput; async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> { // Implement tool logic } async fn definition(&self, _prompt: String) -> ToolDefinition { // Define tool interface } } \`\`\` 11. Q: How can I use Rig for multi-turn conversations? A: Use the `chat` method and maintain a conversation history: \`\`\`rust let mut history = Vec::new(); loop { let user_input = get_user_input(); let response = agent.chat(&user_input, history.clone()).await?; history.push(Message { role: "user".into(), content: user_input }); history.push(Message { role: "assistant".into(), content: response.clone() }); println!("Assistant: {}", response); } \`\`\` 12. Q: How do I implement custom tokenization with Rig? A: Rig uses the tokenizers provided by LLM providers. For custom tokenization, you'd need to implement it at the application level: \`\`\`rust fn custom_tokenize(text: &str) -> Vec<String> { // Your custom tokenization logic } let tokenized = custom_tokenize(&user_input); let response = agent.prompt(&tokenized.join(" ")).await?; \`\`\` 13. Q: How can I use Rig for few-shot learning? A: Include examples in your prompt or preamble: \`\`\`rust let few_shot_agent = openai_client.agent("gpt-4o") .preamble(" Classify the sentiment of the text. Examples: Input: I love this product! Output: Positive Input: This is terrible. Output: Negative Now classify the following: ") .build(); \`\`\`
```

# plugins/discord/documents/Rig_guide.md

```md
# Comprehensive Guide to Rig: Rust Library for LLM-Powered Applications ## 1. Introduction to Rig Rig is an open-source Rust library designed to simplify the development of applications powered by Large Language Models (LLMs). It provides a unified API for working with different LLM providers, advanced AI workflow support, and flexible abstractions for building complex AI systems. Key features of Rig include: - Unified API across multiple LLM providers (e.g., OpenAI, Anthropic, Cohere, Perplexity) - Support for completion and embedding workflows - High-level abstractions for agents and RAG systems - Extensible architecture for custom implementations - Seamless integration with Rust's ecosystem - Vector store support, including in-memory and LanceDB options ## 2. Core Concepts ### 2.1 Completion Models Completion models are the foundation of LLM interactions in Rig. They implement the `CompletionModel` trait, which defines methods for generating completion requests and executing them. \`\`\`rust pub trait CompletionModel: Clone + Send + Sync { type Response: Send + Sync; fn completion( &self, request: CompletionRequest, ) -> impl std::future::Future<Output = Result<CompletionResponse<Self::Response>, CompletionError>> + Send; fn completion_request(&self, prompt: &str) -> CompletionRequestBuilder<Self>; } \`\`\` ### 2.2 Embedding Models Embedding models are used for generating vector representations of text. They implement the `EmbeddingModel` trait: \`\`\`rust pub trait EmbeddingModel: Clone + Sync + Send { const MAX_DOCUMENTS: usize; fn ndims(&self) -> usize; fn embed_documents( &self, documents: Vec<String>, ) -> impl std::future::Future<Output = Result<Vec<Embedding>, EmbeddingError>> + Send; } \`\`\` ### 2.3 Agents Agents in Rig combine an LLM model with a preamble (system prompt) and a set of tools. They are implemented using the `Agent` struct: \`\`\`rust pub struct Agent<M: CompletionModel> { model: M, preamble: String, static_context: Vec<Document>, static_tools: Vec<String>, temperature: Option<f64>, max_tokens: Option<u64>, additional_params: Option<serde_json::Value>, dynamic_context: Vec<(usize, Box<dyn VectorStoreIndexDyn>)>, dynamic_tools: Vec<(usize, Box<dyn VectorStoreIndexDyn>)>, pub tools: ToolSet, } \`\`\` ### 2.4 Tools Tools are functionalities that agents can use to perform specific tasks. They implement the `Tool` trait: \`\`\`rust pub trait Tool: Sized + Send + Sync { const NAME: &'static str; type Error: std::error::Error + Send + Sync + 'static; type Args: for<'a> Deserialize<'a> + Send + Sync; type Output: Serialize; fn name(&self) -> String; fn definition(&self, _prompt: String) -> impl Future<Output = ToolDefinition> + Send + Sync; fn call( &self, args: Self::Args, ) -> impl Future<Output = Result<Self::Output, Self::Error>> + Send + Sync; } \`\`\` ### 2.5 Vector Stores Vector stores are used for storing and retrieving embeddings. They implement the `VectorStore` trait: \`\`\`rust pub trait VectorStore: Send + Sync { type Q; fn add_documents( &mut self, documents: Vec<DocumentEmbeddings>, ) -> impl std::future::Future<Output = Result<(), VectorStoreError>> + Send; fn get_document_embeddings( &self, id: &str, ) -> impl std::future::Future<Output = Result<Option<DocumentEmbeddings>, VectorStoreError>> + Send; // Other methods... } \`\`\` ## 3. Building with Rig ### 3.1 Setting up a Project To start a new project with Rig, add it to your `Cargo.toml`: \`\`\`toml [dependencies] rig-core = "0.2.1" tokio = { version = "1.34.0", features = ["full"] } \`\`\` ### 3.2 Creating a Simple Agent Here's how to create and use a simple agent: \`\`\`rust use rig::{completion::Prompt, providers::openai}; #[tokio::main] async fn main() -> Result<(), anyhow::Error> { let openai_client = openai::Client::from_env(); let agent = openai_client .agent("gpt-4o") .preamble("You are a helpful assistant.") .build(); let response = agent.prompt("Explain quantum computing in one sentence.").await?; println!("Agent: {}", response); Ok(()) } \`\`\` ### 3.3 Implementing a Custom Tool Here's an example of implementing a custom tool: \`\`\`rust use rig::tool::Tool; use rig::completion::ToolDefinition; use serde::{Deserialize, Serialize}; use serde_json::json; #[derive(Deserialize)] struct AddArgs { x: i32, y: i32, } #[derive(Debug, thiserror::Error)] #[error("Math error")] struct MathError; #[derive(Deserialize, Serialize)] struct Adder; impl Tool for Adder { const NAME: &'static str = "add"; type Error = MathError; type Args = AddArgs; type Output = i32; async fn definition(&self, _prompt: String) -> ToolDefinition { ToolDefinition { name: "add".to_string(), description: "Add x and y together".to_string(), parameters: json!({ "type": "object", "properties": { "x": { "type": "number", "description": "The first number to add" }, "y": { "type": "number", "description": "The second number to add" } }, "required": ["x", "y"] }), } } async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> { Ok(args.x + args.y) } } \`\`\` ### 3.4 Creating an Agent with Tools Here's how to create an agent with custom tools: \`\`\`rust let agent = openai_client.agent("gpt-4o") .preamble("You are a calculator assistant.") .tool(Adder) .build(); let response = agent.prompt("Calculate 2 + 3").await?; println!("Agent: {}", response); \`\`\` ### 3.5 Implementing a RAG System Here's an example of setting up a RAG system with Rig: \`\`\`rust use rig::embeddings::EmbeddingsBuilder; use rig::vector_store::{in_memory_store::InMemoryVectorStore, VectorStore}; let embedding_model = openai_client.embedding_model(openai::TEXT_EMBEDDING_ADA_002); let mut vector_store = InMemoryVectorStore::default(); let embeddings = EmbeddingsBuilder::new(embedding_model.clone()) .simple_document("doc1", "Rig is a Rust library for building LLM applications.") .simple_document("doc2", "Rig supports OpenAI, Anthropic, Cohere, and Perplexity as LLM providers.") .build() .await?; vector_store.add_documents(embeddings).await?; let rag_agent = openai_client.agent("gpt-4o") .preamble("You are an assistant that answers questions about Rig.") .dynamic_context(1, vector_store.index(embedding_model)) .build(); let response = rag_agent.prompt("What is Rig?").await?; println!("RAG Agent: {}", response); \`\`\` ## 4. Advanced Features ### 4.1 Customizing Completion Requests Rig allows for fine-tuning completion requests: \`\`\`rust let response = model.completion_request("Translate to French:") .temperature(0.7) .max_tokens(50) .additional_params(json!({"top_p": 0.9})) .send() .await?; \`\`\` ### 4.2 Batched Embeddings For efficient embedding generation: \`\`\`rust let embeddings = EmbeddingsBuilder::new(embedding_model) .simple_documents(vec![ ("doc1", "Content 1"), ("doc2", "Content 2"), // ... ]) .build() .await?; \`\`\` ### 4.3 Using Different LLM Providers Rig supports multiple LLM providers. Here's how to use different providers: \`\`\`rust // OpenAI let openai_client = openai::Client::from_env(); let gpt4_agent = openai_client.agent("gpt-4o").build(); // Anthropic let anthropic_client = anthropic::ClientBuilder::new(&std::env::var("ANTHROPIC_API_KEY")?) .build(); let claude_agent = anthropic_client.agent(anthropic::CLAUDE_3_5_SONNET).build(); // Cohere let cohere_client = cohere::Client::new(&std::env::var("COHERE_API_KEY")?); let command_agent = cohere_client.agent("command").build(); // Perplexity let perplexity_client = perplexity::Client::new(&std::env::var("PERPLEXITY_API_KEY")?); let llama_agent = perplexity_client.agent(perplexity::LLAMA_3_1_70B_INSTRUCT).build(); \`\`\` ### 4.4 Using LanceDB for Vector Storage Here's an example of using LanceDB with Rig: \`\`\`rust use rig_lancedb::{LanceDbVectorStore, SearchParams}; let db = lancedb::connect("data/lancedb-store").execute().await?; let table = db.create_table( "rig_docs", RecordBatchIterator::new(vec![record_batch], Arc::new(rig_lancedb::schema(model.ndims()))), ).execute().await?; let search_params = SearchParams::default(); let vector_store = LanceDbVectorStore::new(table, model, "id", search_params).await?; // Use vector_store in your RAG system... \`\`\` ## 5. Best Practices and Tips 1. **Error Handling**: Use Rig's error types for robust error handling. 2. **Asynchronous Programming**: Leverage Rust's async features with Rig for efficient I/O operations. 3. **Modular Design**: Break down complex AI workflows into reusable tools and agents. 4. **Security**: Always use environment variables or secure vaults for API keys. 5. **Testing**: Write unit tests for custom tools and mock LLM responses for consistent testing. 6. **Model Selection**: Choose appropriate models based on your task complexity and performance requirements. 7. **Prompt Engineering**: Craft clear and specific prompts, utilizing the `preamble` method for setting agent behavior. 8. **Vector Store Usage**: Use vector stores efficiently, generating embeddings once and reusing them when possible. ## 6. Troubleshooting Common Issues 1. **API Rate Limiting**: Implement retries with exponential backoff for API calls. 2. **Memory Usage**: For large document sets, consider using LanceDB or other database-backed vector stores instead of in-memory solutions. 3. **Compatibility**: Ensure you're using compatible versions of Rig and its dependencies. 4. **Embedding Dimensions**: Make sure to use the correct number of dimensions when working with embeddings and vector stores. ## 7. Community and Support - GitHub Repository: https://github.com/0xPlaygrounds/rig - Documentation: https://docs.rs/rig-core/latest/rig/ - Discord Community: [Join here] (replace with actual Discord link when available) ## 8. Future Roadmap - Support for more LLM providers - Enhanced performance optimizations - Advanced AI workflow templates - Ecosystem growth with additional tools and libraries - Improved documentation and examples This comprehensive guide covers the core concepts, usage patterns, and advanced features of Rig. It provides a solid foundation for developing LLM-powered applications using Rig and serves as a reference for both beginners and experienced users of the library.
```

# plugins/discord/src/docs.md

```md
# Introduction Welcome to the Rust Discord Bot documentation. This bot leverages the Rig library to provide AI-powered assistance. # Installation To install the bot, clone the repository and run `cargo run`. # Usage Use the `/hello` command to greet the bot and `/rust` to ask Rust-related questions. # Advanced Features The bot supports Retrieval-Augmented Generation (RAG) to answer questions based on this documentation. # Troubleshooting If you encounter issues, check your environment variables and ensure all dependencies are installed correctly. # rag test test 1: this is the first test, ooopla test 2: this is the second test, delicious
```

# plugins/discord/src/main.rs

```rs
// main.rs mod rig_agent; use anyhow::Result; use dotenvy::dotenv; use rig_agent::RigAgent; use serenity::async_trait; use serenity::model::application::command::Command; use serenity::model::application::command::CommandOptionType; use serenity::model::application::interaction::{Interaction, InteractionResponseType}; use serenity::model::channel::Message; use serenity::model::gateway::Ready; use serenity::prelude::*; use std::env; use std::sync::Arc; use tracing::{debug, error, info}; // Define a key for storing the bot's user ID in the TypeMap struct BotUserId; impl TypeMapKey for BotUserId { type Value = serenity::model::id::UserId; } struct Handler { rig_agent: Arc<RigAgent>, } #[async_trait] impl EventHandler for Handler { async fn interaction_create(&self, ctx: Context, interaction: Interaction) { debug!("Received an interaction"); if let Interaction::ApplicationCommand(command) = interaction { debug!("Received command: {}", command.data.name); let content = match command.data.name.as_str() { "hello" => "Hello! I'm your helpful Rust and Rig-powered assistant. How can I assist you today?".to_string(), "ask" => { let query = command .data .options .get(0) .and_then(|opt| opt.value.as_ref()) .and_then(|v| v.as_str()) .unwrap_or("What would you like to ask?"); debug!("Query: {}", query); match self.rig_agent.process_message(query).await { Ok(response) => response, Err(e) => { error!("Error processing request: {:?}", e); format!("Error processing request: {:?}", e) } } } _ => "Not implemented :(".to_string(), }; debug!("Sending response: {}", content); if let Err(why) = command .create_interaction_response(&ctx.http, |response| { response .kind(InteractionResponseType::ChannelMessageWithSource) .interaction_response_data(|message| message.content(content)) }) .await { error!("Cannot respond to slash command: {}", why); } else { debug!("Response sent successfully"); } } } async fn message(&self, ctx: Context, msg: Message) { if msg.mentions_me(&ctx.http).await.unwrap_or(false) { debug!("Bot mentioned in message: {}", msg.content); let bot_id = { let data = ctx.data.read().await; data.get::<BotUserId>().copied() }; if let Some(bot_id) = bot_id { let mention = format!("<@{}>", bot_id); let content = msg.content.replace(&mention, "").trim().to_string(); debug!("Processed content after removing mention: {}", content); match self.rig_agent.process_message(&content).await { Ok(response) => { if let Err(why) = msg.channel_id.say(&ctx.http, response).await { error!("Error sending message: {:?}", why); } } Err(e) => { error!("Error processing message: {:?}", e); if let Err(why) = msg .channel_id .say(&ctx.http, format!("Error processing message: {:?}", e)) .await { error!("Error sending error message: {:?}", why); } } } } else { error!("Bot user ID not found in TypeMap"); } } } async fn ready(&self, ctx: Context, ready: Ready) { info!("{} is connected!", ready.user.name); { let mut data = ctx.data.write().await; data.insert::<BotUserId>(ready.user.id); } let commands = Command::set_global_application_commands(&ctx.http, |commands| { commands .create_application_command(|command| { command.name("hello").description("Say hello to the bot") }) .create_application_command(|command| { command .name("ask") .description("Ask the bot a question") .create_option(|option| { option .name("query") .description("Your question for the bot") .kind(CommandOptionType::String) .required(true) }) }) }) .await; println!("Created the following global commands: {:#?}", commands); } } #[tokio::main] async fn main() -> Result<()> { dotenvy().ok(); tracing_subscriber::fmt() .with_max_level(tracing::Level::DEBUG) .init(); let token = env::var("DISCORD_TOKEN").expect("Expected DISCORD_TOKEN in environment"); let rig_agent = Arc::new(RigAgent::new().await?); let intents = GatewayIntents::GUILD_MESSAGES | GatewayIntents::DIRECT_MESSAGES | GatewayIntents::MESSAGE_CONTENT; let mut client = Client::builder(&token, intents) .event_handler(Handler { rig_agent: Arc::clone(&rig_agent), }) .await .expect("Err creating client"); if let Err(why) = client.start().await { error!("Client error: {:?}", why); } Ok(()) }
```

# plugins/discord/src/rig_agent.rs

```rs
// rig_agent.rs use anyhow::{Context, Result}; use rig::agent::Agent; use rig::completion::Document; use rig::completion::Prompt; use rig::embeddings::EmbeddingsBuilder; use rig::providers::openai; use rig::vector_store::in_memory_store::InMemoryVectorStore; use std::fs; use std::path::Path; use std::sync::Arc; pub struct RigAgent { agent: Arc<Agent<openai::CompletionModel>>, } impl RigAgent { pub async fn new() -> Result<Self> { // Initialize OpenAI client let openai_client = openai::Client::from_env(); let embedding_model = openai_client.embedding_model(openai::TEXT_EMBEDDING_3_SMALL); // Create vector store let mut vector_store = InMemoryVectorStore::default(); // Get the current directory and construct paths to markdown files let current_dir = std::env::current_dir()?; let documents_dir = current_dir.join("documents"); let md1_path = documents_dir.join("Rig_guide.md"); let md2_path = documents_dir.join("Rig_faq.md"); let md3_path = documents_dir.join("Rig_examples.md"); // Load markdown documents let md1_content = Self::load_md_content(&md1_path)?; let md2_content = Self::load_md_content(&md2_path)?; let md3_content = Self::load_md_content(&md3_path)?; // Create embeddings and add to vector store let embeddings = EmbeddingsBuilder::new(embedding_model.clone()) .add_document(Document::new("Rig_guide", md1_content)) .add_document(Document::new("Rig_faq", md2_content)) .add_document(Document::new("Rig_examples", md3_content)) .build() .await?; vector_store.add_embeddings(embeddings).await?; // Create index let index = vector_store.index(embedding_model); // Create Agent let agent = Arc::new(openai_client.agent(openai::GPT_4o) .preamble("You are an advanced AI assistant powered by Rig, a Rust library for building LLM applications. Your primary function is to provide accurate, helpful, and context-aware responses by leveraging both your general knowledge and specific information retrieved from a curated knowledge base. Key responsibilities and behaviors: 1. Information Retrieval: You have access to a vast knowledge base. When answering questions, always consider the context provided by the retrieved information. 2. Clarity and Conciseness: Provide clear and concise answers. Ensure responses are short and concise. Use bullet points or numbered lists for complex information when appropriate. 3. Technical Proficiency: You have deep knowledge about Rig and its capabilities. When discussing Rig or answering related questions, provide detailed and technically accurate information. 4. Code Examples: When appropriate, provide Rust code examples to illustrate concepts, especially when discussing Rig's functionalities. Always format code examples for proper rendering in Discord by wrapping them in triple backticks and specifying the language as 'rust'. For example: \`\`\`rust let example_code = \"This is how you format Rust code for Discord\"; println!(\"{}\", example_code); \`\`\` 5. Keep your responses short and concise. If the user needs more information, they can ask follow-up questions. ") .dynamic_context(2, index) .build()); Ok(Self { agent }) } fn load_md_content<P: AsRef<Path>>(file_path: P) -> Result<String> { fs::read_to_string(file_path.as_ref()) .with_context(|| format!("Failed to read markdown file: {:?}", file_path.as_ref())) } pub async fn process_message(&self, message: &str) -> Result<String> { self.agent .prompt(message) .await .map_err(anyhow::Error::from) } }
```

# plugins/gibwork/Cargo.toml

```toml
[package] name = "solagent-plugin-gibwork" version = "0.1.1" edition = "2021" authors = ["zTgx <beautifularea@gmail.com>"] repository = "https://github.com/zTgx/solagent.rs" keywords = ["solagent", "plugin", "gibwork"] license = "Apache-2.0" description = "solagent plugin gibwork" [dependencies] solagent-core = "0.1.3" serde = { version = "1.0", features = ["derive"] } base64 = "0.22.1" reqwest = { version = "0.12", features = ["json"] } bincode = "1.3.3"
```

# plugins/gibwork/src/create_gibwork_task.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use base64::{engine::general_purpose, Engine}; use serde::{Deserialize, Serialize}; use solagent_core::{ solana_sdk::{commitment_config::CommitmentConfig, pubkey::Pubkey, transaction::VersionedTransaction}, SolanaAgentKit, }; #[derive(Serialize)] struct TaskRequest { title: String, content: String, requirements: String, tags: Vec<String>, payer: String, token: TokenInfo, } #[derive(Serialize)] struct TokenInfo { #[serde(rename = "mintAddress")] mint_address: String, amount: u64, } #[derive(Deserialize)] struct TaskResponse { #[serde(rename = "taskId")] task_id: String, #[serde(rename = "serializedTransaction")] serialized_transaction: String, } #[derive(Debug, Serialize, Deserialize)] pub struct GibworkCreateTaskResponse { pub status: String, pub task_id: String, pub signature: String, } /// Create a new task on Gibwork /// /// # Arguments /// /// * `agent` - SolanaAgentKit instance /// * `title` - Title of the task /// * `content` - Description of the task /// * `requirements` - Requirements to complete the task /// * `tags` - List of tags associated with the task /// * `token_mint_address` - Token mint address for payment /// * `token_amount` - Payment amount for the task /// * `payer` - Optional payer address (defaults to agent's wallet address) /// /// # Returns /// /// Object containing task creation transaction and generated taskId #[allow(clippy::too_many_arguments)] pub async fn create_gibwork_task( agent: &SolanaAgentKit, title: &str, content: &str, requirements: &str, tags: Vec<String>, token_mint_address: &str, token_amount: u64, payer: Option<Pubkey>, ) -> Result<GibworkCreateTaskResponse, Box<dyn std::error::Error>> { let request = TaskRequest { title: title.to_string(), content: content.to_string(), requirements: requirements.to_string(), tags, payer: payer.unwrap_or(agent.wallet.address).to_string(), token: TokenInfo { mint_address: token_mint_address.to_string(), amount: token_amount }, }; // Send request to Gibwork API let client = reqwest::Client::new(); let response = client.post("https://api2.gib.work/tasks/public/transaction").json(&request).send().await?; if !response.status().is_success() { return Err(format!("API request failed: {}", response.status()).into()); } let task_response: TaskResponse = response.json().await?; // Deserialize and sign transaction let transaction_data = general_purpose::STANDARD.decode(task_response.serialized_transaction.as_str())?; let mut versioned_transaction: VersionedTransaction = bincode::deserialize(&transaction_data)?; // Get latest blockhash and sign transaction let blockhash = agent.connection.get_latest_blockhash()?; versioned_transaction.message.set_recent_blockhash(blockhash); let signed_transaction = VersionedTransaction::try_new(versioned_transaction.message, &[&agent.wallet.wallet])?; // Send and confirm transaction let signature = agent.connection.send_transaction(&signed_transaction)?; agent.connection.confirm_transaction_with_spinner(&signature, &blockhash, CommitmentConfig::confirmed())?; Ok(GibworkCreateTaskResponse { status: "success".to_string(), task_id: task_response.task_id, signature: signature.to_string(), }) }
```

# plugins/gibwork/src/lib.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. mod create_gibwork_task; pub use create_gibwork_task::{create_gibwork_task, GibworkCreateTaskResponse};
```

# plugins/goplus/Cargo.toml

```toml
[package] name = "solagent-plugin-goplus" version = "0.1.0" edition = "2021" authors = ["zTgx <beautifularea@gmail.com>"] repository = "https://github.com/zTgx/solagent.rs" keywords = ["solagent", "plugin", "goplus"] license = "Apache-2.0" description = "solagent.rs plugin goplus" [dependencies] serde_json = "1.0" reqwest = { version = "0.12", features = ["json"] }
```

# plugins/goplus/src/lib.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. pub async fn get_token_security_info( chain_id: &str, contract_address: &str, ) -> Result<serde_json::Value, reqwest::Error> { let url = format!("https://api.gopluslabs.io/api/v1/token_security/{}?contract_addresses={}", chain_id, contract_address); let client = reqwest::Client::new(); let response = client.get(url).header("Content-Type", "application/json").send().await?; let data = response.json::<serde_json::Value>().await?; Ok(data) } pub async fn get_solana_token_security_info(contract_address: &str) -> Result<serde_json::Value, reqwest::Error> { let url = format!("https://api.gopluslabs.io/api/v1/solana/token_security?contract_addresses={}", contract_address); let client = reqwest::Client::new(); let response = client.get(url).header("Content-Type", "application/json").send().await?; let data = response.json::<serde_json::Value>().await?; Ok(data) } pub async fn get_token_malicious_info(chain_id: &str, address: &str) -> Result<serde_json::Value, reqwest::Error> { let url = format!("https://api.gopluslabs.io/api/v1/address_security/{}?chain_id={}", address, chain_id); let client = reqwest::Client::new(); let response = client.get(url).header("Content-Type", "application/json").send().await?; let data = response.json::<serde_json::Value>().await?; Ok(data) } pub async fn get_token_phishing_site_info(url: &str) -> Result<serde_json::Value, reqwest::Error> { let url = format!("https://api.gopluslabs.io/api/v1/aphishing_site?url={}", url); let client = reqwest::Client::new(); let response = client.get(url).header("Content-Type", "application/json").send().await?; let data = response.json::<serde_json::Value>().await?; Ok(data) }
```

# plugins/helius/Cargo.toml

```toml
[package] name = "solagent-plugin-helius" version = "0.1.1" edition = "2021" authors = ["zTgx <beautifularea@gmail.com>"] repository = "https://github.com/zTgx/solagent.rs" keywords = ["solagent", "plugin", "helius"] license = "Apache-2.0" description = "solagent.rs plugin helius" [dependencies] solagent-core = "0.1.3" serde_json = "1.0" serde = { version = "1.0", features = ["derive"] } reqwest = { version = "0.12", features = ["json"] } helius = "0.2.4"
```

# plugins/helius/src/create_webhook.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use serde::{Deserialize, Serialize}; use solagent_core::SolanaAgentKit; #[derive(Deserialize, Serialize)] pub struct HeliusWebhookResponse { pub webhook_url: String, pub webhook_id: String, } pub async fn create_webhook( agent: &SolanaAgentKit, account_addresses: Vec<String>, webhook_url: String, ) -> Result<HeliusWebhookResponse, Box<dyn std::error::Error>> { // Get the Helius API key from the agent's configuration let api_key = match agent.config.helius_api_key.as_ref() { Some(key) => key, None => return Err("Missing Helius API key in agent.config.HELIUS_API_KEY".into()), }; let url = format!("https://api.helius.xyz/v0/webhooks?api-key={}", api_key); let body = serde_json::json!({ "webhookURL": webhook_url, "transactionTypes": ["Any"], "accountAddresses": account_addresses, "webhookType": "enhanced", "txnStatus": "all", }); let client = reqwest::Client::new(); let response = client.post(url).header("Content-Type", "application/json").json(&body).send().await?; let data = response.json::<serde_json::Value>().await?; let webhook_url = data.get("webhookURL").expect("webhookURL field").as_str().expect("webhookURL text"); let webhook_id = data.get("webhookID").expect("webhookID field").as_str().expect("webhookID text"); Ok(HeliusWebhookResponse { webhook_url: webhook_url.to_string(), webhook_id: webhook_id.to_string() }) }
```

# plugins/helius/src/delete_webhook.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use solagent_core::SolanaAgentKit; /// Deletes a Helius Webhook by its ID. /// /// # Arguments /// * `agent` - An instance of SolanaAgentKit (with .config.HELIUS_API_KEY) /// * `webhook_id` - The unique ID of the webhook to delete /// /// # Returns /// The response body from the Helius API (which may contain status or other info) pub async fn delete_webhook( agent: &SolanaAgentKit, webhook_id: &str, ) -> Result<serde_json::Value, Box<dyn std::error::Error>> { // Get the Helius API key from the agent's configuration let api_key = match agent.config.helius_api_key.as_ref() { Some(key) => key, None => return Err("Missing Helius API key in agent.config.HELIUS_API_KEY".into()), }; // Construct the URL for the DELETE request let url = format!("https://api.helius.xyz/v0/webhooks/{}?api-key={}", webhook_id, api_key); // Create an HTTP client let client = reqwest::Client::new(); // Send the DELETE request let response = client.delete(&url).header("Content-Type", "application/json").send().await?; // Check if the request was successful if !response.status().is_success() { return Err(format!( "Failed to delete webhook: {} {}", response.status(), response.status().canonical_reason().unwrap_or("Unknown") ) .into()); } // Handle different response status codes if response.status().as_u16() == 204 { return Ok(serde_json::json!({"message": "Webhook deleted successfully (no content returned)"})); } // Check if the response body is empty let content_length = response.headers().get("Content-Length"); if content_length.is_none() || content_length.expect("HeaderValue").to_str()? == "0" { return Ok(serde_json::json!({"message": "Webhook deleted successfully (empty body)"})); } // Parse the response body as JSON let data: serde_json::Value = response.json().await?; Ok(data) }
```

# plugins/helius/src/get_assets_by_owner.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use serde_json::json; use solagent_core::SolanaAgentKit; pub async fn get_assets_by_owner( agent: &SolanaAgentKit, owner_public_key: &str, limit: u32, ) -> Result<serde_json::Value, Box<dyn std::error::Error>> { // Get the Helius API key from the agent's configuration let api_key = match agent.config.helius_api_key.as_ref() { Some(key) => key, None => return Err("Missing Helius API key in agent.config.HELIUS_API_KEY".into()), }; let url = format!("https://mainnet.helius-rpc.com/?api-key={}", api_key); let client = reqwest::Client::new(); let request_body = json!({ "jsonrpc": "2.0", "id": "get-assets", "method": "getAssetsByOwner", "params": json!({ "ownerAddress": owner_public_key, "page": 3, "limit": limit, "displayOptions": { "showFungible": true }, }), }); let response = client.post(&url).header("Content-Type", "application/json").json(&request_body).send().await?; if !response.status().is_success() { return Err(format!( "Failed to fetch: {} - {}", response.status(), response.status().canonical_reason().unwrap_or("Unknown") ) .into()); } let data: serde_json::Value = response.json().await?; Ok(data) }
```

# plugins/helius/src/get_webhook.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use serde::{Deserialize, Serialize}; use solagent_core::SolanaAgentKit; #[derive(Debug, Serialize, Deserialize)] pub struct HeliusWebhookIdResponse { pub wallet: String, pub webhook_url: String, pub transaction_types: Vec<String>, pub account_addresses: Vec<String>, pub webhook_type: String, } /// Retrieves a Helius Webhook by ID, returning only the specified fields. /// /// # Arguments /// * `agent` - An instance of SolanaAgentKit (with .config.HELIUS_API_KEY) /// * `webhook_id` - The unique ID of the webhook to delete /// /// # Returns /// A HeliusWebhook object containing { wallet, webhookURL, transactionTypes, accountAddresses, webhookType } pub async fn get_webhook( agent: &SolanaAgentKit, webhook_id: &str, ) -> Result<HeliusWebhookIdResponse, Box<dyn std::error::Error>> { // Get the Helius API key from the agent's configuration let api_key = match agent.config.helius_api_key.as_ref() { Some(key) => key, None => return Err("Missing Helius API key in agent.config.HELIUS_API_KEY".into()), }; let client = reqwest::Client::new(); let url = format!("https://api.helius.xyz/v0/webhooks/{}?api-key={}", webhook_id, api_key); let response = client.get(url).header("Content-Type", "application/json").send().await?; let data = response.json::<HeliusWebhookIdResponse>().await?; Ok(data) }
```

# plugins/helius/src/lib.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. mod create_webhook; pub use create_webhook::{create_webhook, HeliusWebhookResponse}; mod delete_webhook; pub use delete_webhook::delete_webhook; mod get_webhook; pub use get_webhook::{get_webhook, HeliusWebhookIdResponse}; mod transaction_parsing; pub use transaction_parsing::transaction_parse; mod get_assets_by_owner; pub use get_assets_by_owner::get_assets_by_owner;
```

# plugins/helius/src/transaction_parsing.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use serde::{Deserialize, Serialize}; use serde_json::json; use solagent_core::SolanaAgentKit; #[derive(Debug, Serialize, Deserialize)] pub struct HeliusWebhookIdResponse { pub wallet: String, pub webhook_url: String, pub transaction_types: Vec<String>, pub account_addresses: Vec<String>, pub webhook_type: String, } /// Parse a Solana transaction using the Helius Enhanced Transactions API /// /// # Arguments /// * `agent` - An instance of SolanaAgentKit (with .config.HELIUS_API_KEY) /// * `transaction_id` - The transaction ID to parse /// /// # Returns /// Parsed transaction data pub async fn transaction_parse( agent: &SolanaAgentKit, transaction_id: &str, ) -> Result<serde_json::Value, Box<dyn std::error::Error>> { // Get the Helius API key from the agent's configuration let api_key = match agent.config.helius_api_key.as_ref() { Some(key) => key, None => return Err("Missing Helius API key in agent.config.HELIUS_API_KEY".into()), }; let client = reqwest::Client::new(); let url = format!("https://api.helius.xyz/v0/transactions/?api-key={}", api_key); let body = json!( { "transactions": vec![transaction_id.to_string()], }); let response = client.post(url).header("Content-Type", "application/json").json(&body).send().await?; let data = response.json().await?; Ok(data) }
```

# plugins/jupiter/Cargo.toml

```toml
[package] name = "solagent-plugin-jupiter" version = "0.1.1" edition = "2021" authors = ["zTgx <beautifularea@gmail.com>"] repository = "https://github.com/zTgx/solagent.rs" keywords = ["solagent", "plugin", "jupiter"] license = "Apache-2.0" description = "solagent.rs plugin jupiter" [dependencies] solagent-core = "0.1.3" serde_json = "1.0" reqwest = { version = "0.12", features = ["json"] } anyhow = "1.0.80" base64 = "0.22.1" serde = { version = "1.0", features = ["derive"] } spl-token = "7.0.0" bincode = "1.3.3"
```

# plugins/jupiter/src/fetch_price.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. #![allow(dead_code)] use crate::JUP_PRICE_V2; use serde::Deserialize; #[derive(Deserialize, Debug)] struct PriceResponse { data: std::collections::HashMap<String, TokenData>, } #[derive(Deserialize, Debug)] struct TokenData { id: Option<String>, #[serde(rename = "type")] typed: Option<String>, price: Option<String>, } /// Fetches the price of a given token quoted in USDC using Jupiter API. /// /// # Parameters /// /// - `token_id`: The token mint address as a string. /// /// # Returns /// /// The price of the token quoted in USDC as a string. pub async fn fetch_price(token_id: &str) -> Result<String, Box<dyn std::error::Error>> { let url = format!("{}{}", JUP_PRICE_V2, token_id); let response = reqwest::get(&url).await?; if !response.status().is_success() { return Err(format!("Failed to fetch price: {}", response.status()).into()); } let data: PriceResponse = response.json().await?; // Get the price for the given token_id if let Some(token_data) = data.data.get(token_id) { if let Some(price) = &token_data.price { return Ok(price.clone()); } } Err("Price data not available for the given token.".into()) }
```

# plugins/jupiter/src/get_token_data_by_address.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. /// Fetches the price of a given token quoted in USDC using Jupiter API. /// /// # Parameters /// /// - `mint`: The token mint address as a string. /// /// # Returns /// /// The token data. pub async fn get_token_data_by_address(mint: &str) -> Result<serde_json::Value, Box<dyn std::error::Error>> { let url = format!("https://tokens.jup.ag/token/{}", mint); let response = reqwest::get(&url).await?; if !response.status().is_success() { return Err(format!("Failed to get token data: {}", response.status()).into()); } let data: serde_json::Value = response.json().await?; Ok(data) }
```

# plugins/jupiter/src/lib.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. mod get_token_data_by_address; pub use get_token_data_by_address::get_token_data_by_address; mod trade; pub use trade::trade; mod fetch_price; pub use fetch_price::fetch_price; mod stake_with_jup; pub use stake_with_jup::stake_with_jup; /// Jupiter API URL pub const JUP_API: &str = "https://quote-api.jup.ag/v6"; pub const JUP_REFERRAL_ADDRESS: &str = "REFER4ZgmyYx9c6He5XfaTMiGfdLwRnkV4RPp9t9iF3"; pub const JUP_PRICE_V2: &str = "https://api.jup.ag/price/v2?ids=";
```

# plugins/jupiter/src/stake_with_jup.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use anyhow::Result; use base64::{engine::general_purpose, Engine as _}; use solagent_core::{ solana_sdk::{commitment_config::CommitmentConfig, transaction::VersionedTransaction}, SolanaAgentKit, }; /// Stake SOL with Jupiter validator /// /// # Arguments /// /// * `agent` - SolanaAgentKit instance /// * `amount` - Amount of SOL to stake (in SOL) /// /// # Returns /// /// Transaction signature as a string pub async fn stake_with_jup(agent: &SolanaAgentKit, amount: f64) -> Result<String, Box<dyn std::error::Error>> { // Convert SOL amount to lamports let amount_lamports = (amount * 1e9) as u64; // Build stake URL let stake_url = format!( "https://worker.jup.ag/blinks/swap/So11111111111111111111111111111111111111112/jupSoLaHXQiZZTSfEWMTRRgpnyFm8f6sZdosWBjx93v/{}", amount_lamports ); // Get stake transaction let client = reqwest::Client::new(); let stake_request = serde_json::json!({ "account": agent.wallet.address.to_string(), }); let response = client.post(&stake_url).json(&stake_request).send().await?; let data: serde_json::Value = response.json().await?; let transaction_data = general_purpose::STANDARD.decode(data["transaction"].as_str().expect("decode transaction"))?; let mut versioned_transaction: VersionedTransaction = bincode::deserialize(&transaction_data)?; let blockhash = agent.connection.get_latest_blockhash()?; versioned_transaction.message.set_recent_blockhash(blockhash); // Sign and send transaction let signed_transaction = VersionedTransaction::try_new(versioned_transaction.message, &[&agent.wallet.wallet])?; let signature = agent.connection.send_transaction(&signed_transaction)?; // Confirm transaction let latest_blockhash = agent.connection.get_latest_blockhash()?; agent.connection.confirm_transaction_with_spinner(&signature, &latest_blockhash, CommitmentConfig::confirmed())?; Ok(signature.to_string()) }
```

# plugins/jupiter/src/trade.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use crate::JUP_API; use anyhow::Result; use base64::{engine::general_purpose, Engine as _}; use serde::{Deserialize, Serialize}; use solagent_core::{ solana_sdk::{ commitment_config::CommitmentConfig, program_pack::Pack, pubkey::Pubkey, transaction::VersionedTransaction, }, SolanaAgentKit, }; use spl_token::state::Mint; use std::str::FromStr; #[derive(Serialize)] struct SwapRequest { #[serde(rename = "quoteResponse")] quote_response: QuoteResponse, #[serde(rename = "userPublicKey")] user_public_key: String, #[serde(rename = "wrapAndUnwrapSol")] wrap_and_unwrap_sol: bool, #[serde(rename = "dynamicComputeUnitLimit")] dynamic_compute_unit_limit: bool, #[serde(rename = "prioritizationFeeLamports")] prioritization_fee_lamports: String, #[serde(rename = "feeAccount")] fee_account: Option<String>, } #[derive(Deserialize)] struct SwapResponse { #[serde(rename = "swapTransaction")] swap_transaction: String, } #[derive(Deserialize, Serialize)] struct QuoteResponse { #[serde(flatten)] extra: serde_json::Value, } /// Swap tokens using Jupiter Exchange /// /// # Arguments /// /// * `agent` - SolanaAgentKit instance /// * `output_mint` - Target token mint address /// * `input_amount` - Amount to swap (in token decimals) /// * `input_mint` - Source token mint address (defaults to SOL) /// * `slippage_bps` - Slippage tolerance in basis points (default: 300 = 3%) /// /// # Returns /// /// Transaction signature as a string pub async fn trade( agent: &SolanaAgentKit, output_mint: &str, input_amount: f64, input_mint: Option<String>, slippage_bps: Option<u32>, ) -> Result<String, Box<dyn std::error::Error>> { // Convert strings to Pubkeys let output_mint = Pubkey::from_str(output_mint)?; let input_mint = input_mint.as_deref().map(Pubkey::from_str).transpose()?.unwrap_or(spl_token::native_mint::id()); // Use defaults if not provided let slippage_bps = slippage_bps.unwrap_or(300); // Check if input token is native SOL let is_native_sol = input_mint == spl_token::native_mint::id(); // Get input token decimals let input_decimals = if is_native_sol { 9 } else { let account = agent.connection.get_account(&input_mint)?; let mint = Mint::unpack(&account.data)?; mint.decimals }; // Calculate scaled amount let scaled_amount = (input_amount * 10f64.powf(input_decimals as f64)) as u64; // Build quote URL let quote_url = format!( "{}/quote?inputMint={}&outputMint={}&amount={}&slippageBps={}&onlyDirectRoutes=true&maxAccounts=20", JUP_API, input_mint, output_mint, scaled_amount, slippage_bps ); // Get quote let client = reqwest::Client::new(); let quote_response: QuoteResponse = client.get(&quote_url).send().await?.json().await?; // Get swap transaction let swap_request = SwapRequest { quote_response, user_public_key: agent.wallet.address.to_string(), wrap_and_unwrap_sol: true, dynamic_compute_unit_limit: true, prioritization_fee_lamports: "auto".to_string(), fee_account: None, }; let swap_response: SwapResponse = client.post(format!("{}/swap", JUP_API)).json(&swap_request).send().await?.json().await?; let swap_transaction = general_purpose::STANDARD.decode(&swap_response.swap_transaction).expect("decode swap_transaction"); let versioned_transaction: VersionedTransaction = bincode::deserialize(&swap_transaction)?; let signed_transaction = VersionedTransaction::try_new(versioned_transaction.message, &[&agent.wallet.wallet])?; let signature = agent.connection.send_transaction(&signed_transaction)?; let latest_blockhash = agent.connection.get_latest_blockhash()?; agent.connection.confirm_transaction_with_spinner(&signature, &latest_blockhash, CommitmentConfig::confirmed())?; Ok(signature.to_string()) }
```

# plugins/pumpfun/Cargo.toml

```toml
[package] name = "solagent-plugin-pumpfun" version = "0.1.1" edition = "2021" authors = ["zTgx <beautifularea@gmail.com>"] repository = "https://github.com/zTgx/solagent.rs" keywords = ["solagent", "plugin", "pumpfun"] license = "Apache-2.0" description = "solagent.rs plugin pumpfun" [dependencies] solagent-core = "0.1.3" serde = { version = "1.0", features = ["derive"] } serde_json = "1.0" base64 = "0.22.1" reqwest = { version = "0.12", features = ["json", "multipart"] } bincode = "1.3.3"
```

# plugins/pumpfun/src/launch_token_pumpfun.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use reqwest::{multipart::Part, Client as ReqwestClient}; use serde::{Deserialize, Serialize}; use solagent_core::{ solana_client, solana_sdk::{ commitment_config::CommitmentConfig, signature::Signer, signer::keypair::Keypair, transaction::VersionedTransaction, }, SolanaAgentKit, }; #[derive(Serialize, Deserialize, Debug)] pub struct PumpFunTokenOptions { pub twitter: Option<String>, pub telegram: Option<String>, pub website: Option<String>, pub initial_liquidity_sol: f64, pub slippage_bps: u16, pub priority_fee: f64, } #[derive(Serialize, Deserialize, Debug)] pub struct PumpfunTokenResponse { pub signature: String, pub mint: String, pub metadata_uri: String, } pub struct TokenMetadata { pub name: String, pub symbol: String, pub uri: String, } /// Launch a token on Pump.fun. /// /// # Arguments /// /// - `agent` - An instance of `SolanaAgentKit`. /// - `tokenName` - The name of the token. /// - `tokenTicker` - The ticker of the token. /// - `description` - The description of the token. /// - `imageUrl` - The URL of the token image. /// - `options` - Optional token options which include `twitter`, `telegram`, `website`, `initialLiquiditySOL`, `slippageBps`, and `priorityFee`. /// /// # Returns /// /// If successful, it returns the signature of the transaction, the mint address, and the metadata URI. Otherwise, it returns an error. /// /// To get a transaction for signing and sending with a custom RPC, send a POST request to: /// https://pumpportal.fun/local-trading-api/trading-api/ /// /// The request body must include the following options: /// /// * `publicKey` - Your wallet's public key. /// * `action` - Either "buy" or "sell", indicating the trading action you want to perform. /// * `mint` - The contract address of the token you wish to trade. This is the text that appears after the '/' in the pump.fun url for the specific token. /// * `amount` - The quantity of SOL or tokens to be traded. When selling, the amount can be specified as a percentage of the tokens in your wallet (e.g., amount: "100%"). /// * `denominatedInSol` - Set to "true" if the `amount` is specified in SOL, and "false" if it's specified in tokens. /// * `slippage` - The percentage of slippage that is allowed during the trading process. /// * `priorityFee` - The amount to be used as the priority fee. /// * `pool` - (Optional) Currently, 'pump' and 'raydium' are the supported options. The default value is 'pump'. /// /// /// https://pumpportal.fun/creation /// pub async fn launch_token_pumpfun( agent: &SolanaAgentKit, token_name: &str, token_symbol: &str, description: &str, image_url: &str, options: Option<PumpFunTokenOptions>, ) -> Result<PumpfunTokenResponse, Box<dyn std::error::Error>> { let reqwest_client = ReqwestClient::new(); // 0. download image let image_data = fetch_image(&reqwest_client, image_url).await.expect("fetch_image"); // 1. fetch token metadata metadataUri let token_metadata = fetch_token_metadata(&reqwest_client, token_name, token_symbol, description, options, &image_data) .await .expect("fetch_token_metadata"); // 2. Create a new keypair for the mint let mint_keypair = Keypair::new(); // 3. request pumpportal tx let mut versioned_tx = request_pumpportal_tx(agent, &reqwest_client, &token_metadata, &mint_keypair) .await .expect("request_pumpportal_tx"); // 4. sign&send transaction let signature = sign_and_send_tx(agent, &mut versioned_tx, &mint_keypair).await.expect("sign_and_send_tx"); let res = PumpfunTokenResponse { signature, mint: mint_keypair.pubkey().to_string(), metadata_uri: token_metadata.uri }; Ok(res) } // try signed vtx: NotEnoughSigners -> mint_keypair is needed async fn sign_and_send_tx( agent: &SolanaAgentKit, vtx: &mut VersionedTransaction, mint_keypair: &Keypair, ) -> Result<String, Box<std::io::Error>> { let recent_blockhash = agent.connection.get_latest_blockhash().expect("get_latest_blockhash"); vtx.message.set_recent_blockhash(recent_blockhash); let signed_vtx = VersionedTransaction::try_new(vtx.message.clone(), &[mint_keypair, &agent.wallet.wallet]) .expect("try signed vtx"); let signature = agent .connection .send_and_confirm_transaction_with_spinner_and_config( &signed_vtx, CommitmentConfig::finalized(), solana_client::rpc_config::RpcSendTransactionConfig { skip_preflight: false, ..Default::default() }, ) .expect("send_and_confirm_tx"); Ok(signature.to_string()) } async fn fetch_image(client: &ReqwestClient, image_url: &str) -> Result<Vec<u8>, Box<dyn std::error::Error>> { let response = client.get(image_url).send().await?; if response.status().is_success() { let image_data = response.bytes().await.expect("image data"); return Ok(image_data.to_vec()); } Err("fetch image error".into()) } async fn fetch_token_metadata( client: &ReqwestClient, name: &str, symbol: &str, description: &str, options: Option<PumpFunTokenOptions>, image_data: &[u8], ) -> Result<TokenMetadata, Box<dyn std::error::Error>> { let part = Part::bytes(image_data.to_vec()).file_name("image_name").mime_str("image/png")?; // Important: set the correct MIME type let mut form = reqwest::multipart::Form::new() .text("name", name.to_owned()) .text("symbol", symbol.to_owned()) .text("description", description.to_owned()) .part("file", part); if let Some(option) = options { if let Some(x) = option.twitter { form = form.text("twitter", x); } if let Some(tele) = option.telegram { form = form.text("telegram", tele); } if let Some(website) = option.website { form = form.text("website", website); } form = form.text("showName", "true"); } let res = client.post("https://pump.fun/api/ipfs").multipart(form).send().await?; let status = res.status(); if !status.is_success() { let text = res.text().await?; eprintln!("Error response: {}", text); return Err(format!("Upload failed with status: {}", status).into()); } let response_json = res.json::<serde_json::Value>().await?; let md = TokenMetadata { name: name.to_string(), symbol: symbol.to_string(), uri: response_json.get("metadataUri").expect("metadataUri").to_string(), }; Ok(md) } async fn request_pumpportal_tx( agent: &SolanaAgentKit, client: &ReqwestClient, token_matedata: &TokenMetadata, mint_keypair: &Keypair, ) -> Result<VersionedTransaction, Box<dyn std::error::Error>> { let request_body = serde_json::json!({ "publicKey": agent.wallet.address.to_string(), "action": "create", "tokenMetadata": { "name": token_matedata.name, "symbol": token_matedata.symbol, "uri": token_matedata.uri }, "mint": mint_keypair.pubkey().to_string(), "denominatedInSol": "true", "amount": 1, "slippage": 10, "priorityFee": 0.0005, "pool": "pump" }); let res = client .post("https://pumpportal.fun/api/trade-local") .header("Content-Type", "application/json") .json(&request_body) .send() .await?; let status = res.status(); if !status.is_success() { let text = res.text().await?; eprintln!("Error response: {}", text); return Err(format!("trade-local failed with status: {}", status).into()); } if let Ok(bytes) = res.bytes().await { if let Ok(tx) = bincode::deserialize::<VersionedTransaction>(&bytes) { return Ok(tx); } } Err("fetch token metadata error".into()) }
```

# plugins/pumpfun/src/lib.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. mod launch_token_pumpfun; pub use launch_token_pumpfun::{launch_token_pumpfun, PumpFunTokenOptions, PumpfunTokenResponse};
```

# plugins/pyth/Cargo.toml

```toml
[package] name = "solagent-plugin-pyth" version = "0.1.0" edition = "2021" authors = ["zTgx <beautifularea@gmail.com>"] repository = "https://github.com/zTgx/solagent.rs" keywords = ["solagent", "plugin", "pyth"] license = "Apache-2.0" description = "solagent.rs plugin pyth" [dependencies] serde = { version = "1.0", features = ["derive"] } reqwest = { version = "0.12", features = ["json"] }
```

# plugins/pyth/src/lib.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. mod pyth_fetch_price; pub use pyth_fetch_price::{fetch_price_by_pyth, fetch_pyth_price_feed_id}; pub const PYTH_API: &str = "https://hermes.pyth.network/v2";
```

# plugins/pyth/src/pyth_fetch_price.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. #![allow(dead_code)] use crate::PYTH_API; use serde::{Deserialize, Serialize}; #[derive(Debug, Serialize, Deserialize)] struct Price { price: String, conf: String, expo: i32, publish_time: i64, } #[derive(Debug, Serialize, Deserialize)] struct ParsedData { id: String, price: Price, ema_price: Price, metadata: Metadata, } #[derive(Debug, Serialize, Deserialize)] struct BinaryData { encoding: String, data: Vec<String>, } #[derive(Debug, Serialize, Deserialize)] struct Metadata { slot: u64, proof_available_time: i64, prev_publish_time: i64, } #[derive(Debug, Serialize, Deserialize)] struct Response { binary: BinaryData, parsed: Vec<ParsedData>, } /// Fetch the price of a given price feed from Pyth. /// /// # Parameters /// - `price_feed_id`: Price feed ID. /// /// # Returns /// Latest price value from feed. /// /// You can find priceFeedIDs here: https://www.pyth.network/developers/price-feed-ids#stable /// get Hermes service URL from https://docs.pyth.network/price-feeds/api-instances-and-providers/hermes pub async fn fetch_price_by_pyth(price_feed_id: &str) -> Result<f64, Box<dyn std::error::Error>> { let url = format!("{}{}", PYTH_API, "/updates/price/latest"); let ids = [price_feed_id]; let response = reqwest::Client::new().get(&url).query(&ids.iter().map(|id| ("ids[]", *id)).collect::<Vec<_>>()).send().await?; if !response.status().is_success() { return Err(format!("Failed to fetch price: {}", response.status()).into()); } let data: Response = response.json().await?; let parsed_data = data.parsed; if !parsed_data.is_empty() { let price_data = &parsed_data[0]; let price_info = &price_data.price; let price = price_info.price.parse::<f64>().expect("parse price errors"); let expo = price_info.expo; let price = price * (10.0_f64.powi(expo)); return Ok(price); } Err("Price data not available for the given token.".into()) } #[derive(Debug, Serialize, Deserialize)] struct Asset { id: String, attributes: Attributes, } #[derive(Debug, Serialize, Deserialize)] struct Attributes { asset_type: String, base: String, description: String, display_symbol: String, generic_symbol: String, quote_currency: String, schedule: String, symbol: String, } /// Fetch the price feed ID for a given token symbol from Pyth. /// /// # Parameters /// - `token_symbol`: Token symbol /// pub async fn fetch_pyth_price_feed_id(token_symbol: &str) -> Result<String, Box<dyn std::error::Error>> { let url = format!("{}{}?query={}&asset_type=crypto", PYTH_API, "/price_feeds", token_symbol); let response = reqwest::Client::new().get(&url).send().await?; if !response.status().is_success() { return Err(format!("Failed to fetch price: {}", response.status()).into()); } let data: Vec<Asset> = response.json().await?; if data.is_empty() { return Err(format!("No price feed found for {}", token_symbol).into()); } let filter_data: Vec<&Asset> = data.iter().filter(|a| a.attributes.base.to_ascii_lowercase() == token_symbol.to_ascii_lowercase()).collect(); if filter_data.is_empty() { return Err(format!("No price feed found for {}", token_symbol).into()); } Ok(filter_data[0].id.clone()) }
```

# plugins/rugcheck/Cargo.toml

```toml
[package] name = "solagent-plugin-rugcheck" version = "0.1.0" edition = "2021" authors = ["zTgx <beautifularea@gmail.com>"] repository = "https://github.com/zTgx/solagent.rs" keywords = ["solagent", "plugin", "rugcheck"] license = "Apache-2.0" description = "solagent.rs plugin rugcheck" [dependencies] serde_json = "1.0" serde = { version = "1.0", features = ["derive"] } reqwest = { version = "0.12", features = ["json"] }
```

# plugins/rugcheck/src/lib.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. mod token_report_summary; pub use token_report_summary::{fetch_summary_report, TokenCheck}; mod token_report_detailed; pub use token_report_detailed::fetch_detailed_report; pub const RUGCHECK_URL: &str = "https://api.rugcheck.xyz/v1";
```

# plugins/rugcheck/src/token_report_detailed.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use crate::RUGCHECK_URL; use serde_json::Value; /// Fetches a detailed report for a specific token. /// /// # Parameters /// /// - `mint` - The mint address of the token. /// /// # Returns /// Token detailed report. /// /// # Errors /// Throws an error if the API call fails. pub async fn fetch_detailed_report(mint: String) -> Result<Value, Box<dyn std::error::Error>> { let url = format!("{}/tokens/{}/report", RUGCHECK_URL, mint); let response = reqwest::get(&url).await?; if !response.status().is_success() { return Err(format!("HTTP error! status: {}", response.status()).into()); } let data: serde_json::Value = response.json().await?; Ok(data) }
```

# plugins/rugcheck/src/token_report_summary.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use crate::RUGCHECK_URL; use serde::{Deserialize, Serialize}; #[derive(Debug, Serialize, Deserialize, Default)] pub struct Risk { pub name: String, pub level: String, pub description: String, pub score: f64, } #[derive(Debug, Serialize, Deserialize, Default)] pub struct TokenCheck { pub token_program: String, pub token_type: String, pub risks: Vec<Risk>, } /// Fetches a summary report for a specific token. /// /// # Parameters /// /// - `mint` - The mint address of the token. /// /// # Returns /// Token summary report. /// /// # Errors /// Throws an error if the API call fails. pub async fn fetch_summary_report(mint: String) -> Result<TokenCheck, Box<dyn std::error::Error>> { let url = format!("{}/tokens/{}/report/summary", RUGCHECK_URL, mint); let response = reqwest::get(&url).await?; if !response.status().is_success() { return Err(format!("HTTP error! status: {}", response.status()).into()); } let data: serde_json::Value = response.json().await?; let mut token_check = TokenCheck::default(); let token_program = data.get("tokenProgram").and_then(|p| p.as_str()).expect("tokenProgram field"); token_check.token_program = token_program.into(); let token_type = data.get("tokenType").and_then(|p| p.as_str()).expect("tokenType field"); token_check.token_type = token_type.into(); let mut risks: Vec<Risk> = vec![]; let risks_data = data.get("risks").and_then(|p| p.as_array()).expect("risks field"); for risk in risks_data { let mut r = Risk::default(); let name = risk.get("name").and_then(|p| p.as_str()).expect("name field"); let description = risk.get("description").and_then(|p| p.as_str()).expect("description field"); let score = risk.get("score").and_then(|p| p.as_f64()).expect("score field"); let level = risk.get("level").and_then(|p| p.as_str()).expect("level field"); r.name = name.into(); r.description = description.into(); r.score = score; r.level = level.into(); risks.push(r); } token_check.risks = risks; Ok(token_check) }
```

# plugins/solana/Cargo.toml

```toml
[package] name = "solagent-plugin-solana" version = "0.1.1" edition = "2021" authors = ["zTgx <beautifularea@gmail.com>"] repository = "https://github.com/zTgx/solagent.rs" keywords = ["solagent", "plugin", "solana"] license = "Apache-2.0" description = "solagent.rs plugin solana" [dependencies] solagent-core = { path = "../../solagent-core" } serde = { version = "1.0", features = ["derive"] } spl-token = "=7.0.0" spl-associated-token-account = "=6.0.0" solana-account-decoder = "=1.14.16" spl-token-2022 = "=6.0.0" serde_json = "1.0" mpl-token-metadata = { version = "=5.1.0", features = ["serde"] }
```

# plugins/solana/src/close_empty_token_accounts.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use serde::{Deserialize, Serialize}; use solagent_core::{ solana_client::rpc_request::TokenAccountsFilter, solana_sdk::{instruction::Instruction, pubkey::Pubkey, transaction::Transaction}, SolanaAgentKit, }; use spl_token::instruction::close_account; pub const USDC: &str = "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v"; #[derive(serde::Deserialize)] pub struct Parsed { pub info: SplToken, } #[derive(serde::Deserialize)] pub struct SplToken { pub mint: String, #[serde(rename(deserialize = "tokenAmount"))] pub token_amount: Amount, } #[allow(dead_code)] #[derive(serde::Deserialize)] pub struct Amount { pub amount: String, #[serde(rename(deserialize = "uiAmountString"))] ui_amount_string: String, #[serde(rename(deserialize = "uiAmount"))] pub ui_amount: f64, pub decimals: u8, } #[derive(Serialize, Deserialize, Debug, Default)] pub struct CloseEmptyTokenAccountsData { pub signature: String, pub closed_size: usize, } impl CloseEmptyTokenAccountsData { pub fn new(signature: String, closed_size: usize) -> Self { CloseEmptyTokenAccountsData { signature, closed_size } } } /// Close Empty SPL Token accounts of the agent. /// /// # Parameters /// /// - `agent`: An instance of `SolanaAgentKit`. /// /// # Returns /// /// Transaction signature and total number of accounts closed or an error if the account doesn't exist. pub async fn close_empty_token_accounts( agent: &SolanaAgentKit, ) -> Result<CloseEmptyTokenAccountsData, Box<dyn std::error::Error>> { let max_instructions = 40_u32; let mut transaction: Vec<Instruction> = vec![]; let mut closed_size = 0; let token_programs = vec![spl_token::ID, spl_token_2022::ID]; for token_program in token_programs { let accounts = agent .connection .get_token_accounts_by_owner( &agent.wallet.address, TokenAccountsFilter::ProgramId(token_program.to_owned()), ) .expect("get_token_accounts_by_owner"); closed_size += accounts.len(); for account in accounts { if transaction.len() >= max_instructions as usize { break; } if let solana_account_decoder::UiAccountData::Json(d) = &account.account.data { if let Ok(parsed) = serde_json::from_value::<Parsed>(d.parsed.clone()) { if parsed.info.token_amount.amount.parse::<u32>().unwrap_or_default() == 0_u32 && parsed.info.mint != USDC { let account_pubkey = Pubkey::from_str_const(&account.pubkey); if let Ok(instruct) = close_account( &token_program, &account_pubkey, &agent.wallet.address, &agent.wallet.address, &[&agent.wallet.address], ) { transaction.push(instruct); } } } } } } if transaction.is_empty() { return Ok(CloseEmptyTokenAccountsData::default()); } // Create and send transaction let recent_blockhash = agent.connection.get_latest_blockhash()?; let transaction = Transaction::new_signed_with_payer( &transaction, Some(&agent.wallet.address), &[&agent.wallet.wallet], recent_blockhash, ); let signature = agent.connection.send_and_confirm_transaction(&transaction)?; let data = CloseEmptyTokenAccountsData::new(signature.to_string(), closed_size); Ok(data) }
```

# plugins/solana/src/deploy_collection.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use crate::{DeployedData, NFTMetadata}; use mpl_token_metadata::{ instructions::{CreateMasterEditionV3, CreateMetadataAccountV3, CreateMetadataAccountV3InstructionArgs}, types::DataV2, }; use solagent_core::{ solana_client::client_error::ClientError, solana_program, solana_sdk::{ program_pack::Pack, pubkey::Pubkey, signature::{Keypair, Signer}, system_instruction, sysvar, transaction::Transaction, }, SolanaAgentKit, }; use spl_associated_token_account::instruction::create_associated_token_account; /// Deploys a new NFT collection. /// /// # Parameters /// /// - `agent`: An instance of `SolanaAgentKit`. /// - `options`: Collection options including name, URI, royalties, and creators. /// /// # Returns /// /// An object containing the collection address and metadata. pub async fn deploy_collection(agent: &SolanaAgentKit, options: &NFTMetadata) -> Result<DeployedData, ClientError> { // Create a new mint for the collection let collection_mint = Keypair::new(); let collection_mint_pubkey = collection_mint.pubkey(); // Create token mint account let min_rent = agent.connection.get_minimum_balance_for_rent_exemption(spl_token::state::Mint::LEN)?; // Create metadata account let metadata_seeds = &["metadata".as_bytes(), mpl_token_metadata::ID.as_ref(), collection_mint_pubkey.as_ref()]; let (metadata_account, _) = Pubkey::find_program_address(metadata_seeds, &mpl_token_metadata::ID); // Create master edition account let master_edition_seeds = &[ "metadata".as_bytes(), mpl_token_metadata::ID.as_ref(), collection_mint_pubkey.as_ref(), "edition".as_bytes(), ]; let (master_edition_account, _) = Pubkey::find_program_address(master_edition_seeds, &mpl_token_metadata::ID); // Create associated token account for the mint let associated_token_account = spl_associated_token_account::get_associated_token_address(&agent.wallet.address, &collection_mint_pubkey); // let create_mint_account_ix = system_instruction::create_account( // &agent.wallet.address, // &collection_mint.pubkey(), // min_rent, // spl_token::state::Mint::LEN as u64, // &spl_token::id(), // ); // // Initialize mint // let init_mint_ix = spl_token::instruction::initialize_mint( // &spl_token::id(), // &collection_mint_pubkey, // &agent.wallet.address, // Some(&agent.wallet.address), // 0, // ) // .unwrap(); // Create associated token account let create_assoc_account_ix = create_associated_token_account( &agent.wallet.address, &agent.wallet.address, &collection_mint_pubkey, &spl_token::id(), ); // Mint one token to the associated token account let mint_to_ix = spl_token::instruction::mint_to( &spl_token::id(), &collection_mint.pubkey(), &associated_token_account, &agent.wallet.address, &[&agent.wallet.address], 1, ) .expect("mint_to"); // Create metadata let create_metadata_ix = CreateMetadataAccountV3 { metadata: metadata_account, mint: collection_mint.pubkey(), mint_authority: agent.wallet.address, payer: agent.wallet.address, update_authority: (agent.wallet.address, false), system_program: solana_program::system_program::id(), rent: Some(sysvar::rent::id()), } .instruction(CreateMetadataAccountV3InstructionArgs { data: DataV2 { name: options.name.clone(), symbol: "SOLAGENT".to_string(), uri: options.uri.clone(), seller_fee_basis_points: options.basis_points.unwrap_or(0), creators: options.creators.clone(), collection: None, uses: None, }, is_mutable: true, collection_details: None, }); // Create master edition let create_master_edition_ix = CreateMasterEditionV3 { edition: master_edition_account, mint: collection_mint.pubkey(), update_authority: agent.wallet.address, mint_authority: agent.wallet.address, payer: agent.wallet.address, metadata: metadata_account, token_program: spl_token::id(), system_program: solana_program::system_program::id(), rent: Some(sysvar::rent::id()), } .instruction(mpl_token_metadata::instructions::CreateMasterEditionV3InstructionArgs { max_supply: Some(0) }); // Max supply, 0 means unlimited // Create mint account let create_mint_account_ix = system_instruction::create_account( &agent.wallet.address, &collection_mint.pubkey(), min_rent, 82, &spl_token::id(), ); // Initialize mint let init_mint_ix = spl_token::instruction::initialize_mint( &spl_token::id(), &collection_mint.pubkey(), &agent.wallet.address, Some(&agent.wallet.address), 0, ) .expect("initialize_mint"); // Create and send transaction let recent_blockhash = agent.connection.get_latest_blockhash()?; let transaction = Transaction::new_signed_with_payer( &[ create_mint_account_ix, init_mint_ix, create_assoc_account_ix, mint_to_ix, create_metadata_ix, create_master_edition_ix, ], Some(&agent.wallet.address), &[&agent.wallet.wallet, &collection_mint], recent_blockhash, ); let signature = agent.connection.send_and_confirm_transaction(&transaction)?; Ok(DeployedData::new(collection_mint_pubkey.to_string(), signature.to_string())) }
```

# plugins/solana/src/deploy_token.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use crate::DeployedData; use mpl_token_metadata::{ accounts::Metadata, instructions::{CreateV1, CreateV1InstructionArgs}, types::{PrintSupply, TokenStandard}, }; use solagent_core::{ solana_client::{client_error::ClientError, rpc_config::RpcSendTransactionConfig}, solana_program, solana_sdk::{ program_pack::Pack, signature::{Keypair, Signer}, system_instruction, system_program, {commitment_config::CommitmentConfig, transaction::Transaction}, }, SolanaAgentKit, }; use spl_associated_token_account::get_associated_token_address; use spl_token::instruction as spl_token_instruction; /// Deploys a new SPL token. /// /// # Parameters /// /// - `agent`: An instance of `SolanaAgentKit`. /// - `name`: Name of the token. /// - `uri`: URI for the token metadata. /// - `symbol`: Symbol of the token. /// - `decimals`: Number of decimals for the token (default: 9). /// - `initial_supply`: Initial supply to mint (optional). /// /// # Returns /// /// An object containing the token mint address. pub async fn deploy_token( agent: &SolanaAgentKit, name: String, uri: String, symbol: String, decimals: u8, initial_supply: Option<u64>, ) -> Result<DeployedData, ClientError> { let mint = Keypair::new(); let mint_pubkey = mint.pubkey(); // Create token mint account let min_rent = agent.connection.get_minimum_balance_for_rent_exemption(spl_token::state::Mint::LEN)?; let create_mint_account_ix = system_instruction::create_account( &agent.wallet.address, &mint_pubkey, min_rent, spl_token::state::Mint::LEN as u64, &spl_token::id(), ); let initialize_mint_ix = spl_token_instruction::initialize_mint( &spl_token::id(), &mint_pubkey, &agent.wallet.address, Some(&agent.wallet.address), decimals, ) .expect("initialize_mint"); // Create metadata account let (metadata, _x) = Metadata::find_pda(&mint_pubkey); // instruction args let args = CreateV1InstructionArgs { name, symbol, uri, seller_fee_basis_points: 500, primary_sale_happened: false, is_mutable: true, token_standard: TokenStandard::Fungible, collection: None, uses: None, collection_details: None, creators: None, rule_set: None, decimals: Some(18), print_supply: Some(PrintSupply::Zero), }; // instruction accounts let create_ix = CreateV1 { metadata, master_edition: None, mint: (mint_pubkey, true), authority: agent.wallet.address, payer: agent.wallet.address, update_authority: (agent.wallet.address, true), system_program: system_program::ID, sysvar_instructions: solana_program::sysvar::instructions::ID, spl_token_program: Some(spl_token::ID), }; let create_metadata_ix = create_ix.instruction(args); let mut instructions = vec![create_mint_account_ix, initialize_mint_ix, create_metadata_ix]; if let Some(supply) = initial_supply { let associated_token_account = get_associated_token_address(&agent.wallet.address, &mint_pubkey); let create_associated_token_account_ix = spl_associated_token_account::instruction::create_associated_token_account( &agent.wallet.address, &agent.wallet.address, &mint_pubkey, &spl_token::id(), ); let mint_to_ix = spl_token_instruction::mint_to( &spl_token::id(), &mint_pubkey, &associated_token_account, &agent.wallet.address, &[&agent.wallet.address], supply, ) .expect("mint_to"); instructions.push(create_associated_token_account_ix); instructions.push(mint_to_ix); } let recent_blockhash = agent.connection.get_latest_blockhash()?; let transaction = Transaction::new_signed_with_payer( &instructions, Some(&agent.wallet.address), &[&agent.wallet.wallet, &mint], recent_blockhash, ); let signature = agent.connection.send_and_confirm_transaction_with_spinner_and_config( &transaction, CommitmentConfig::finalized(), RpcSendTransactionConfig { skip_preflight: true, ..Default::default() }, )?; Ok(DeployedData::new(mint_pubkey.to_string(), signature.to_string())) }
```

# plugins/solana/src/get_balance_other.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use crate::get_balance; use solagent_core::{ solana_client::{client_error::ClientError, rpc_request::TokenAccountsFilter}, solana_sdk::{native_token::LAMPORTS_PER_SOL, pubkey::Pubkey}, SolanaAgentKit, }; /// Gets the balance of SOL or an SPL token for the specified wallet address. /// /// # Parameters /// /// - `agent`: An instance of `SolanaAgentKit`, which contains the connection to the Solana cluster. /// - `wallet_address`: The public key of the wallet to check balance for. /// - `token_address`: An optional SPL token mint address. If not provided, returns SOL balance. /// /// # Returns /// /// A `Result` containing the balance as a number (in UI units) or an error if fetching fails. pub async fn get_balance_other( agent: &SolanaAgentKit, wallet_address: Pubkey, token_address: Option<Pubkey>, ) -> Result<f64, ClientError> { if let Some(token_address) = token_address { // Get token accounts by owner for the specified token mint address let token_accounts = agent .connection .get_token_accounts_by_owner(&wallet_address, TokenAccountsFilter::Mint(token_address)) .expect("get_token_accounts_by_owner"); if token_accounts.is_empty() { println!("No token accounts found for wallet {} and token {}", wallet_address, token_address); return Ok(0.0); } // Get the first token account's parsed account info let lamports = token_accounts[0].account.lamports; Ok(lamports as f64 / LAMPORTS_PER_SOL as f64) } else { // Get SOL balance if no token address is provided let balance = get_balance(agent, Some(wallet_address.to_string())).await?; Ok(balance) } }
```

# plugins/solana/src/get_balance.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use solagent_core::{ solana_client::client_error::ClientError, solana_sdk::{native_token::LAMPORTS_PER_SOL, pubkey::Pubkey}, SolanaAgentKit, }; use std::str::FromStr; /// Gets the balance of SOL or an SPL token for the agent's wallet. /// /// # Parameters /// /// - `agent`: An instance of `SolanaAgentKit`. /// - `token_address`: An optional SPL token mint address. If not provided, returns the SOL balance. /// /// # Returns /// /// A `Result` that resolves to the balance as a number (in UI units) or an error if the account doesn't exist. pub async fn get_balance(agent: &SolanaAgentKit, token_address: Option<String>) -> Result<f64, ClientError> { if let Some(token_address) = token_address { // Get SPL token account balance if let Ok(pubkey) = Pubkey::from_str(&token_address) { let token_account = agent.connection.get_token_account_balance(&pubkey)?; let ui_amount = token_account.ui_amount.unwrap_or(0.0); return Ok(ui_amount); } } // Get SOL balance let balance = agent.connection.get_balance(&agent.wallet.address)?; Ok(balance as f64 / LAMPORTS_PER_SOL as f64) }
```

# plugins/solana/src/get_tps.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use solagent_core::{solana_client::client_error::ClientError, SolanaAgentKit}; /// Gets the transactions per second (TPS) from the Solana network. /// /// # Parameters /// /// - `agent`: An instance of `SolanaAgentKit` that connects to the Solana cluster. /// /// # Returns /// /// A `Result` containing the TPS as a `f64`, or an error if fetching performance samples fails. pub async fn get_tps(agent: &SolanaAgentKit) -> Result<f64, ClientError> { // Fetch recent performance samples let limit = 1; let perf_samples = agent.connection.get_recent_performance_samples(Some(limit))?; // Check if there are any samples available if !perf_samples.is_empty() { // Calculate TPS let num_transactions = perf_samples[0].num_transactions; let sample_period_secs = perf_samples[0].sample_period_secs; let tps = num_transactions as f64 / sample_period_secs as f64; return Ok(tps); } Ok(0.0) }
```

# plugins/solana/src/get_wallet_address.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use solagent_core::SolanaAgentKit; /// Get the agent's wallet address. /// /// # Parameters /// - `agent`: A `SolanaAgentKit` instance. /// /// # Returns /// A string representing the wallet address in base58 format. pub fn get_wallet_address(agent: &SolanaAgentKit) -> String { agent.wallet.address.to_string() }
```

# plugins/solana/src/lib.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. mod close_empty_token_accounts; pub use close_empty_token_accounts::{close_empty_token_accounts, CloseEmptyTokenAccountsData}; mod get_balance; pub use get_balance::get_balance; mod request_faucet_funds; pub use request_faucet_funds::request_faucet_funds; mod get_tps; pub use get_tps::get_tps; mod transfer; pub use transfer::transfer; mod deploy_token; pub use deploy_token::deploy_token; mod deploy_collection; pub use deploy_collection::deploy_collection; mod get_balance_other; pub use get_balance_other::get_balance_other; mod get_wallet_address; pub use get_wallet_address::get_wallet_address; mod mint_nft; pub use mint_nft::mint_nft_to_collection; use mpl_token_metadata::types::Creator; use serde::{Deserialize, Serialize}; use solagent_core::solana_sdk::pubkey::Pubkey; #[derive(Serialize, Deserialize, Debug)] pub struct DeployedData { pub mint: String, // mint address pub signature: String, // Tx hash } impl DeployedData { pub fn new(mint: String, signature: String) -> Self { DeployedData { mint, signature } } } /// Metadata for deploying an NFT/Collection. /// /// # Fields /// /// - `name`: The name of the NFT. /// - `uri`: The URI for the collection's metadata. /// - `basis_points`: Optional. The basis points for the NFT. /// - `creators`: Optional. A list of creators associated with the NFT. /// // #[derive(BorshSerialize, BorshDeserialize, Clone, Debug, Eq, PartialEq)] #[derive(serde::Serialize, serde::Deserialize)] pub struct NFTMetadata { pub name: String, pub uri: String, pub basis_points: Option<u16>, // Optional basis points pub creators: Option<Vec<Creator>>, // Optional list of creators } impl NFTMetadata { pub fn new(name: &str, uri: &str, basis_points: Option<u16>, creators: Option<Vec<(Pubkey, u8)>>) -> Self { let creators = creators.map(|creator_tuples| { creator_tuples .into_iter() .map(|(pubkey, share)| Creator { address: pubkey, verified: true, share }) .collect::<Vec<Creator>>() }); NFTMetadata { name: name.to_string(), uri: uri.to_string(), basis_points, creators } } }
```

# plugins/solana/src/mint_nft.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use crate::{DeployedData, NFTMetadata}; use mpl_token_metadata::{ instructions::{CreateMasterEditionV3, CreateMetadataAccountV3, CreateMetadataAccountV3InstructionArgs}, types::{Collection, DataV2}, }; use solagent_core::{ solana_client::client_error::ClientError, solana_program, solana_sdk::{ self, program_pack::Pack, pubkey::Pubkey, signature::{Keypair, Signer}, sysvar, transaction::Transaction, }, SolanaAgentKit, }; /// Mints a new NFT /// /// # Arguments /// - `agent`: An instance of `SolanaAgentKit`. /// - `collection`: The public key of the collection to which the NFT belongs. This is used to associate the NFT with a specific collection if applicable. /// - `metadata`: A struct containing the NFT's metadata: /// * `name`: The name of the NFT as a string. /// * `uri`: A URI pointing to the NFT's assets (e.g., image, description) as a string. /// * `seller_fee_basis_points`: An optional seller fee basis points as a number. This represents a percentage of the sale price (e.g., 500 means 5%). /// * `creators`: An optional array of creator information. Each element contains the creator's address (as a string, to be converted to a `Pubkey` in practice) and their share (as a number, representing their contribution percentage). /// /// # Returns /// The transaction signature. pub async fn mint_nft_to_collection( agent: &SolanaAgentKit, collection: Pubkey, metadata: NFTMetadata, ) -> Result<DeployedData, ClientError> { // Create a new keypair for the mint let mint_keypair = Keypair::new(); let mint_pubkey = mint_keypair.pubkey(); // Create token mint account let min_rent = agent.connection.get_minimum_balance_for_rent_exemption(spl_token::state::Mint::LEN)?; // Find the metadata account let metadata_seeds = &["metadata".as_bytes(), mpl_token_metadata::ID.as_ref(), mint_pubkey.as_ref()]; let (metadata_account, _) = Pubkey::find_program_address(metadata_seeds, &mpl_token_metadata::ID); // Create the mint account let create_mint_account_ix = solana_sdk::system_instruction::create_account( &agent.wallet.address, &mint_pubkey, min_rent, 82, &spl_token::id(), ); // Initialize the mint let init_mint_ix = spl_token::instruction::initialize_mint( &spl_token::id(), &mint_pubkey, &agent.wallet.address, Some(&agent.wallet.address), 0, ) .expect("initialize_mint"); // Create Associated Token Account let associated_token_account = spl_associated_token_account::get_associated_token_address(&agent.wallet.address, &mint_pubkey); let create_assoc_account_ix = spl_associated_token_account::instruction::create_associated_token_account( &agent.wallet.address, &agent.wallet.address, &mint_pubkey, &spl_token::id(), ); // Mint one token let mint_to_ix = spl_token::instruction::mint_to( &spl_token::id(), &mint_pubkey, &associated_token_account, &agent.wallet.address, &[&agent.wallet.address], 1, ) .expect("mint_to"); // Create metadata account let create_metadata_ix = CreateMetadataAccountV3 { metadata: metadata_account, mint: mint_pubkey, mint_authority: agent.wallet.address, payer: agent.wallet.address, update_authority: (agent.wallet.address, false), system_program: solana_program::system_program::id(), rent: Some(sysvar::rent::id()), } .instruction(CreateMetadataAccountV3InstructionArgs { data: DataV2 { name: metadata.name.clone(), symbol: "SOLAGENT".to_string(), uri: metadata.uri.clone(), seller_fee_basis_points: metadata.basis_points.unwrap_or(0), creators: metadata.creators, collection: Some(Collection { verified: false, key: collection }), uses: None, }, is_mutable: true, collection_details: None, }); // Create master edition account let master_edition_seeds = &["metadata".as_bytes(), mpl_token_metadata::ID.as_ref(), mint_pubkey.as_ref(), "edition".as_bytes()]; let (master_edition_account, _) = Pubkey::find_program_address(master_edition_seeds, &mpl_token_metadata::ID); let create_master_edition_ix = CreateMasterEditionV3 { edition: master_edition_account, mint: mint_pubkey, update_authority: agent.wallet.address, mint_authority: agent.wallet.address, payer: agent.wallet.address, metadata: metadata_account, token_program: spl_token::id(), system_program: solana_program::system_program::id(), rent: Some(sysvar::rent::id()), } .instruction(mpl_token_metadata::instructions::CreateMasterEditionV3InstructionArgs { max_supply: Some(1) }); // Verify the collection use mpl_token_metadata::instructions::VerifyCollection; let collection_metadata_seeds = &["metadata".as_bytes(), mpl_token_metadata::ID.as_ref(), collection.as_ref()]; let (collection_metadata_account, _) = Pubkey::find_program_address(collection_metadata_seeds, &mpl_token_metadata::ID); let collection_master_edition_seeds = &["metadata".as_bytes(), mpl_token_metadata::ID.as_ref(), collection.as_ref(), "edition".as_bytes()]; let (collection_master_edition_account, _) = Pubkey::find_program_address(collection_master_edition_seeds, &mpl_token_metadata::ID); let verify_collection_ix = VerifyCollection { metadata: metadata_account, collection_authority: agent.wallet.address, payer: agent.wallet.address, collection_mint: collection, collection: collection_metadata_account, collection_master_edition_account, collection_authority_record: None, } .instruction(); // Create the transaction let recent_blockhash = agent.connection.get_latest_blockhash()?; let transaction = Transaction::new_signed_with_payer( &[ create_mint_account_ix, init_mint_ix, create_assoc_account_ix, mint_to_ix, create_metadata_ix, create_master_edition_ix, verify_collection_ix, ], Some(&agent.wallet.address), &[&agent.wallet.wallet, &mint_keypair], recent_blockhash, ); // Send and confirm the transaction let signature = agent.connection.send_and_confirm_transaction(&transaction)?; Ok(DeployedData { mint: mint_pubkey.to_string(), signature: signature.to_string() }) }
```

# plugins/solana/src/request_faucet_funds.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use solagent_core::{ solana_client::client_error::ClientError, solana_sdk::native_token::LAMPORTS_PER_SOL, SolanaAgentKit, }; /// Requests SOL from the Solana faucet (devnet/testnet only). /// /// # Parameters /// /// - `agent`: An instance of `SolanaAgentKit`. /// /// # Returns /// /// A transaction signature as a `String`. /// /// # Errors /// /// Returns an error if the request fails or times out. pub async fn request_faucet_funds(agent: &SolanaAgentKit) -> Result<String, ClientError> { // Request airdrop of 5 SOL (5 * LAMPORTS_PER_SOL) let tx = agent.connection.request_airdrop(&agent.wallet.address, 5 * LAMPORTS_PER_SOL)?; // Confirm the transaction agent.connection.confirm_transaction(&tx)?; Ok(tx.to_string()) }
```

# plugins/solana/src/transfer.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use solagent_core::{ solana_client::client_error::ClientError, solana_sdk::{program_pack::Pack, pubkey::Pubkey, system_instruction, transaction::Transaction}, SolanaAgentKit, }; use spl_associated_token_account::get_associated_token_address; use spl_token::{instruction::transfer as transfer_instruct, state::Mint}; /// Transfer SOL or SPL tokens to a recipient /// /// `agent` - SolanaAgentKit instance /// `to` - Recipient's public key /// `amount` - Amount to transfer /// `mint` - Optional mint address for SPL tokens /// /// Returns the transaction signature. pub async fn transfer( agent: &SolanaAgentKit, to: &str, amount: u64, mint: Option<String>, ) -> Result<String, ClientError> { match mint { Some(mint) => { // Transfer SPL Token let mint = Pubkey::from_str_const(&mint); let to = Pubkey::from_str_const(to); let from_ata = get_associated_token_address(&mint, &agent.wallet.address); let to_ata = get_associated_token_address(&mint, &to); let account_info = &agent.connection.get_account(&mint).expect("get_account"); let mint_info = Mint::unpack_from_slice(&account_info.data).expect("unpack_from_slice"); let adjusted_amount = amount * 10u64.pow(mint_info.decimals as u32); let transfer_instruction = transfer_instruct( &spl_token::id(), &from_ata, &to_ata, &from_ata, &[&agent.wallet.address], adjusted_amount, ) .expect("transfer_instruct"); let transaction = Transaction::new_signed_with_payer( &[transfer_instruction], Some(&agent.wallet.address), &[&agent.wallet.wallet], agent.connection.get_latest_blockhash().expect("new_signed_with_payer"), ); let signature = agent.connection.send_and_confirm_transaction(&transaction).expect("send_and_confirm_transaction"); Ok(signature.to_string()) } None => { let transfer_instruction = system_instruction::transfer(&agent.wallet.address, &Pubkey::from_str_const(to), amount); let transaction = Transaction::new_signed_with_payer( &[transfer_instruction], Some(&agent.wallet.address), &[&agent.wallet.wallet], agent.connection.get_latest_blockhash().expect("get_latest_blockhash"), ); let signature = agent.connection.send_and_confirm_transaction(&transaction).expect("send_and_confirm_transaction"); Ok(signature.to_string()) } } }
```

# plugins/solayer/Cargo.toml

```toml
[package] name = "solagent-plugin-solayer" version = "0.1.1" edition = "2021" authors = ["zTgx <beautifularea@gmail.com>"] repository = "https://github.com/zTgx/solagent.rs" keywords = ["solagent", "plugin", "solayer"] license = "Apache-2.0" description = "solagent.rs plugin solayer" [dependencies] solagent-core = "0.1.3" serde = { version = "1.0", features = ["derive"] } serde_json = "1.0" base64 = "0.22.1" reqwest = { version = "0.12", features = ["json"] } bincode = "1.3.3"
```

# plugins/solayer/src/lib.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. pub mod stake_with_solayer; pub use stake_with_solayer::stake_with_solayer;
```

# plugins/solayer/src/stake_with_solayer.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use base64::{engine::general_purpose, Engine}; use reqwest::header::{HeaderMap, HeaderValue, CONTENT_TYPE}; use serde::{Deserialize, Serialize}; use solagent_core::{ solana_sdk::{commitment_config::CommitmentConfig, transaction::VersionedTransaction}, SolanaAgentKit, }; #[derive(Serialize)] struct StakeRequest { account: String, } #[derive(Deserialize)] struct StakeResponse { transaction: String, } // Stake SOL with Solayer /// Create a new task on Gibwork /// /// # Arguments /// /// * `agent` - SolanaAgentKit instance /// * `amount` - Amount of SOL to stake /// /// # Returns /// /// Transaction signature pub async fn stake_with_solayer(agent: &SolanaAgentKit, amount: f64) -> Result<String, Box<dyn std::error::Error>> { let url = format!("https://app.solayer.org/api/action/restake/ssol?amount={}", amount); let mut headers = HeaderMap::new(); headers.insert(CONTENT_TYPE, HeaderValue::from_static("application/json")); let request = StakeRequest { account: agent.wallet.address.to_string() }; let client = reqwest::Client::new(); let response = client .post(&url) .headers(headers) .json(&request) .send() .await .map_err(|e| format!("Failed to send request: {}", e))?; if !response.status().is_success() { let error_data: serde_json::Value = response.json().await.map_err(|e| format!("Failed to parse error response: {}", e))?; let message = error_data.get("message").and_then(|v| v.as_str()).unwrap_or("Staking request failed"); return Err(message.to_string().into()); } let stake_response: StakeResponse = response.json().await.map_err(|e| format!("Failed to parse stake response: {}", e))?; let transaction_data = general_purpose::STANDARD.decode(stake_response.transaction.as_str())?; let versioned_transaction: VersionedTransaction = bincode::deserialize(&transaction_data)?; let signed_transaction = VersionedTransaction::try_new(versioned_transaction.message, &[&agent.wallet.wallet])?; let signature = agent.connection.send_transaction(&signed_transaction)?; let latest_blockhash = agent.connection.get_latest_blockhash()?; agent.connection.confirm_transaction_with_spinner(&signature, &latest_blockhash, CommitmentConfig::confirmed())?; Ok(signature.to_string()) }
```

# plugins/story/Cargo.toml

```toml
[package] name = "solagent-plugin-story" version = "0.1.2" edition = "2021" authors = ["zTgx <beautifularea@gmail.com>"] repository = "https://github.com/zTgx/solagent.rs" keywords = ["solagent", "plugin", "story"] license = "Apache-2.0" description = "solagent.rs plugin story" [dependencies] serde_json = "1.0" reqwest = { version = "0.12", features = ["json"] } serde = { version = "1.0", features = ["derive"] }
```

# plugins/story/src/lib.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. mod transaction; pub use transaction::*; mod license_tokens; pub use license_tokens::*; const STORY_API_URL: &str = "https://staging-api.storyprotocol.net/api/v2"; use serde::{Deserialize, Serialize}; #[derive(Serialize, Deserialize)] pub struct StoryConfig { pub api_key: String, pub chain: u32, } impl StoryConfig { pub fn new(api_key: &str, chain: u32) -> Self { StoryConfig { api_key: api_key.to_string(), chain } } } #[derive(Serialize, Deserialize)] pub struct StoryPaginationObject { pub after: String, pub before: String, pub limit: u32, } #[derive(Serialize, Deserialize)] pub struct StoryBodyWhereObject { pub block_number: String, pub id: String, pub ip_id: String, pub resource_type: String, pub transaction_hash: String, } #[derive(Serialize, Deserialize)] pub struct StoryBodyParams { order_by: String, order_direction: String, pagination: StoryPaginationObject, where_obj: StoryBodyWhereObject, }
```

# plugins/story/src/license_tokens.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use serde_json::json; use crate::{StoryBodyParams, StoryConfig, STORY_API_URL}; /// Retrieve a LicenseToken /// /// # Arguments /// /// * `config` - API Config /// * `license_token_id` - License Token ID /// /// # Returns /// /// License Token data pub async fn get_license_token( config: &StoryConfig, license_token_id: &str, ) -> Result<serde_json::Value, Box<dyn std::error::Error>> { let url = format!("{}/licenses/tokens/{}", STORY_API_URL, license_token_id); let client = reqwest::Client::new(); let response = client .get(url) .header("X-Api-Key", &config.api_key) .header("X-Chain", config.chain) .header("accept", "application/json") .send() .await?; let data: serde_json::Value = response.json().await?; Ok(data) } /// Retrieve a paginated, filtered list of LicenseTokens /// /// # Arguments /// /// * `config` - API Config /// * `story_body_params` - Query Parameters must be wrapped in options object and may be empty. OrderBy must be blockNumber, resourceType or empty. /// /// # Returns /// /// list of LicenseTokens pub async fn list_license_tokens( config: &StoryConfig, body: Option<StoryBodyParams>, ) -> Result<serde_json::Value, Box<dyn std::error::Error>> { let url = format!("{}/licenses/tokens", STORY_API_URL); let client = reqwest::Client::new(); let mut request_builder = client .post(url) .header("X-Api-Key", &config.api_key) .header("X-Chain", config.chain) .header("accept", "application/json") .header("content-type", "application/json"); if let Some(body_params) = body { let request_data = json!({ "options": body_params }); let json_body = serde_json::to_string(&request_data)?; request_builder = request_builder.body(json_body); } let response = request_builder.send().await?; let data: serde_json::Value = response.json().await?; Ok(data) }
```

# plugins/story/src/transaction.rs

```rs
// Copyright 2025 zTgx // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS IS" BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. use crate::{StoryBodyParams, StoryConfig, STORY_API_URL}; /// Retrieve a Transaction /// /// # Arguments /// /// * `config` - API Config /// * `trx_id` - Transaction ID /// /// # Returns /// /// Transaction pub async fn get_a_transaction( config: &StoryConfig, trx_id: &str, ) -> Result<serde_json::Value, Box<dyn std::error::Error>> { let url = format!("{}/transactions/{}", STORY_API_URL, trx_id); let client = reqwest::Client::new(); let response = client .get(url) .header("X-Api-Key", &config.api_key) .header("X-Chain", config.chain) .header("accept", "application/json") .send() .await?; let data: serde_json::Value = response.json().await?; Ok(data) } /// Retrieve a paginated, filtered list of Transactions /// /// # Arguments /// /// * `config` - API Config /// * `story_body_params` - Query Parameters must be wrapped in options object and may be empty. OrderBy must be blockNumber, resourceType or empty. /// /// # Returns /// /// list of Transactions pub async fn list_transactions( config: &StoryConfig, body: Option<StoryBodyParams>, ) -> Result<serde_json::Value, Box<dyn std::error::Error>> { query_transactions(STORY_API_URL, config, body).await } /// Retrieve a paginated, filtered list of Latest Transactions /// /// # Arguments /// /// * `config` - API Config /// * `story_body_params` - Query Parameters must be wrapped in options object and may be empty. OrderBy must be blockNumber, resourceType or empty. /// /// # Returns /// /// list of Latest Transactions pub async fn list_latest_transactions( config: &StoryConfig, body: Option<StoryBodyParams>, ) -> Result<serde_json::Value, Box<dyn std::error::Error>> { let url = format!("{}/transactions/latest", STORY_API_URL); query_transactions(&url, config, body).await } async fn query_transactions( url: &str, config: &StoryConfig, body: Option<StoryBodyParams>, ) -> Result<serde_json::Value, Box<dyn std::error::Error>> { let client = reqwest::Client::new(); let mut request_builder = client .post(url) .header("X-Api-Key", &config.api_key) .header("X-Chain", config.chain) .header("accept", "application/json") .header("content-type", "application/json"); if let Some(body_params) = body { let json_body = serde_json::to_string(&body_params)?; request_builder = request_builder.body(json_body); } let response = request_builder.send().await?; let data: serde_json::Value = response.json().await?; Ok(data) }
```

# plugins/twitter/.gitignore

```
# Generated by Cargo /target/ # Backup files **/*.rs.bk Cargo.lock # IDE specific files .idea/ .vscode/ *.swp *.swo # Debug files **/*.pdb .env .repo *.sqlite
```

# plugins/twitter/Cargo.toml

```toml
[package] name = "cainam-twitter" version = "0.1.2" edition = "2021" description = "A Twitter/X client library using cookies" license = "MIT" readme = "README.md" keywords = ["twitter", "x.com", "api", "client"] categories = ["api-bindings", "web-programming::http-client"] [dependencies] reqwest = { version = "0.11", features = ["json", "cookies", "multipart"] } serde = { version = "1.0", features = ["derive"] } serde_json = "1.0" cookie = "0.16" async-trait = "0.1" thiserror = "1.0" chrono = { version = "0.4", features = ["serde"] } lazy_static = "1.4" totp-rs = "5.4" urlencoding = "2.1.3" regex = "1.5" url = "2.5.0" tokio = { version = "1.0", features = ["full"] } tracing = "0.1" # Environment variables dotenvy = "0.15.7"
```

# plugins/twitter/direct_message_conversations.json

```json
{ "conversations": [ { "conversation_id": "1850569762965016576-1855214422265925632", "messages": [ { "id": "1872159766019203135", "text": "1", "sender_id": "1855214422265925632", "recipient_id": "1850569762965016576", "created_at": "1735192645000", "media_urls": null, "sender_screen_name": "justchillbotx", "recipient_screen_name": "VibesGogh" }, { "id": "1872159787192062264", "text": "1", "sender_id": "1850569762965016576", "recipient_id": "1855214422265925632", "created_at": "1735192650000", "media_urls": null, "sender_screen_name": "VibesGogh", "recipient_screen_name": "justchillbotx" }, { "id": "1872159896600523178", "text": "2", "sender_id": "1855214422265925632", "recipient_id": "1850569762965016576", "created_at": "1735192677000", "media_urls": null, "sender_screen_name": "justchillbotx", "recipient_screen_name": "VibesGogh" }, { "id": "1872160868026487285", "text": "3", "sender_id": "1850569762965016576", "recipient_id": "1855214422265925632", "created_at": "1735192908000", "media_urls": null, "sender_screen_name": "VibesGogh", "recipient_screen_name": "justchillbotx" } ], "participants": [ { "id": "1850569762965016576", "screen_name": "VibesGogh" }, { "id": "1855214422265925632", "screen_name": "justchillbotx" } ] }, { "conversation_id": "401019307-1855214422265925632", "messages": [ { "id": "1872159370064314383", "text": "g", "sender_id": "401019307", "recipient_id": "1855214422265925632", "created_at": "1735192551000", "media_urls": null, "sender_screen_name": "0xleductam", "recipient_screen_name": "justchillbotx" }, { "id": "1872159897951019164", "text": "2", "sender_id": "1855214422265925632", "recipient_id": "401019307", "created_at": "1735192677000", "media_urls": null, "sender_screen_name": "justchillbotx", "recipient_screen_name": "0xleductam" }, { "id": "1872159970810302655", "text": "1", "sender_id": "401019307", "recipient_id": "1855214422265925632", "created_at": "1735192694000", "media_urls": null, "sender_screen_name": "0xleductam", "recipient_screen_name": "justchillbotx" }, { "id": "1872159993895801288", "text": "2", "sender_id": "1855214422265925632", "recipient_id": "401019307", "created_at": "1735192700000", "media_urls": null, "sender_screen_name": "justchillbotx", "recipient_screen_name": "0xleductam" } ], "participants": [ { "id": "401019307", "screen_name": "0xleductam" }, { "id": "1855214422265925632", "screen_name": "justchillbotx" } ] } ], "users": [ { "id": "1850569762965016576", "screen_name": "VibesGogh", "name": "Apple Crab", "profile_image_url": "https://pbs.twimg.com/profile_images/1869408265190432768/kgaMZvYi_normal.jpg", "description": null, "verified": false, "protected": false, "followers_count": 85, "friends_count": 4 }, { "id": "1855214422265925632", "screen_name": "justchillbotx", "name": "Just a chill bot", "profile_image_url": "https://pbs.twimg.com/profile_images/1862393701365841920/z5h3OQZQ_normal.jpg", "description": "GRv9SNShfSuc5EgE72wcenAo9TrHyjdqQkXLrxY7pump", "verified": false, "protected": false, "followers_count": 40, "friends_count": 4 }, { "id": "401019307", "screen_name": "0xleductam", "name": "leductam", "profile_image_url": "https://pbs.twimg.com/profile_images/1850859270377779200/tIDUy51Z_normal.jpg", "description": "life has dreams, each is wonderful.", "verified": false, "protected": false, "followers_count": 3751, "friends_count": 1601 } ], "cursor": "GRwmnoDS-eTBn_szFurH2NH9mKD7MyUOAAA", "last_seen_event_id": "1872160868026487285", "trusted_last_seen_event_id": "1872160868026487285", "untrusted_last_seen_event_id": "0", "inbox_timelines": { "trusted": { "status": "AT_END", "min_entry_id": "1872159993895801288" }, "untrusted": { "status": "AT_END", "min_entry_id": null } }, "user_id": "1855214422265925632" }
```

# plugins/twitter/README.md

```md
# Twitter Scraper Library A Rust port of the TypeScript library (<https://github.com/ai16z/agent-twitter-client>). This package does not require the Twitter API to use! A Rust library that provides a scraper interface for the Twitter API. Easily interact with Twitter through authentication, timeline fetching, user operations, and more. ## Features - Authentication with cookies - Comprehensive user profile management - Timeline retrieval - Tweet interactions (like, retweet, post) - Advanced search capabilities - User relationship management (follow/unfollow) ## Installation Add these dependencies to your `Cargo.toml`: \`\`\`toml [dependencies] cainam-twitter = "0.1.2" tokio = { version = "1.0", features = ["full"] } # Environment variables dotenvy = "0.15.7" \`\`\` ## Quick Start ### Authentication #### Method 1: Login with Credentials \`\`\`rust use agent_twitter_client::scraper::Scraper; use dotenv::dotenv; use std::env; #[tokio::main] async fn main() -> Result<()> { dotenv().ok(); let mut scraper = Scraper::new().await?; scraper.login( env::var("TWITTER_USERNAME")?, env::var("TWITTER_PASSWORD")?, Some(env::var("TWITTER_EMAIL")?), Some(env::var("TWITTER_2FA_SECRET")?) ).await?; Ok(()) } \`\`\` #### Method 2: Login with Cookie String \`\`\`rust use agent_twitter_client::scraper::Scraper; use agent_twitter_client::error::Result; use dotenv::dotenv; #[tokio::main] async fn main() -> Result<()> { dotenv().ok(); let mut scraper = Scraper::new().await?; let cookie_string = std::env::var("TWITTER_COOKIE_STRING") .expect("TWITTER_COOKIE_STRING environment variable not set"); scraper.set_from_cookie_string(&cookie_string).await?; Ok(()) } \`\`\` ### User Operations \`\`\`rust use agent_twitter_client::scraper::Scraper; use agent_twitter_client::error::Result; use dotenv::dotenv; #[tokio::main] async fn main() -> Result<()> { dotenv().ok(); let mut scraper = Scraper::new().await?; let cookie_string = std::env::var("TWITTER_COOKIE_STRING") .expect("TWITTER_COOKIE_STRING environment variable not set"); scraper.set_from_cookie_string(&cookie_string).await?; // Follow a user scraper.follow_user("Rina_RIG").await?; // Get user profile let profile = scraper.get_profile("Rina_RIG").await?; // Get user's followers let (followers, next_cursor) = scraper.get_followers("Rina_RIG", 20, None).await?; Ok(()) } \`\`\` # Get direct message conversations \`\`\`rust use agent_twitter_client::scraper::Scraper; use agent_twitter_client::error::Result; use dotenv::dotenv; use std::io::Write; #[tokio::main] async fn main() -> Result<()> { dotenv().ok(); let mut scraper = Scraper::new().await?; let cookie_string = std::env::var("TWITTER_COOKIE_STRING") .expect("TWITTER_COOKIE_STRING environment variable not set"); scraper.set_from_cookie_string(&cookie_string).await?; let home_timeline = scraper.get_direct_message_conversations("chakaboommm", None).await?; println!("Direct message conversations: {:?}", home_timeline); } \`\`\` # Send direct message \`\`\`rust use agent_twitter_client::scraper::Scraper; use agent_twitter_client::error::Result; use dotenv::dotenv; #[tokio::main] async fn main() -> Result<()> { dotenv().ok(); let mut scraper = Scraper::new().await?; let cookie_string = std::env::var("TWITTER_COOKIE_STRING") .expect("TWITTER_COOKIE_STRING environment variable not set"); scraper.set_from_cookie_string(&cookie_string).await?; let conversation_id = "1234567890"; scraper.send_direct_message(conversation_id, "Hello, world!").await?; Ok(()) } \`\`\` ### Search Operations \`\`\`rust use agent_twitter_client::scraper::Scraper; use agent_twitter_client::search::SearchMode; use agent_twitter_client::error::Result; use dotenv::dotenv; #[tokio::main] async fn main() -> Result<()> { dotenv().ok(); let mut scraper = Scraper::new().await?; let cookie_string = std::env::var("TWITTER_COOKIE_STRING") .expect("TWITTER_COOKIE_STRING environment variable not set"); scraper.set_from_cookie_string(&cookie_string).await?; // Search tweets let tweets = scraper.search_tweets( "@Rina_RIG", 20, SearchMode::Latest, None ).await?; // Search user profiles let profiles = scraper.search_profiles("rust", 20, None).await?; Ok(()) } \`\`\` ### Timeline Operations \`\`\`rust use agent_twitter_client::scraper::Scraper; use agent_twitter_client::error::Result; use dotenv::dotenv; #[tokio::main] async fn main() -> Result<()> { dotenv().ok(); let mut scraper = Scraper::new().await?; let cookie_string = std::env::var("TWITTER_COOKIE_STRING") .expect("TWITTER_COOKIE_STRING environment variable not set"); scraper.set_from_cookie_string(&cookie_string).await?; // Get home timeline let tweets = scraper.get_home_timeline(20, vec![]).await?; // Get user's tweets and replies let tweets = scraper.fetch_tweets_and_replies("username", 20, None).await?; Ok(()) } \`\`\` ### Tweet Interactions \`\`\`rust use std::fs::File; use std::io::Read; use agent_twitter_client::scraper::Scraper; use agent_twitter_client::error::Result; use dotenv::dotenv; #[tokio::main] async fn main() -> Result<()> { dotenv().ok(); let mut scraper = Scraper::new().await?; let cookie_string = std::env::var("TWITTER_COOKIE_STRING") .expect("TWITTER_COOKIE_STRING environment variable not set"); scraper.set_from_cookie_string(&cookie_string).await?; // Like a tweet scraper.like_tweet("tweet_id").await?; // Retweet scraper.retweet("tweet_id").await?; // Post a new tweet scraper.send_tweet("Hello, Twitter!", None, None).await?; // Send a simple tweet let tweet = scraper.send_tweet("Hello world!", None, None).await?; // Create media data tuple with image data and MIME type let mut file = File::open("image.jpg")?; let mut image_data = Vec::new(); file.read_to_end(&mut image_data)?; let media_data = vec![(image_data, "image/jpeg".to_string())]; // Send the tweet with the image let tweet_with_media = scraper.send_tweet( "Check out this image!", None, Some(media_data) ).await?; Ok(()) } \`\`\` ## Configuration Create a `.env` file with your credentials: \`\`\`env TWITTER_USERNAME=your_username TWITTER_PASSWORD=your_password TWITTER_EMAIL=your_email@example.com TWITTER_2FA_SECRET=your_2fa_secret # Optional TWITTER_COOKIE_STRING='your_cookie_string' \`\`\` ## License Created by [Rina](https://x.com/Rina_RIG) ![banner](https://github.com/user-attachments/assets/b2e37bc8-7fe9-4285-a85b-c41dae9d288b) ## Contributing We welcome contributions! Please feel free to submit a Pull Request.
```

# plugins/twitter/src/api/client.rs

```rs
use crate::auth::user_auth::TwitterUserAuth; use crate::error::{Result, TwitterError}; use crate::models::Tweet; use reqwest::{Client, Method}; use serde::de::DeserializeOwned; use std::time::Duration; pub struct TwitterClient { pub client: Client, pub auth: TwitterUserAuth, } impl TwitterClient { pub fn new(auth: TwitterUserAuth) -> Result<Self> { let client = Client::builder() .timeout(Duration::from_secs(30)) .cookie_store(true) .build()?; Ok(Self { client, auth }) } pub async fn send_tweet(&self, text: &str, media_ids: Option<Vec<String>>) -> Result<Tweet> { let mut params = serde_json::json!({ "text": text, }); if let Some(ids) = media_ids { params["media"] = serde_json::json!({ "media_ids": ids }); } let endpoint = "https://api.twitter.com/2/tweets"; self.post(endpoint, Some(params)).await } pub async fn get_tweet(&self, tweet_id: &str) -> Result<Tweet> { let endpoint = format!("https://api.twitter.com/2/tweets/{}", tweet_id); self.get(&endpoint).await } pub async fn get_user_tweets(&self, user_id: &str, limit: usize) -> Result<Vec<Tweet>> { let endpoint = format!("https://api.twitter.com/2/users/{}/tweets", user_id); let params = serde_json::json!({ "max_results": limit, "tweet.fields": "created_at,author_id,conversation_id,public_metrics" }); self.get_with_params(&endpoint, Some(params)).await } pub async fn get<T: DeserializeOwned>(&self, endpoint: &str) -> Result<T> { self.request(Method::GET, endpoint, None).await } pub async fn get_with_params<T: DeserializeOwned>( &self, endpoint: &str, params: Option<serde_json::Value>, ) -> Result<T> { self.request(Method::GET, endpoint, params).await } pub async fn post<T: DeserializeOwned>( &self, endpoint: &str, params: Option<serde_json::Value>, ) -> Result<T> { self.request(Method::POST, endpoint, params).await } pub async fn request<T: DeserializeOwned>( &self, method: Method, endpoint: &str, params: Option<serde_json::Value>, ) -> Result<T> { let mut headers = reqwest::header::HeaderMap::new(); self.auth.install_headers(&mut headers).await?; let mut request = self.client.request(method, endpoint); request = request.headers(headers); if let Some(params) = params { request = request.json(&params); } let response = request.send().await?; if response.status().is_success() { Ok(response.json().await?) } else { Err(TwitterError::Api(format!( "Request failed with status: {}", response.status() ))) } } }
```

# plugins/twitter/src/api/endpoints.rs

```rs
use std::collections::HashMap; use urlencoding; // Constants for default options matching TypeScript pub const DEFAULT_EXPANSIONS: &[&str] = &[ "attachments.poll_ids", "attachments.media_keys", "author_id", "referenced_tweets.id", "in_reply_to_user_id", "edit_history_tweet_ids", "geo.place_id", "entities.mentions.username", "referenced_tweets.id.author_id", ]; pub const DEFAULT_TWEET_FIELDS: &[&str] = &[ "attachments", "author_id", "context_annotations", "conversation_id", "created_at", "entities", "geo", "id", "in_reply_to_user_id", "lang", "public_metrics", "edit_controls", "possibly_sensitive", "referenced_tweets", "reply_settings", "source", "text", "withheld", "note_tweet", ]; #[derive(Debug, Clone)] pub struct ApiEndpoint { pub url: String, pub variables: Option<HashMap<String, serde_json::Value>>, pub features: Option<HashMap<String, bool>>, pub field_toggles: Option<HashMap<String, bool>>, } impl ApiEndpoint { pub fn to_request_url(&self) -> String { let mut params = Vec::new(); if let Some(variables) = &self.variables { params.push(format!( "variables={}", urlencoding::encode(&serde_json::to_string(&variables).unwrap()) )); } if let Some(features) = &self.features { params.push(format!( "features={}", urlencoding::encode(&serde_json::to_string(&features).unwrap()) )); } if let Some(toggles) = &self.field_toggles { params.push(format!( "fieldToggles={}", urlencoding::encode(&serde_json::to_string(&toggles).unwrap()) )); } if params.is_empty() { self.url.clone() } else { format!("{}?{}", self.url, params.join("&")) } } } pub struct Endpoints; impl Endpoints { pub fn tweet_detail(tweet_id: &str) -> ApiEndpoint { ApiEndpoint { url: "https://twitter.com/i/api/graphql/xOhkmRac04YFZmOzU9PJHg/TweetDetail".to_string(), variables: Some(HashMap::from([ ("focalTweetId".to_string(), tweet_id.into()), ("with_rux_injections".to_string(), false.into()), ("includePromotedContent".to_string(), true.into()), ("withCommunity".to_string(), true.into()), ( "withQuickPromoteEligibilityTweetFields".to_string(), true.into(), ), ("withBirdwatchNotes".to_string(), true.into()), ("withVoice".to_string(), true.into()), ("withV2Timeline".to_string(), true.into()), ])), features: Some(HashMap::from([ ( "responsive_web_graphql_exclude_directive_enabled".to_string(), true, ), ("verified_phone_label_enabled".to_string(), false), ( "creator_subscriptions_tweet_preview_api_enabled".to_string(), true, ), ( "responsive_web_graphql_timeline_navigation_enabled".to_string(), true, ), ( "responsive_web_graphql_skip_user_profile_image_extensions_enabled".to_string(), false, ), ("tweetypie_unmention_optimization_enabled".to_string(), true), ("responsive_web_edit_tweet_api_enabled".to_string(), true), ( "graphql_is_translatable_rweb_tweet_is_translatable_enabled".to_string(), true, ), ("view_counts_everywhere_api_enabled".to_string(), true), ("longform_notetweets_consumption_enabled".to_string(), true), ("tweet_awards_web_tipping_enabled".to_string(), false), ( "freedom_of_speech_not_reach_fetch_enabled".to_string(), true, ), ("standardized_nudges_misinfo".to_string(), true), ( "responsive_web_twitter_article_tweet_consumption_enabled".to_string(), false, ), ( "tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled" .to_string(), true, ), ( "longform_notetweets_rich_text_read_enabled".to_string(), true, ), ("longform_notetweets_inline_media_enabled".to_string(), true), ( "responsive_web_media_download_video_enabled".to_string(), false, ), ("responsive_web_enhance_cards_enabled".to_string(), false), ])), field_toggles: Some(HashMap::from([( "withArticleRichContentState".to_string(), false, )])), } } pub fn tweet_by_rest_id(tweet_id: &str) -> ApiEndpoint { ApiEndpoint { url: "https://twitter.com/i/api/graphql/DJS3BdhUhcaEpZ7B7irJDg/TweetResultByRestId" .to_string(), variables: Some(HashMap::from([ ("tweetId".to_string(), tweet_id.into()), ("withCommunity".to_string(), false.into()), ("includePromotedContent".to_string(), false.into()), ("withVoice".to_string(), false.into()), ])), features: Some(HashMap::from([ ( "creator_subscriptions_tweet_preview_api_enabled".to_string(), true, ), ("tweetypie_unmention_optimization_enabled".to_string(), true), ("responsive_web_edit_tweet_api_enabled".to_string(), true), ( "graphql_is_translatable_rweb_tweet_is_translatable_enabled".to_string(), true, ), ("view_counts_everywhere_api_enabled".to_string(), true), ("longform_notetweets_consumption_enabled".to_string(), true), ( "responsive_web_twitter_article_tweet_consumption_enabled".to_string(), false, ), ("tweet_awards_web_tipping_enabled".to_string(), false), ( "freedom_of_speech_not_reach_fetch_enabled".to_string(), true, ), ("standardized_nudges_misinfo".to_string(), true), ])), field_toggles: None, } } pub fn user_tweets(user_id: &str, count: i32, cursor: Option<&str>) -> ApiEndpoint { let mut variables = HashMap::from([ ("userId".to_string(), user_id.into()), ("count".to_string(), count.into()), ("includePromotedContent".to_string(), true.into()), ( "withQuickPromoteEligibilityTweetFields".to_string(), true.into(), ), ("withVoice".to_string(), true.into()), ("withV2Timeline".to_string(), true.into()), ]); if let Some(cursor_value) = cursor { variables.insert("cursor".to_string(), cursor_value.into()); } ApiEndpoint { url: "https://twitter.com/i/api/graphql/V7H0Ap3_Hh2FyS75OCDO3Q/UserTweets".to_string(), variables: Some(variables), features: Some(HashMap::from([ ("rweb_tipjar_consumption_enabled".to_string(), true), ( "responsive_web_graphql_exclude_directive_enabled".to_string(), true, ), ("verified_phone_label_enabled".to_string(), false), ( "creator_subscriptions_tweet_preview_api_enabled".to_string(), true, ), ( "responsive_web_graphql_timeline_navigation_enabled".to_string(), true, ), ( "responsive_web_graphql_skip_user_profile_image_extensions_enabled".to_string(), false, ), ( "communities_web_enable_tweet_community_results_fetch".to_string(), true, ), ( "c9s_tweet_anatomy_moderator_badge_enabled".to_string(), true, ), ("articles_preview_enabled".to_string(), true), ("tweetypie_unmention_optimization_enabled".to_string(), true), ("responsive_web_edit_tweet_api_enabled".to_string(), true), ( "graphql_is_translatable_rweb_tweet_is_translatable_enabled".to_string(), true, ), ("view_counts_everywhere_api_enabled".to_string(), true), ("longform_notetweets_consumption_enabled".to_string(), true), ( "responsive_web_twitter_article_tweet_consumption_enabled".to_string(), true, ), ("tweet_awards_web_tipping_enabled".to_string(), false), ( "creator_subscriptions_quote_tweet_preview_enabled".to_string(), false, ), ( "freedom_of_speech_not_reach_fetch_enabled".to_string(), true, ), ("standardized_nudges_misinfo".to_string(), true), ( "tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled" .to_string(), true, ), ("rweb_video_timestamps_enabled".to_string(), true), ( "longform_notetweets_rich_text_read_enabled".to_string(), true, ), ("longform_notetweets_inline_media_enabled".to_string(), true), ("responsive_web_enhance_cards_enabled".to_string(), false), ])), field_toggles: Some(HashMap::from([("withArticlePlainText".to_string(), false)])), } } pub fn user_tweets_and_replies(user_id: &str, count: i32, cursor: Option<&str>) -> ApiEndpoint { let mut variables = HashMap::from([ ("userId".to_string(), user_id.into()), ("count".to_string(), count.into()), ("includePromotedContent".to_string(), true.into()), ("withCommunity".to_string(), true.into()), ("withVoice".to_string(), true.into()), ("withV2Timeline".to_string(), true.into()), ]); if let Some(cursor_value) = cursor { variables.insert("cursor".to_string(), cursor_value.into()); } ApiEndpoint { url: "https://twitter.com/i/api/graphql/E4wA5vo2sjVyvpliUffSCw/UserTweetsAndReplies" .to_string(), variables: Some(variables), features: Some(HashMap::from([ ("rweb_tipjar_consumption_enabled".to_string(), true), ( "responsive_web_graphql_exclude_directive_enabled".to_string(), true, ), ("verified_phone_label_enabled".to_string(), false), ( "creator_subscriptions_tweet_preview_api_enabled".to_string(), true, ), ( "responsive_web_graphql_timeline_navigation_enabled".to_string(), true, ), ( "responsive_web_graphql_skip_user_profile_image_extensions_enabled".to_string(), false, ), ( "communities_web_enable_tweet_community_results_fetch".to_string(), true, ), ( "c9s_tweet_anatomy_moderator_badge_enabled".to_string(), true, ), ("articles_preview_enabled".to_string(), true), ("tweetypie_unmention_optimization_enabled".to_string(), true), ("responsive_web_edit_tweet_api_enabled".to_string(), true), ( "graphql_is_translatable_rweb_tweet_is_translatable_enabled".to_string(), true, ), ("view_counts_everywhere_api_enabled".to_string(), true), ("longform_notetweets_consumption_enabled".to_string(), true), ( "responsive_web_twitter_article_tweet_consumption_enabled".to_string(), true, ), ("tweet_awards_web_tipping_enabled".to_string(), false), ( "creator_subscriptions_quote_tweet_preview_enabled".to_string(), false, ), ( "freedom_of_speech_not_reach_fetch_enabled".to_string(), true, ), ("standardized_nudges_misinfo".to_string(), true), ( "tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled" .to_string(), true, ), ("rweb_video_timestamps_enabled".to_string(), true), ( "longform_notetweets_rich_text_read_enabled".to_string(), true, ), ("longform_notetweets_inline_media_enabled".to_string(), true), ("responsive_web_enhance_cards_enabled".to_string(), false), ])), field_toggles: Some(HashMap::from([("withArticlePlainText".to_string(), false)])), } } }
```

# plugins/twitter/src/api/mod.rs

```rs
pub mod client; pub mod endpoints; pub mod requests; pub use client::TwitterClient; pub use endpoints::Endpoints; pub use reqwest::Method;
```

# plugins/twitter/src/api/requests.rs

```rs
use crate::error::Result; use reqwest::multipart::Form; use reqwest::{header::HeaderMap, Client, Method}; use serde::de::DeserializeOwned; pub async fn request_api<T>( client: &Client, url: &str, headers: HeaderMap, method: Method, body: Option<serde_json::Value>, ) -> Result<(T, HeaderMap)> where T: DeserializeOwned, { let mut request = client.request(method, url).headers(headers); if let Some(json_body) = body { request = request.json(&json_body); } let response = request.send().await?; if response.status().is_success() { let headers = response.headers().clone(); let text = response.text().await?; let parsed: T = serde_json::from_str(&text)?; Ok((parsed, headers)) } else { Err(crate::error::TwitterError::Api(format!( "Request failed with status: {}", response.status() ))) } } pub async fn get_guest_token(client: &Client, bearer_token: &str) -> Result<String> { let mut headers = HeaderMap::new(); headers.insert( "Authorization", format!("Bearer {}", bearer_token).parse().unwrap(), ); let (response, _) = request_api::<serde_json::Value>( client, "https://api.twitter.com/1.1/guest/activate.json", headers, Method::POST, None, ) .await?; response .get("guest_token") .and_then(|token| token.as_str()) .map(String::from) .ok_or_else(|| crate::error::TwitterError::Auth("Failed to get guest token".into())) } pub async fn request_multipart_api<T>( client: &Client, url: &str, headers: HeaderMap, form: Form, ) -> Result<(T, HeaderMap)> where T: DeserializeOwned, { let request = client .request(Method::POST, url) .headers(headers) .multipart(form); let response = request.send().await?; if response.status().is_success() { let headers = response.headers().clone(); let text = response.text().await?; let parsed: T = serde_json::from_str(&text)?; Ok((parsed, headers)) } else { Err(crate::error::TwitterError::Api(format!( "Request failed with status: {}", response.status() ))) } } pub async fn request_form_api<T>( client: &Client, url: &str, headers: HeaderMap, form_data: Vec<(String, String)>, ) -> Result<(T, HeaderMap)> where T: DeserializeOwned, { let request = client .request(Method::POST, url) .headers(headers) .form(&form_data); let response = request.send().await?; if response.status().is_success() { let headers = response.headers().clone(); let text = response.text().await?; let parsed: T = serde_json::from_str(&text)?; Ok((parsed, headers)) } else { Err(crate::error::TwitterError::Api(format!( "Request failed with status: {}", response.status() ))) } }
```

# plugins/twitter/src/auth/config.rs

```rs
pub struct AuthConfig { pub username: Option<String>, pub password: Option<String>, pub email: Option<String>, pub bearer_token: String, pub two_factor_secret: Option<String>, } impl AuthConfig { pub fn new(bearer_token: String) -> Self { Self { username: None, password: None, email: None, bearer_token, two_factor_secret: None, } } pub fn with_credentials( mut self, username: String, password: String, email: Option<String>, ) -> Self { self.username = Some(username); self.password = Some(password); self.email = email; self } }
```

# plugins/twitter/src/auth/mod.rs

```rs
pub mod config; pub mod user_auth;
```

# plugins/twitter/src/auth/user_auth.rs

```rs
use crate::api::requests::request_api; use crate::error::{Result, TwitterError}; use async_trait::async_trait; use chrono::{DateTime, Utc}; use cookie::CookieJar; use reqwest::header::{HeaderMap, HeaderValue}; use reqwest::Client; use serde::{Deserialize, Serialize}; use serde_json::json; use std::any::Any; use std::fs::{File, OpenOptions}; use std::io::{Read, Write}; use std::path::Path; use std::sync::Arc; use tokio::sync::Mutex; use totp_rs::{Algorithm, TOTP}; use tracing; #[derive(Debug)] enum SubtaskType { LoginJsInstrumentation, LoginEnterUserIdentifier, LoginEnterPassword, LoginAcid, AccountDuplicationCheck, LoginTwoFactorAuthChallenge, LoginEnterAlternateIdentifier, LoginSuccess, DenyLogin, Unknown(String), } impl From<&str> for SubtaskType { fn from(s: &str) -> Self { match s { "LoginJsInstrumentationSubtask" => Self::LoginJsInstrumentation, "LoginEnterUserIdentifierSSO" => Self::LoginEnterUserIdentifier, "LoginEnterPassword" => Self::LoginEnterPassword, "LoginAcid" => Self::LoginAcid, "AccountDuplicationCheck" => Self::AccountDuplicationCheck, "LoginTwoFactorAuthChallenge" => Self::LoginTwoFactorAuthChallenge, "LoginEnterAlternateIdentifierSubtask" => Self::LoginEnterAlternateIdentifier, "LoginSuccessSubtask" => Self::LoginSuccess, "DenyLoginSubtask" => Self::DenyLogin, other => Self::Unknown(other.to_string()), } } } #[async_trait] pub trait TwitterAuth: Send + Sync + Any { async fn install_headers(&self, headers: &mut HeaderMap) -> Result<()>; async fn get_cookies(&self) -> Result<Vec<cookie::Cookie<'_>>>; fn delete_token(&mut self); fn as_any(&self) -> &dyn Any; } #[derive(Debug, Serialize)] struct FlowInitRequest { flow_name: String, input_flow_data: serde_json::Value, } #[derive(Debug, Serialize)] struct FlowTaskRequest { flow_token: String, subtask_inputs: Vec<serde_json::Value>, } #[derive(Debug, Deserialize)] struct FlowResponse { flow_token: String, subtasks: Option<Vec<Subtask>>, } #[derive(Debug, Deserialize)] struct Subtask { subtask_id: String, } #[derive(Clone)] pub struct TwitterUserAuth { bearer_token: String, guest_token: Option<String>, cookie_jar: Arc<Mutex<CookieJar>>, created_at: Option<DateTime<Utc>>, } impl TwitterUserAuth { pub async fn new(bearer_token: String) -> Result<Self> { Ok(Self { bearer_token, guest_token: None, cookie_jar: Arc::new(Mutex::new(CookieJar::new())), created_at: None, }) } async fn init_login(&mut self, client: &Client) -> Result<FlowResponse> { self.update_guest_token(client).await?; let init_request = FlowInitRequest { flow_name: "login".to_string(), input_flow_data: json!({ "flow_context": { "debug_overrides": {}, "start_location": { "location": "splash_screen" } } }), }; let mut headers = HeaderMap::new(); self.install_headers(&mut headers).await?; let (response, _) = request_api( client, "https://api.twitter.com/1.1/onboarding/task.json", headers, reqwest::Method::POST, Some(json!(init_request)), ) .await?; Ok(response) } async fn execute_flow_task( &self, client: &Client, request: FlowTaskRequest, ) -> Result<FlowResponse> { let mut headers = HeaderMap::new(); self.install_headers(&mut headers).await?; let (flow_response, raw_response) = request_api::<FlowResponse>( client, "https://api.twitter.com/1.1/onboarding/task.json", headers, reqwest::Method::POST, Some(json!(request)), ) .await?; let mut cookie_jar = self.cookie_jar.lock().await; for cookie_header in raw_response.get_all("set-cookie") { if let Ok(cookie_str) = cookie_header.to_str() { if let Ok(cookie) = cookie::Cookie::parse(cookie_str) { cookie_jar.add(cookie.into_owned()); } } } if let Some(subtasks) = &flow_response.subtasks { if subtasks.iter().any(|s| s.subtask_id == "DenyLoginSubtask") { return Err(TwitterError::Auth("Login denied".into())); } } Ok(flow_response) } pub async fn login( &mut self, client: &Client, username: &str, password: &str, email: Option<&str>, two_factor_secret: Option<&str>, ) -> Result<()> { let mut flow_response = self.init_login(client).await?; let mut flow_token = flow_response.flow_token; while let Some(subtasks) = &flow_response.subtasks { if let Some(subtask) = subtasks.first() { flow_response = match SubtaskType::from(subtask.subtask_id.as_str()) { SubtaskType::LoginJsInstrumentation => { self.handle_js_instrumentation_subtask(client, flow_token) .await? } SubtaskType::LoginEnterUserIdentifier => { self.handle_username_input(client, flow_token, username) .await? } SubtaskType::LoginEnterPassword => { self.handle_password_input(client, flow_token, password) .await? } SubtaskType::LoginAcid => { if let Some(email_str) = email { self.handle_email_verification(client, flow_token, email_str) .await? } else { return Err(TwitterError::Auth( "Email required for verification".into(), )); } } SubtaskType::AccountDuplicationCheck => { self.handle_account_duplication_check(client, flow_token) .await? } SubtaskType::LoginTwoFactorAuthChallenge => { if let Some(secret) = two_factor_secret { self.handle_two_factor_auth(client, flow_token, secret) .await? } else { return Err(TwitterError::Auth( "Two factor authentication required".into(), )); } } SubtaskType::LoginEnterAlternateIdentifier => { if let Some(email_str) = email { self.handle_alternate_identifier(client, flow_token, email_str) .await? } else { return Err(TwitterError::Auth( "Email required for alternate identifier".into(), )); } } SubtaskType::LoginSuccess => { self.handle_success_subtask(client, flow_token).await? } SubtaskType::DenyLogin => { return Err(TwitterError::Auth("Login denied".into())); } SubtaskType::Unknown(id) => { return Err(TwitterError::Auth(format!("Unhandled subtask: {}", id))); } }; flow_token = flow_response.flow_token; } else { break; } } Ok(()) } async fn handle_js_instrumentation_subtask( &self, client: &Client, flow_token: String, ) -> Result<FlowResponse> { let request = FlowTaskRequest { flow_token, subtask_inputs: vec![json!({ "subtask_id": "LoginJsInstrumentationSubtask", "js_instrumentation": { "response": "{}", "link": "next_link" } })], }; self.execute_flow_task(client, request).await } async fn handle_username_input( &self, client: &Client, flow_token: String, username: &str, ) -> Result<FlowResponse> { let request = FlowTaskRequest { flow_token, subtask_inputs: vec![json!({ "subtask_id": "LoginEnterUserIdentifierSSO", "settings_list": { "setting_responses": [ { "key": "user_identifier", "response_data": { "text_data": { "result": username } } } ], "link": "next_link" } })], }; self.execute_flow_task(client, request).await } async fn handle_password_input( &self, client: &Client, flow_token: String, password: &str, ) -> Result<FlowResponse> { let request = FlowTaskRequest { flow_token, subtask_inputs: vec![json!({ "subtask_id": "LoginEnterPassword", "enter_password": { "password": password, "link": "next_link" } })], }; self.execute_flow_task(client, request).await } async fn handle_email_verification( &self, client: &Client, flow_token: String, email: &str, ) -> Result<FlowResponse> { let request = FlowTaskRequest { flow_token, subtask_inputs: vec![json!({ "subtask_id": "LoginAcid", "enter_text": { "text": email, "link": "next_link" } })], }; self.execute_flow_task(client, request).await } async fn handle_account_duplication_check( &self, client: &Client, flow_token: String, ) -> Result<FlowResponse> { let request = FlowTaskRequest { flow_token, subtask_inputs: vec![json!({ "subtask_id": "AccountDuplicationCheck", "check_logged_in_account": { "link": "AccountDuplicationCheck_false" } })], }; self.execute_flow_task(client, request).await } async fn handle_two_factor_auth( &self, client: &Client, flow_token: String, secret: &str, ) -> Result<FlowResponse> { let totp = TOTP::new(Algorithm::SHA1, 6, 1, 30, secret.as_bytes().to_vec()) .map_err(|e| TwitterError::Auth(format!("Failed to create TOTP: {}", e)))?; let code = totp .generate_current() .map_err(|e| TwitterError::Auth(format!("Failed to generate TOTP code: {}", e)))?; let request = FlowTaskRequest { flow_token, subtask_inputs: vec![json!({ "subtask_id": "LoginTwoFactorAuthChallenge", "enter_text": { "text": code, "link": "next_link" } })], }; self.execute_flow_task(client, request).await } async fn handle_alternate_identifier( &self, client: &Client, flow_token: String, email: &str, ) -> Result<FlowResponse> { let request = FlowTaskRequest { flow_token, subtask_inputs: vec![json!({ "subtask_id": "LoginEnterAlternateIdentifierSubtask", "enter_text": { "text": email, "link": "next_link" } })], }; self.execute_flow_task(client, request).await } async fn handle_success_subtask( &self, client: &Client, flow_token: String, ) -> Result<FlowResponse> { let request = FlowTaskRequest { flow_token, subtask_inputs: vec![], }; self.execute_flow_task(client, request).await } async fn update_guest_token(&mut self, client: &Client) -> Result<()> { let url = "https://api.twitter.com/1.1/guest/activate.json"; let mut headers = HeaderMap::new(); headers.insert( "Authorization", HeaderValue::from_str(&format!("Bearer {}", self.bearer_token)) .map_err(|e| TwitterError::Auth(e.to_string()))?, ); let (response, _) = request_api::<serde_json::Value>(client, url, headers, reqwest::Method::POST, None) .await?; let guest_token = response .get("guest_token") .and_then(|token| token.as_str()) .ok_or_else(|| TwitterError::Auth("Failed to get guest token".into()))?; self.guest_token = Some(guest_token.to_string()); self.created_at = Some(Utc::now()); Ok(()) } pub async fn update_cookies(&self, response: &reqwest::Response) -> Result<()> { tracing::trace!("Updating cookies - attempting to lock"); let mut cookie_jar = self.cookie_jar.lock().await; for cookie_header in response.headers().get_all("set-cookie") { if let Ok(cookie_str) = cookie_header.to_str() { if let Ok(cookie) = cookie::Cookie::parse(cookie_str) { tracing::trace!(?cookie, "Adding cookie"); cookie_jar.add(cookie.into_owned()); } } } Ok(()) } pub async fn save_cookies_to_file(&self, file_path: &str) -> Result<()> { tracing::trace!("Saving cookies - attempting to lock"); let cookie_jar = self.cookie_jar.lock().await; let cookies: Vec<_> = cookie_jar.iter().collect(); let cookie_data: Vec<(String, String)> = cookies .iter() .map(|cookie| (cookie.name().to_string(), cookie.value().to_string())) .collect(); let json = serde_json::to_string_pretty(&cookie_data) .map_err(|e| TwitterError::Cookie(format!("Failed to serialize cookies: {}", e)))?; let mut file = OpenOptions::new() .write(true) .create(true) .truncate(true) .open(file_path) .map_err(|e| TwitterError::Cookie(format!("Failed to open cookie file: {}", e)))?; file.write_all(json.as_bytes()) .map_err(|e| TwitterError::Cookie(format!("Failed to write cookies: {}", e)))?; Ok(()) } pub async fn load_cookies_from_file(&mut self, file_path: &str) -> Result<()> { tracing::trace!("Loading cookies - attempting to lock"); if !Path::new(file_path).exists() { return Err(TwitterError::Cookie("Cookie file does not exist".into())); } let mut file = File::open(file_path) .map_err(|e| TwitterError::Cookie(format!("Failed to open cookie file: {}", e)))?; let mut contents = String::new(); file.read_to_string(&mut contents) .map_err(|e| TwitterError::Cookie(format!("Failed to read cookie file: {}", e)))?; let cookie_data: Vec<(String, String)> = serde_json::from_str(&contents) .map_err(|e| TwitterError::Cookie(format!("Failed to parse cookie file: {}", e)))?; tracing::trace!(?cookie_data, "Loaded cookie data"); let mut cookie_jar = self.cookie_jar.lock().await; *cookie_jar = CookieJar::new(); for (name, value) in cookie_data { let cookie = cookie::Cookie::build(name, value) .path("/") .domain("twitter.com") .secure(true) .http_only(true) .finish(); cookie_jar.add(cookie.into_owned()); } let mut headers = HeaderMap::new(); self.install_headers(&mut headers).await?; Ok(()) } pub async fn get_cookie_string(&self) -> Result<String> { let cookie_jar = self.cookie_jar.lock().await; let cookies: Vec<_> = cookie_jar.iter().collect(); let cookie_string = cookies .iter() .map(|c| format!("{}={}", c.name(), c.value())) .collect::<Vec<_>>() .join("; "); Ok(cookie_string) } pub async fn set_cookies(&mut self, json_str: &str) -> Result<()> { let cookie_data: Vec<(String, String)> = serde_json::from_str(json_str) .map_err(|e| TwitterError::Cookie(format!("Failed to parse cookie JSON: {}", e)))?; let mut cookie_jar = self.cookie_jar.lock().await; *cookie_jar = CookieJar::new(); for (name, value) in cookie_data { let cookie = cookie::Cookie::build(name, value) .path("/") .domain("twitter.com") .secure(true) .http_only(true) .finish(); cookie_jar.add(cookie.into_owned()); } std::mem::drop(cookie_jar); let mut headers = HeaderMap::new(); self.install_headers(&mut headers).await?; Ok(()) } pub async fn set_from_cookie_string(&mut self, cookie_string: &str) -> Result<()> { let mut cookie_jar = self.cookie_jar.lock().await; *cookie_jar = CookieJar::new(); for cookie_str in cookie_string.split(';') { let cookie_str = cookie_str.trim(); if let Ok(cookie) = cookie::Cookie::parse(cookie_str) { let cookie = cookie::Cookie::build(cookie.name().to_string(), cookie.value().to_string()) .path("/") .domain("twitter.com") .secure(true) .http_only(true) .finish(); cookie_jar.add(cookie.into_owned()); } } let has_essential_cookies = cookie_jar.iter().any(|c| c.name() == "ct0") && cookie_jar.iter().any(|c| c.name() == "auth_token"); if !has_essential_cookies { return Err(TwitterError::Cookie( "Missing essential cookies (ct0 or auth_token)".into(), )); } Ok(()) } pub async fn is_logged_in(&self, client: &Client) -> Result<bool> { let mut headers = HeaderMap::new(); self.install_headers(&mut headers).await?; let (response, _) = request_api::<serde_json::Value>( client, "https://api.twitter.com/1.1/account/verify_credentials.json", headers, reqwest::Method::GET, None, ) .await?; if let Some(errors) = response.get("errors") { if let Some(errors_array) = errors.as_array() { if !errors_array.is_empty() { let error_msg = errors_array .first() .and_then(|e| e.get("message")) .and_then(|m| m.as_str()) .unwrap_or("Unknown error"); return Err(TwitterError::Auth(error_msg.to_string())); } } } Ok(true) } pub async fn install_headers(&self, headers: &mut HeaderMap) -> Result<()> { let cookie_jar = self.cookie_jar.lock().await; let cookies: Vec<_> = cookie_jar.iter().collect(); if !cookies.is_empty() { let cookie_header = cookies .iter() .map(|c| format!("{}={}", c.name(), c.value())) .collect::<Vec<_>>() .join("; "); headers.insert( "Cookie", HeaderValue::from_str(&cookie_header) .map_err(|e| TwitterError::Auth(e.to_string()))?, ); if let Some(csrf_cookie) = cookies.iter().find(|c| c.name() == "ct0") { headers.insert( "x-csrf-token", HeaderValue::from_str(csrf_cookie.value()) .map_err(|e| TwitterError::Auth(e.to_string()))?, ); } } headers.insert( "Authorization", HeaderValue::from_str(&format!("Bearer {}", self.bearer_token)) .map_err(|e| TwitterError::Auth(e.to_string()))?, ); if let Some(token) = &self.guest_token { headers.insert( "x-guest-token", HeaderValue::from_str(token).map_err(|e| TwitterError::Auth(e.to_string()))?, ); } headers.insert("x-twitter-active-user", HeaderValue::from_static("yes")); headers.insert("x-twitter-client-language", HeaderValue::from_static("en")); headers.insert( "x-twitter-auth-type", HeaderValue::from_static("OAuth2Client"), ); Ok(()) } pub async fn get_cookies(&self) -> Result<Vec<cookie::Cookie<'_>>> { let jar = self.cookie_jar.lock().await; Ok(jar.iter().map(|c| c.to_owned()).collect()) } pub fn delete_token(&mut self) { self.guest_token = None; self.created_at = None; } pub fn as_any(&self) -> &dyn Any { self } }
```

# plugins/twitter/src/constants.rs

```rs
pub const BEARER_TOKEN: &str = "AAAAAAAAAAAAAAAAAAAAAFQODgEAAAAAVHTp76lzh3rFzcHbmHVvQxYYpTw%3DckAlMINMjmCwxUcaXbAN4XqJVdgMJaHqNOFgPMK0zN1qLqLQCF";
```

# plugins/twitter/src/error.rs

```rs
use serde::Deserialize; use std::env::VarError; use thiserror::Error; #[derive(Debug, Error, Deserialize)] pub enum TwitterError { #[error("API error: {0}")] Api(String), #[error("Authentication error: {0}")] Auth(String), #[error("Network error: {0}")] #[serde(skip)] Network(#[from] reqwest::Error), #[error("Rate limit exceeded")] RateLimit, #[error("Invalid response format: {0}")] InvalidResponse(String), #[error("Missing environment variable: {0}")] EnvVar(String), #[error("Cookie error: {0}")] Cookie(String), #[error("JSON error: {0}")] #[serde(skip)] Json(#[from] serde_json::Error), #[error("IO error: {0}")] #[serde(skip)] Io(#[from] std::io::Error), } impl From<VarError> for TwitterError { fn from(err: VarError) -> Self { TwitterError::EnvVar(err.to_string()) } } pub type Result<T> = std::result::Result<T, TwitterError>;
```

# plugins/twitter/src/lib.rs

```rs
pub mod api; pub mod auth; pub mod constants; pub mod error; pub mod messages; pub mod models; pub mod profile; pub mod relationships; pub mod scraper; pub mod search; pub mod timeline; pub mod tweets;
```

# plugins/twitter/src/messages.rs

```rs
use crate::api::client::TwitterClient; use crate::error::{Result, TwitterError}; use reqwest::header::HeaderMap; use reqwest::Method; use serde::{Deserialize, Serialize}; use serde_json::{json, Value}; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct DirectMessage { pub id: String, pub text: String, pub sender_id: String, pub recipient_id: String, pub created_at: String, pub media_urls: Option<Vec<String>>, pub sender_screen_name: Option<String>, pub recipient_screen_name: Option<String>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct DirectMessageConversation { pub conversation_id: String, pub messages: Vec<DirectMessage>, pub participants: Vec<Participant>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct Participant { pub id: String, pub screen_name: String, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct DirectMessagesResponse { pub conversations: Vec<DirectMessageConversation>, pub users: Vec<TwitterUser>, pub cursor: Option<String>, pub last_seen_event_id: Option<String>, pub trusted_last_seen_event_id: Option<String>, pub untrusted_last_seen_event_id: Option<String>, pub inbox_timelines: Option<InboxTimelines>, pub user_id: String, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct InboxTimelines { pub trusted: Option<TimelineStatus>, pub untrusted: Option<TimelineStatus>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TimelineStatus { pub status: String, pub min_entry_id: Option<String>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TwitterUser { pub id: String, pub screen_name: String, pub name: String, pub profile_image_url: String, pub description: Option<String>, pub verified: Option<bool>, pub protected: Option<bool>, pub followers_count: Option<i32>, pub friends_count: Option<i32>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct DirectMessageEvent { pub id: String, pub type_: String, // Using type_ since 'type' is a keyword in Rust pub message_create: MessageCreate, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MessageCreate { pub sender_id: String, pub target: MessageTarget, pub message_data: MessageData, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MessageTarget { pub recipient_id: String, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MessageData { pub text: String, pub created_at: String, pub entities: Option<MessageEntities>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MessageEntities { pub urls: Option<Vec<UrlEntity>>, pub media: Option<Vec<MediaEntity>>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct UrlEntity { pub url: String, pub expanded_url: String, pub display_url: String, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MediaEntity { pub url: String, #[serde(rename = "type")] pub media_type: String, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct SendDirectMessageResponse { pub entries: Vec<MessageEntry>, pub users: std::collections::HashMap<String, TwitterUser>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MessageEntry { pub message: MessageInfo, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MessageInfo { pub id: String, pub time: String, pub affects_sort: bool, pub conversation_id: String, pub message_data: MessageData, } pub async fn get_direct_message_conversations( client: &TwitterClient, screen_name: &str, cursor: Option<&str>, ) -> Result<DirectMessagesResponse> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let message_list_url = "https://x.com/i/api/1.1/dm/inbox_initial_state.json"; let url = if let Some(cursor_val) = cursor { format!("{}?cursor={}", message_list_url, cursor_val) } else { message_list_url.to_string() }; let (data, _) = crate::api::requests::request_api::<Value>( &client.client, &url, headers, Method::GET, None, ) .await?; let user_id = crate::profile::get_user_id_by_screen_name(client, screen_name).await?; parse_direct_message_conversations(&data, &user_id) } pub async fn send_direct_message( client: &TwitterClient, conversation_id: &str, text: &str, ) -> Result<Value> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let message_dm_url = "https://x.com/i/api/1.1/dm/new2.json"; let payload = json!({ "conversation_id": conversation_id, "recipient_ids": false, "text": text, "cards_platform": "Web-12", "include_cards": 1, "include_quote_count": true, "dm_users": false, }); let (response, _) = crate::api::requests::request_api::<Value>( &client.client, message_dm_url, headers, Method::POST, Some(payload), ) .await?; Ok(response) } fn parse_direct_message_conversations( data: &Value, user_id: &str, ) -> Result<DirectMessagesResponse> { let inbox_state = data .get("inbox_initial_state") .ok_or_else(|| TwitterError::Api("Missing inbox_initial_state".into()))?; let empty_map = serde_json::Map::new(); let conversations = inbox_state .get("conversations") .and_then(|v| v.as_object()) .unwrap_or(&empty_map); let empty_vec = Vec::new(); let entries = inbox_state .get("entries") .and_then(|v| v.as_array()) .unwrap_or(&empty_vec); let users = inbox_state .get("users") .and_then(|v| v.as_object()) .unwrap_or(&empty_map); // Parse users first let parsed_users = parse_users(users); // Group messages by conversation_id let messages_by_conversation = group_messages_by_conversation(entries); // Convert to DirectMessageConversation array let parsed_conversations = conversations .iter() .map(|(conv_id, conv)| { let messages = messages_by_conversation .get(conv_id) .map(|v| v.as_slice()) .unwrap_or(&[]); parse_conversation(conv_id, conv, messages, users) }) .collect(); Ok(DirectMessagesResponse { conversations: parsed_conversations, users: parsed_users, cursor: inbox_state .get("cursor") .and_then(|v| v.as_str()) .map(String::from), last_seen_event_id: inbox_state .get("last_seen_event_id") .and_then(|v| v.as_str()) .map(String::from), trusted_last_seen_event_id: inbox_state .get("trusted_last_seen_event_id") .and_then(|v| v.as_str()) .map(String::from), untrusted_last_seen_event_id: inbox_state .get("untrusted_last_seen_event_id") .and_then(|v| v.as_str()) .map(String::from), inbox_timelines: parse_inbox_timelines(inbox_state), user_id: user_id.to_string(), }) } fn parse_users(users: &serde_json::Map<String, Value>) -> Vec<TwitterUser> { users .values() .filter_map(|user| { Some(TwitterUser { id: user.get("id_str")?.as_str()?.to_string(), screen_name: user.get("screen_name")?.as_str()?.to_string(), name: user.get("name")?.as_str()?.to_string(), profile_image_url: user.get("profile_image_url_https")?.as_str()?.to_string(), description: user .get("description") .and_then(|v| v.as_str()) .map(String::from), verified: user.get("verified").and_then(|v| v.as_bool()), protected: user.get("protected").and_then(|v| v.as_bool()), followers_count: user .get("followers_count") .and_then(|v| v.as_i64()) .map(|v| v as i32), friends_count: user .get("friends_count") .and_then(|v| v.as_i64()) .map(|v| v as i32), }) }) .collect() } fn group_messages_by_conversation( entries: &[Value], ) -> std::collections::HashMap<String, Vec<&Value>> { let mut messages_by_conversation: std::collections::HashMap<String, Vec<&Value>> = std::collections::HashMap::new(); for entry in entries { if let Some(message) = entry.get("message") { if let Some(conv_id) = message.get("conversation_id").and_then(|v| v.as_str()) { messages_by_conversation .entry(conv_id.to_string()) .or_default() .push(message); } } } messages_by_conversation } fn parse_conversation( conv_id: &str, conv: &Value, messages: &[&Value], users: &serde_json::Map<String, Value>, ) -> DirectMessageConversation { let parsed_messages = parse_direct_messages(messages, users); let participants = conv .get("participants") .and_then(|p| p.as_array()) .map(|parts| { parts .iter() .filter_map(|p| { Some(Participant { id: p.get("user_id")?.as_str()?.to_string(), screen_name: users .get(p.get("user_id")?.as_str()?) .and_then(|u| u.get("screen_name")) .and_then(|s| s.as_str()) .unwrap_or(p.get("user_id")?.as_str()?) .to_string(), }) }) .collect() }) .unwrap_or_default(); DirectMessageConversation { conversation_id: conv_id.to_string(), messages: parsed_messages, participants, } } fn parse_direct_messages( messages: &[&Value], users: &serde_json::Map<String, Value>, ) -> Vec<DirectMessage> { messages .iter() .filter_map(|msg| { let message_data = msg.get("message_data")?; Some(DirectMessage { id: message_data.get("id")?.as_str()?.to_string(), text: message_data.get("text")?.as_str()?.to_string(), sender_id: message_data.get("sender_id")?.as_str()?.to_string(), recipient_id: message_data.get("recipient_id")?.as_str()?.to_string(), created_at: message_data.get("time")?.as_str()?.to_string(), media_urls: extract_media_urls(message_data), sender_screen_name: users .get(message_data.get("sender_id")?.as_str()?) .and_then(|u| u.get("screen_name")) .and_then(|s| s.as_str()) .map(String::from), recipient_screen_name: users .get(message_data.get("recipient_id")?.as_str()?) .and_then(|u| u.get("screen_name")) .and_then(|s| s.as_str()) .map(String::from), }) }) .collect() } fn extract_media_urls(message_data: &Value) -> Option<Vec<String>> { let mut urls = Vec::new(); if let Some(entities) = message_data.get("entities") { // Extract URLs if let Some(url_entities) = entities.get("urls").and_then(|u| u.as_array()) { for url in url_entities { if let Some(expanded_url) = url.get("expanded_url").and_then(|u| u.as_str()) { urls.push(expanded_url.to_string()); } } } // Extract media URLs if let Some(media_entities) = entities.get("media").and_then(|m| m.as_array()) { for media in media_entities { if let Some(media_url) = media .get("media_url_https") .or_else(|| media.get("media_url")) .and_then(|u| u.as_str()) { urls.push(media_url.to_string()); } } } } if urls.is_empty() { None } else { Some(urls) } } fn parse_inbox_timelines(inbox_state: &Value) -> Option<InboxTimelines> { inbox_state .get("inbox_timelines") .map(|timelines| InboxTimelines { trusted: parse_timeline_status(timelines.get("trusted")), untrusted: parse_timeline_status(timelines.get("untrusted")), }) } fn parse_timeline_status(timeline: Option<&Value>) -> Option<TimelineStatus> { timeline.map(|t| TimelineStatus { status: t .get("status") .and_then(|s| s.as_str()) .unwrap_or("") .to_string(), min_entry_id: t .get("min_entry_id") .and_then(|m| m.as_str()) .map(String::from), }) }
```

# plugins/twitter/src/models/mod.rs

```rs
pub mod tweets; pub use tweets::*; pub mod profile; pub use profile::Profile;
```

# plugins/twitter/src/models/profile.rs

```rs
use chrono::{DateTime, Utc}; use serde::{Deserialize, Serialize}; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct Profile { pub id: String, pub username: String, pub name: String, pub description: Option<String>, pub location: Option<String>, pub url: Option<String>, pub protected: bool, pub verified: bool, pub followers_count: i32, pub following_count: i32, pub tweets_count: i32, pub listed_count: i32, pub created_at: DateTime<Utc>, pub profile_image_url: Option<String>, pub profile_banner_url: Option<String>, pub pinned_tweet_id: Option<String>, pub is_blue_verified: Option<bool>, }
```

# plugins/twitter/src/models/tweets.rs

```rs
use chrono::{DateTime, Utc}; use serde::{Deserialize, Serialize}; #[derive(Debug, Clone, Deserialize, Serialize)] pub struct Tweet { pub ext_views: Option<i32>, pub created_at: Option<String>, pub bookmark_count: Option<i32>, pub conversation_id: Option<String>, pub hashtags: Vec<String>, pub html: Option<String>, pub id: Option<String>, pub in_reply_to_status: Option<Box<Tweet>>, pub in_reply_to_status_id: Option<String>, pub is_quoted: Option<bool>, pub is_pin: Option<bool>, pub is_reply: Option<bool>, pub is_retweet: Option<bool>, pub is_self_thread: Option<bool>, pub likes: Option<i32>, pub name: Option<String>, pub mentions: Vec<Mention>, pub permanent_url: Option<String>, pub photos: Vec<Photo>, pub place: Option<PlaceRaw>, pub quoted_status: Option<Box<Tweet>>, pub quoted_status_id: Option<String>, pub replies: Option<i32>, pub retweets: Option<i32>, pub retweeted_status: Option<Box<Tweet>>, pub retweeted_status_id: Option<String>, pub text: Option<String>, pub thread: Vec<Tweet>, pub time_parsed: Option<DateTime<Utc>>, pub timestamp: Option<i64>, pub urls: Vec<String>, pub user_id: Option<String>, pub username: Option<String>, pub videos: Vec<Video>, pub views: Option<i32>, pub sensitive_content: Option<bool>, pub poll: Option<PollV2>, pub quote_count: Option<i32>, pub reply_count: Option<i32>, pub retweet_count: Option<i32>, pub screen_name: Option<String>, pub thread_id: Option<String>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct Mention { pub id: String, pub username: Option<String>, pub name: Option<String>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct Photo { pub id: String, pub url: String, pub alt_text: Option<String>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct Video { pub id: String, pub preview: String, pub url: Option<String>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct PlaceRaw { pub id: Option<String>, pub place_type: Option<String>, pub name: Option<String>, pub full_name: Option<String>, pub country_code: Option<String>, pub country: Option<String>, pub bounding_box: Option<BoundingBox>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct BoundingBox { #[serde(rename = "type")] pub type_: Option<String>, pub coordinates: Option<Vec<Vec<Vec<f64>>>>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct PollV2 { pub id: Option<String>, pub end_datetime: Option<String>, pub voting_status: Option<String>, pub options: Vec<PollOption>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct PollOption { pub position: Option<i32>, pub label: String, pub votes: Option<i32>, }
```

# plugins/twitter/src/profile.rs

```rs
use crate::api::client::TwitterClient; use crate::api::requests::request_api; use crate::error::{Result, TwitterError}; use crate::models::Profile; use chrono::{DateTime, Utc}; use lazy_static::lazy_static; use reqwest::header::HeaderMap; use reqwest::Method; use serde::{Deserialize, Serialize}; use serde_json::json; use std::collections::HashMap; use std::sync::Mutex; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct UserProfile { pub id: String, pub id_str: String, pub name: String, pub screen_name: String, pub location: Option<String>, pub description: Option<String>, pub url: Option<String>, pub protected: bool, pub followers_count: i32, pub friends_count: i32, pub listed_count: i32, pub created_at: String, pub favourites_count: i32, pub verified: bool, pub statuses_count: i32, pub profile_image_url_https: String, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct LegacyUserRaw { pub created_at: Option<String>, pub description: Option<String>, pub entities: Option<UserEntitiesRaw>, pub favourites_count: Option<i32>, pub followers_count: Option<i32>, pub friends_count: Option<i32>, pub media_count: Option<i32>, pub statuses_count: Option<i32>, pub id_str: Option<String>, pub listed_count: Option<i32>, pub name: Option<String>, pub location: String, pub geo_enabled: Option<bool>, pub pinned_tweet_ids_str: Option<Vec<String>>, pub profile_background_color: Option<String>, pub profile_banner_url: Option<String>, pub profile_image_url_https: Option<String>, pub protected: Option<bool>, pub screen_name: Option<String>, pub verified: Option<bool>, pub has_custom_timelines: Option<bool>, pub has_extended_profile: Option<bool>, pub url: Option<String>, pub can_dm: Option<bool>, #[serde(rename = "userId")] pub user_id: Option<String>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct UserEntitiesRaw { pub url: Option<UserUrlEntity>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct UserUrlEntity { pub urls: Option<Vec<ExpandedUrl>>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct ExpandedUrl { pub expanded_url: Option<String>, } lazy_static! { static ref ID_CACHE: Mutex<HashMap<String, String>> = Mutex::new(HashMap::new()); } pub fn parse_profile(user: &LegacyUserRaw, is_blue_verified: Option<bool>) -> Profile { let mut profile = Profile { id: user.user_id.clone().unwrap_or_default(), username: user.screen_name.clone().unwrap_or_default(), name: user.name.clone().unwrap_or_default(), description: user.description.clone(), location: Some(user.location.clone()), url: user.url.clone(), protected: user.protected.unwrap_or(false), verified: user.verified.unwrap_or(false), followers_count: user.followers_count.unwrap_or(0), following_count: user.friends_count.unwrap_or(0), tweets_count: user.statuses_count.unwrap_or(0), listed_count: user.listed_count.unwrap_or(0), is_blue_verified: Some(is_blue_verified.unwrap_or(false)), created_at: user .created_at .as_ref() .and_then(|date_str| { DateTime::parse_from_str(date_str, "%a %b %d %H:%M:%S %z %Y") .ok() .map(|dt| dt.with_timezone(&Utc)) }) .unwrap_or_else(Utc::now), profile_image_url: user .profile_image_url_https .as_ref() .map(|url| url.replace("_normal", "")), profile_banner_url: user.profile_banner_url.clone(), pinned_tweet_id: user .pinned_tweet_ids_str .as_ref() .and_then(|ids| ids.first().cloned()), }; // Set website URL from entities using functional chaining user.entities .as_ref() .and_then(|entities| entities.url.as_ref()) .and_then(|url_entity| url_entity.urls.as_ref()) .and_then(|urls| urls.first()) .and_then(|first_url| first_url.expanded_url.as_ref()) .map(|expanded_url| profile.url = Some(expanded_url.clone())); profile } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct UserResults { pub result: UserResult, } #[derive(Debug, Clone, Serialize, Deserialize)] #[serde(tag = "__typename")] pub enum UserResult { User(UserData), UserUnavailable(UserUnavailable), } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct UserData { pub id: String, pub rest_id: String, pub affiliates_highlighted_label: Option<serde_json::Value>, pub has_graduated_access: bool, pub is_blue_verified: bool, pub profile_image_shape: String, pub legacy: LegacyUserRaw, pub smart_blocked_by: bool, pub smart_blocking: bool, pub legacy_extended_profile: Option<serde_json::Value>, pub is_profile_translatable: bool, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct UserUnavailable { pub reason: String, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct UserRaw { pub data: UserRawData, pub errors: Option<Vec<TwitterApiErrorRaw>>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct UserRawData { pub user: UserRawUser, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct UserRawUser { pub result: UserRawResult, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct UserRawResult { pub rest_id: Option<String>, pub is_blue_verified: Option<bool>, pub legacy: LegacyUserRaw, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TwitterApiErrorRaw { pub message: String, pub code: i32, } pub async fn get_profile(client: &TwitterClient, screen_name: &str) -> Result<Profile> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let variables = json!({ "screen_name": screen_name, "withSafetyModeUserFields": true }); let features = json!({ "hidden_profile_likes_enabled": false, "hidden_profile_subscriptions_enabled": false, "responsive_web_graphql_exclude_directive_enabled": true, "verified_phone_label_enabled": false, "subscriptions_verification_info_is_identity_verified_enabled": false, "subscriptions_verification_info_verified_since_enabled": true, "highlights_tweets_tab_ui_enabled": true, "creator_subscriptions_tweet_preview_api_enabled": true, "responsive_web_graphql_skip_user_profile_image_extensions_enabled": false, "responsive_web_graphql_timeline_navigation_enabled": true }); let field_toggles = json!({ "withAuxiliaryUserLabels": false }); let (response, _) = request_api::<UserRaw>( &client.client, "https://twitter.com/i/api/graphql/G3KGOASz96M-Qu0nwmGXNg/UserByScreenName", headers, Method::GET, Some(json!({ "variables": variables, "features": features, "fieldToggles": field_toggles })), ) .await?; if let Some(errors) = response.errors { if !errors.is_empty() { return Err(TwitterError::Api(errors[0].message.clone())); } } let user_raw_result = &response.data.user.result; let mut legacy = user_raw_result.legacy.clone(); let rest_id = user_raw_result.rest_id.clone(); let is_blue_verified = user_raw_result.is_blue_verified; legacy.user_id = rest_id; if legacy.screen_name.is_none() || legacy.screen_name.as_ref().unwrap().is_empty() { return Err(TwitterError::Api(format!( "Either {} does not exist or is private.", screen_name ))); } Ok(parse_profile(&legacy, is_blue_verified)) } pub async fn get_screen_name_by_user_id(client: &TwitterClient, user_id: &str) -> Result<String> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let variables = json!({ "userId": user_id, "withSafetyModeUserFields": true }); let features = json!({ "hidden_profile_subscriptions_enabled": true, "rweb_tipjar_consumption_enabled": true, "responsive_web_graphql_exclude_directive_enabled": true, "verified_phone_label_enabled": false, "highlights_tweets_tab_ui_enabled": true, "responsive_web_twitter_article_notes_tab_enabled": true, "subscriptions_feature_can_gift_premium": false, "creator_subscriptions_tweet_preview_api_enabled": true, "responsive_web_graphql_skip_user_profile_image_extensions_enabled": false, "responsive_web_graphql_timeline_navigation_enabled": true }); let (response, _) = request_api::<UserRaw>( &client.client, "https://twitter.com/i/api/graphql/xf3jd90KKBCUxdlI_tNHZw/UserByRestId", headers, Method::GET, Some(json!({ "variables": variables, "features": features })), ) .await?; if let Some(errors) = response.errors { if !errors.is_empty() { return Err(TwitterError::Api(errors[0].message.clone())); } } if let Some(user) = response.data.user.result.legacy.screen_name { Ok(user) } else { Err(TwitterError::Api(format!( "Either user with ID {} does not exist or is private.", user_id ))) } } pub async fn get_user_id_by_screen_name( client: &TwitterClient, screen_name: &str, ) -> Result<String> { if let Some(cached_id) = ID_CACHE.lock().unwrap().get(screen_name) { return Ok(cached_id.clone()); } let profile = get_profile(client, screen_name).await?; if let Some(user_id) = Some(profile.id) { ID_CACHE .lock() .unwrap() .insert(screen_name.to_string(), user_id.clone()); Ok(user_id) } else { Err(TwitterError::Api("User ID is undefined".into())) } }
```

# plugins/twitter/src/relationships.rs

```rs
use crate::api::client::TwitterClient; use crate::api::requests::request_api; use crate::api::requests::request_form_api; use crate::error::{Result, TwitterError}; use crate::models::Profile; use crate::timeline::v1::QueryProfilesResponse; use chrono::{DateTime, Utc}; use reqwest::Method; use serde::Deserialize; use serde_json::{json, Value}; #[derive(Debug, Deserialize)] pub struct RelationshipResponse { pub data: Option<RelationshipData>, #[serde(skip)] pub errors: Option<Vec<TwitterError>>, } #[derive(Debug, Deserialize)] pub struct RelationshipData { pub user: UserRelationships, } #[derive(Debug, Deserialize)] pub struct UserRelationships { pub result: UserResult, } #[derive(Debug, Deserialize)] pub struct UserResult { pub timeline: Timeline, pub rest_id: Option<String>, } #[derive(Debug, Deserialize)] pub struct Timeline { pub timeline: TimelineData, } #[derive(Debug, Deserialize)] pub struct TimelineData { pub instructions: Vec<TimelineInstruction>, } #[derive(Debug, Deserialize)] #[serde(tag = "type")] pub enum TimelineInstruction { #[serde(rename = "TimelineAddEntries")] AddEntries { entries: Vec<TimelineEntry> }, #[serde(rename = "TimelineReplaceEntry")] ReplaceEntry { entry: TimelineEntry }, } #[derive(Debug, Deserialize)] pub struct TimelineEntry { pub content: EntryContent, pub entry_id: String, pub sort_index: String, } #[derive(Debug, Deserialize)] pub struct EntryContent { #[serde(rename = "itemContent")] pub item_content: Option<ItemContent>, pub cursor: Option<CursorContent>, } #[derive(Debug, Deserialize)] pub struct ItemContent { #[serde(rename = "user_results")] pub user_results: Option<UserResults>, #[serde(rename = "userDisplayType")] pub user_display_type: Option<String>, } #[derive(Debug, Deserialize)] pub struct UserResults { pub result: UserResultData, } #[derive(Debug, Deserialize)] pub struct UserResultData { #[serde(rename = "typename")] pub type_name: Option<String>, #[serde(rename = "mediaColor")] pub media_color: Option<MediaColor>, pub id: Option<String>, pub rest_id: Option<String>, pub affiliates_highlighted_label: Option<Value>, pub has_graduated_access: Option<bool>, pub is_blue_verified: Option<bool>, pub profile_image_shape: Option<String>, pub legacy: Option<UserLegacy>, pub professional: Option<Professional>, } #[derive(Debug, Deserialize)] pub struct MediaColor { pub r: Option<ColorPalette>, } #[derive(Debug, Deserialize)] pub struct ColorPalette { pub ok: Option<Value>, } #[derive(Debug, Deserialize)] pub struct UserLegacy { pub following: Option<bool>, pub followed_by: Option<bool>, pub screen_name: Option<String>, pub name: Option<String>, pub description: Option<String>, pub location: Option<String>, pub url: Option<String>, pub protected: Option<bool>, pub verified: Option<bool>, pub followers_count: Option<i32>, pub friends_count: Option<i32>, pub statuses_count: Option<i32>, pub listed_count: Option<i32>, pub created_at: Option<String>, pub profile_image_url_https: Option<String>, pub profile_banner_url: Option<String>, pub pinned_tweet_ids_str: Option<String>, } #[derive(Debug, Deserialize)] pub struct Professional { pub rest_id: Option<String>, pub professional_type: Option<String>, pub category: Option<Vec<ProfessionalCategory>>, } #[derive(Debug, Deserialize)] pub struct ProfessionalCategory { pub id: i64, pub name: String, } #[derive(Debug, Deserialize)] pub struct CursorContent { pub value: String, pub cursor_type: Option<String>, } #[derive(Debug, Deserialize)] pub struct RelationshipTimeline { pub data: Option<RelationshipTimelineData>, pub errors: Option<Vec<TwitterError>>, } #[derive(Debug, Deserialize)] pub struct RelationshipTimelineData { pub user: UserData, } #[derive(Debug, Deserialize)] pub struct UserData { pub result: RelationshipUserResult, } #[derive(Debug, Deserialize)] pub struct RelationshipUserResult { pub timeline: Timeline, } #[derive(Debug, Deserialize)] pub struct InnerTimeline { pub instructions: Vec<Instruction>, } #[derive(Debug, Deserialize)] #[serde(tag = "type")] pub enum Instruction { #[serde(rename = "TimelineAddEntries")] AddEntries { entries: Vec<RelationshipTimelineEntry>, }, #[serde(rename = "TimelineReplaceEntry")] ReplaceEntry { entry: RelationshipTimelineEntry }, } #[derive(Debug, Deserialize)] pub struct RelationshipTimelineEntry { pub content: EntryContent, pub entry_id: String, pub sort_index: String, } #[derive(Debug, Deserialize)] pub struct RelationshipTimelineContainer { pub timeline: InnerTimeline, } #[derive(Debug, Deserialize)] pub struct RelationshipTimelineWrapper { pub timeline: InnerTimeline, } pub async fn get_following( client: &TwitterClient, user_id: &str, count: i32, cursor: Option<String>, ) -> Result<(Vec<Profile>, Option<String>)> { let response = fetch_profile_following(client, user_id, count, cursor).await?; Ok((response.profiles, response.next)) } pub async fn get_followers( client: &TwitterClient, user_id: &str, count: i32, cursor: Option<String>, ) -> Result<(Vec<Profile>, Option<String>)> { let response = fetch_profile_following(client, user_id, count, cursor).await?; Ok((response.profiles, response.next)) } pub async fn fetch_profile_following( client: &TwitterClient, user_id: &str, max_profiles: i32, cursor: Option<String>, ) -> Result<QueryProfilesResponse> { let timeline = get_following_timeline(client, user_id, max_profiles, cursor).await?; Ok(parse_relationship_timeline(&timeline)) } async fn get_following_timeline( client: &TwitterClient, user_id: &str, max_items: i32, cursor: Option<String>, ) -> Result<RelationshipTimeline> { let count = if max_items > 50 { 50 } else { max_items }; let mut variables = json!({ "userId": user_id, "count": count, "includePromotedContent": false, }); if let Some(cursor_val) = cursor { if !cursor_val.is_empty() { variables["cursor"] = json!(cursor_val); } } let features = json!({ "responsive_web_twitter_article_tweet_consumption_enabled": false, "tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled": true, "longform_notetweets_inline_media_enabled": true, "responsive_web_media_download_video_enabled": false, }); let url = format!( "https://twitter.com/i/api/graphql/iSicc7LrzWGBgDPL0tM_TQ/Following?variables={}&features={}", urlencoding::encode(&variables.to_string()), urlencoding::encode(&features.to_string()) ); let mut headers = reqwest::header::HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let (_data, _) = request_api::<RelationshipTimeline>(&client.client, &url, headers, Method::GET, None) .await?; Ok(_data) } fn parse_relationship_timeline(timeline: &RelationshipTimeline) -> QueryProfilesResponse { let mut profiles = Vec::new(); let mut next_cursor = None; let mut previous_cursor = None; if let Some(data) = &timeline.data { for instruction in &data.user.result.timeline.timeline.instructions { match instruction { TimelineInstruction::AddEntries { entries } => { for entry in entries { if let Some(item_content) = &entry.content.item_content { if let Some(user_results) = &item_content.user_results { if let Some(legacy) = &user_results.result.legacy { let profile = Profile { username: legacy.screen_name.clone().unwrap_or_default(), name: legacy.name.clone().unwrap_or_default(), id: user_results .result .rest_id .as_ref() .map(String::from) .unwrap_or_default(), description: legacy.description.clone(), location: legacy.location.clone(), url: legacy.url.clone(), protected: legacy.protected.unwrap_or_default(), verified: legacy.verified.unwrap_or_default(), followers_count: legacy.followers_count.unwrap_or_default(), following_count: legacy.friends_count.unwrap_or_default(), tweets_count: legacy.statuses_count.unwrap_or_default(), listed_count: legacy.listed_count.unwrap_or_default(), created_at: legacy .created_at .as_ref() .and_then(|date| { DateTime::parse_from_str( date, "%a %b %d %H:%M:%S %z %Y", ) .ok() .map(|dt| dt.with_timezone(&Utc)) }) .unwrap_or_default(), profile_image_url: legacy.profile_image_url_https.clone(), profile_banner_url: legacy.profile_banner_url.clone(), pinned_tweet_id: legacy.pinned_tweet_ids_str.clone(), is_blue_verified: Some( user_results.result.is_blue_verified.unwrap_or(false), ), }; profiles.push(profile); } } } else if let Some(cursor_content) = &entry.content.cursor { match cursor_content.cursor_type.as_deref() { Some("Bottom") => next_cursor = Some(cursor_content.value.clone()), Some("Top") => previous_cursor = Some(cursor_content.value.clone()), _ => {} } } } } TimelineInstruction::ReplaceEntry { entry } => { if let Some(cursor_content) = &entry.content.cursor { match cursor_content.cursor_type.as_deref() { Some("Bottom") => next_cursor = Some(cursor_content.value.clone()), Some("Top") => previous_cursor = Some(cursor_content.value.clone()), _ => {} } } } } } } QueryProfilesResponse { profiles, next: next_cursor, previous: previous_cursor, } } pub async fn follow_user(client: &TwitterClient, username: &str) -> Result<()> { let user_id = crate::profile::get_user_id_by_screen_name(client, username).await?; let url = "https://api.twitter.com/1.1/friendships/create.json"; let form = vec![ ( "include_profile_interstitial_type".to_string(), "1".to_string(), ), ("skip_status".to_string(), "true".to_string()), ("user_id".to_string(), user_id), ]; let mut headers = reqwest::header::HeaderMap::new(); client.auth.install_headers(&mut headers).await?; headers.insert( "Content-Type", "application/x-www-form-urlencoded".parse().unwrap(), ); headers.insert( "Referer", format!("https://twitter.com/{}", username).parse().unwrap(), ); headers.insert("X-Twitter-Active-User", "yes".parse().unwrap()); headers.insert("X-Twitter-Auth-Type", "OAuth2Session".parse().unwrap()); headers.insert("X-Twitter-Client-Language", "en".parse().unwrap()); let (_, _) = request_form_api::<Value>(&client.client, url, headers, form).await?; Ok(()) } pub async fn unfollow_user(client: &TwitterClient, username: &str) -> Result<()> { let user_id = crate::profile::get_user_id_by_screen_name(client, username).await?; let url = "https://api.twitter.com/1.1/friendships/destroy.json"; let form = vec![ ( "include_profile_interstitial_type".to_string(), "1".to_string(), ), ("skip_status".to_string(), "true".to_string()), ("user_id".to_string(), user_id), ]; let mut headers = reqwest::header::HeaderMap::new(); client.auth.install_headers(&mut headers).await?; headers.insert( "Content-Type", "application/x-www-form-urlencoded".parse().unwrap(), ); headers.insert( "Referer", format!("https://twitter.com/{}", username).parse().unwrap(), ); headers.insert("X-Twitter-Active-User", "yes".parse().unwrap()); headers.insert("X-Twitter-Auth-Type", "OAuth2Session".parse().unwrap()); headers.insert("X-Twitter-Client-Language", "en".parse().unwrap()); let (_, _) = request_form_api::<Value>(&client.client, url, headers, form).await?; Ok(()) }
```

# plugins/twitter/src/scraper.rs

```rs
use crate::api::client::TwitterClient; use crate::auth::user_auth::TwitterUserAuth; use crate::constants::BEARER_TOKEN; use crate::error::Result; use crate::error::TwitterError; use crate::messages::DirectMessagesResponse; use crate::models::{Profile, Tweet}; use crate::search::{fetch_search_tweets, SearchMode}; use crate::timeline::v1::{QueryProfilesResponse, QueryTweetsResponse}; use crate::timeline::v2::QueryTweetsResponse as V2QueryTweetsResponse; use serde_json::Value; pub struct Scraper { pub twitter_client: TwitterClient, } impl Scraper { pub async fn new() -> Result<Self> { let auth = TwitterUserAuth::new(BEARER_TOKEN.to_string()).await?; let twitter_client = TwitterClient::new(auth.clone())?; Ok(Self { twitter_client }) } pub async fn login( &mut self, username: String, password: String, email: Option<String>, two_factor_secret: Option<String>, ) -> Result<()> { if let Some(user_auth) = self .twitter_client .auth .as_any() .downcast_ref::<TwitterUserAuth>() { let mut auth = user_auth.clone(); auth.login( &self.twitter_client.client, &username, &password, email.as_deref(), two_factor_secret.as_deref(), ) .await?; self.twitter_client.auth = auth.clone(); //self.client = TwitterClient::new(Box::new(auth))?; Ok(()) } else { Err(TwitterError::Auth("Invalid auth type".into())) } } pub async fn get_profile(&self, username: &str) -> Result<crate::models::Profile> { crate::profile::get_profile(&self.twitter_client, username).await } pub async fn send_tweet( &self, text: &str, reply_to: Option<&str>, media_data: Option<Vec<(Vec<u8>, String)>>, ) -> Result<Value> { crate::tweets::create_tweet_request(&self.twitter_client, text, reply_to, media_data).await } pub async fn get_home_timeline( &self, count: i32, seen_tweet_ids: Vec<String>, ) -> Result<Vec<Value>> { crate::timeline::home::fetch_home_timeline(&self.twitter_client, count, seen_tweet_ids) .await } pub async fn save_cookies(&self, cookie_file: &str) -> Result<()> { if let Some(user_auth) = self .twitter_client .auth .as_any() .downcast_ref::<TwitterUserAuth>() { user_auth.save_cookies_to_file(cookie_file).await } else { Err(TwitterError::Auth("Invalid auth type".into())) } } pub async fn get_cookie_string(&self) -> Result<String> { if let Some(user_auth) = self .twitter_client .auth .as_any() .downcast_ref::<TwitterUserAuth>() { user_auth.get_cookie_string().await } else { Err(TwitterError::Auth("Invalid auth type".into())) } } pub async fn set_cookies(&mut self, json_str: &str) -> Result<()> { if let Some(user_auth) = self .twitter_client .auth .as_any() .downcast_ref::<TwitterUserAuth>() { let mut auth = user_auth.clone(); auth.set_cookies(json_str).await?; self.twitter_client.auth = auth.clone(); self.twitter_client = TwitterClient::new(auth)?; Ok(()) } else { Err(TwitterError::Auth("Invalid auth type".into())) } } pub async fn set_from_cookie_string(&mut self, cookie_string: &str) -> Result<()> { if let Some(user_auth) = self .twitter_client .auth .as_any() .downcast_ref::<TwitterUserAuth>() { let mut auth = user_auth.clone(); auth.set_from_cookie_string(cookie_string).await?; self.twitter_client.auth = auth.clone(); self.twitter_client = TwitterClient::new(auth)?; Ok(()) } else { Err(TwitterError::Auth("Invalid auth type".into())) } } pub async fn get_followers( &self, user_id: &str, count: i32, cursor: Option<String>, ) -> Result<(Vec<Profile>, Option<String>)> { crate::relationships::get_followers(&self.twitter_client, user_id, count, cursor).await } pub async fn get_following( &self, user_id: &str, count: i32, cursor: Option<String>, ) -> Result<(Vec<Profile>, Option<String>)> { crate::relationships::get_following(&self.twitter_client, user_id, count, cursor).await } pub async fn follow_user(&self, username: &str) -> Result<()> { crate::relationships::follow_user(&self.twitter_client, username).await } pub async fn unfollow_user(&self, username: &str) -> Result<()> { crate::relationships::unfollow_user(&self.twitter_client, username).await } pub async fn send_quote_tweet( &self, text: &str, quoted_tweet_id: &str, media_data: Option<Vec<(Vec<u8>, String)>>, ) -> Result<Value> { crate::tweets::create_quote_tweet(&self.twitter_client, text, quoted_tweet_id, media_data) .await } pub async fn fetch_tweets_and_replies( &self, username: &str, max_tweets: i32, cursor: Option<&str>, ) -> Result<V2QueryTweetsResponse> { crate::tweets::fetch_tweets_and_replies(&self.twitter_client, username, max_tweets, cursor) .await } pub async fn fetch_tweets_and_replies_by_user_id( &self, user_id: &str, max_tweets: i32, cursor: Option<&str>, ) -> Result<V2QueryTweetsResponse> { crate::tweets::fetch_tweets_and_replies_by_user_id( &self.twitter_client, user_id, max_tweets, cursor, ) .await } pub async fn fetch_list_tweets( &self, list_id: &str, max_tweets: i32, cursor: Option<&str>, ) -> Result<Value> { crate::tweets::fetch_list_tweets(&self.twitter_client, list_id, max_tweets, cursor).await } pub async fn like_tweet(&self, tweet_id: &str) -> Result<Value> { crate::tweets::like_tweet(&self.twitter_client, tweet_id).await } pub async fn retweet(&self, tweet_id: &str) -> Result<Value> { crate::tweets::retweet(&self.twitter_client, tweet_id).await } pub async fn create_long_tweet( &self, text: &str, reply_to: Option<&str>, media_ids: Option<Vec<String>>, ) -> Result<Value> { crate::tweets::create_long_tweet(&self.twitter_client, text, reply_to, media_ids).await } pub async fn get_tweet(&self, id: &str) -> Result<(Option<Tweet>, Option<Vec<Tweet>>)> { crate::tweets::get_tweet(&self.twitter_client, id).await } pub async fn search_tweets( &self, query: &str, max_tweets: i32, search_mode: SearchMode, cursor: Option<String>, ) -> Result<QueryTweetsResponse> { fetch_search_tweets(&self.twitter_client, query, max_tweets, search_mode, cursor).await } pub async fn search_profiles( &self, query: &str, max_profiles: i32, cursor: Option<String>, ) -> Result<QueryProfilesResponse> { crate::search::search_profiles(&self.twitter_client, query, max_profiles, cursor).await } pub async fn get_user_tweets( &self, user_id: &str, count: i32, cursor: Option<String>, ) -> Result<V2QueryTweetsResponse> { crate::tweets::fetch_user_tweets(&self.twitter_client, user_id, count, cursor.as_deref()) .await } pub async fn get_direct_message_conversations( &self, screen_name: &str, cursor: Option<&str>, ) -> Result<DirectMessagesResponse> { crate::messages::get_direct_message_conversations(&self.twitter_client, screen_name, cursor) .await } pub async fn send_direct_message(&self, conversation_id: &str, text: &str) -> Result<Value> { crate::messages::send_direct_message(&self.twitter_client, conversation_id, text).await } }
```

# plugins/twitter/src/search.rs

```rs
use crate::api::client::TwitterClient; use crate::api::requests::request_api; use crate::error::Result; use crate::timeline::search::{ parse_search_timeline_tweets, parse_search_timeline_users, SearchTimeline, }; use crate::timeline::v1::{QueryProfilesResponse, QueryTweetsResponse}; use reqwest::Method; use serde_json::json; #[derive(Debug, Clone, Copy)] pub enum SearchMode { Top, Latest, Photos, Videos, Users, } pub async fn fetch_search_tweets( client: &TwitterClient, query: &str, max_tweets: i32, search_mode: SearchMode, cursor: Option<String>, ) -> Result<QueryTweetsResponse> { let timeline = get_search_timeline(client, query, max_tweets, search_mode, cursor).await?; Ok(parse_search_timeline_tweets(&timeline)) } pub async fn search_profiles( client: &TwitterClient, query: &str, max_profiles: i32, cursor: Option<String>, ) -> Result<QueryProfilesResponse> { let timeline = get_search_timeline(client, query, max_profiles, SearchMode::Users, cursor).await?; Ok(parse_search_timeline_users(&timeline)) } async fn get_search_timeline( client: &TwitterClient, query: &str, max_items: i32, search_mode: SearchMode, _cursor: Option<String>, ) -> Result<SearchTimeline> { let max_items = if max_items > 50 { 50 } else { max_items }; let mut variables = json!({ "rawQuery": query, "count": max_items, "querySource": "typed_query", "product": "Top" }); // Set product based on search mode match search_mode { SearchMode::Latest => { variables["product"] = json!("Latest"); } SearchMode::Photos => { variables["product"] = json!("Photos"); } SearchMode::Videos => { variables["product"] = json!("Videos"); } SearchMode::Users => { variables["product"] = json!("People"); } _ => {} } let features = json!({ "longform_notetweets_inline_media_enabled": true, "responsive_web_enhance_cards_enabled": false, "responsive_web_media_download_video_enabled": false, "responsive_web_twitter_article_tweet_consumption_enabled": false, "tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled": true, "interactive_text_enabled": false, "responsive_web_text_conversations_enabled": false, "vibe_api_enabled": false, "rweb_lists_timeline_redesign_enabled": true, "responsive_web_graphql_exclude_directive_enabled": true, "verified_phone_label_enabled": false, "creator_subscriptions_tweet_preview_api_enabled": true, "responsive_web_graphql_timeline_navigation_enabled": true, "responsive_web_graphql_skip_user_profile_image_extensions_enabled": false, "tweetypie_unmention_optimization_enabled": true, "responsive_web_edit_tweet_api_enabled": true, "graphql_is_translatable_rweb_tweet_is_translatable_enabled": true, "view_counts_everywhere_api_enabled": true, "longform_notetweets_consumption_enabled": true, "tweet_awards_web_tipping_enabled": false, "freedom_of_speech_not_reach_fetch_enabled": true, "standardized_nudges_misinfo": true, "longform_notetweets_rich_text_read_enabled": true, "responsive_web_enhance_cards_enabled": false, "subscriptions_verification_info_enabled": true, "subscriptions_verification_info_reason_enabled": true, "subscriptions_verification_info_verified_since_enabled": true, "super_follow_badge_privacy_enabled": false, "super_follow_exclusive_tweet_notifications_enabled": false, "super_follow_tweet_api_enabled": false, "super_follow_user_api_enabled": false, "android_graphql_skip_api_media_color_palette": false, "creator_subscriptions_subscription_count_enabled": false, "blue_business_profile_image_shape_enabled": false, "unified_cards_ad_metadata_container_dynamic_card_content_query_enabled": false }); let field_toggles = json!({ "withArticleRichContentState": false }); let params = [ ("variables", serde_json::to_string(&variables)?), ("features", serde_json::to_string(&features)?), ("fieldToggles", serde_json::to_string(&field_toggles)?), ]; let query_string = params .iter() .map(|(k, v)| format!("{}={}", k, urlencoding::encode(v))) .collect::<Vec<_>>() .join("&"); let mut headers = reqwest::header::HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let url = format!( "https://api.twitter.com/graphql/gkjsKepM6gl_HmFWoWKfgg/SearchTimeline?{}", query_string ); let (response, _) = request_api::<SearchTimeline>(&client.client, &url, headers, Method::GET, None).await?; Ok(response) }
```

# plugins/twitter/src/timeline/home.rs

```rs
use crate::api::client::TwitterClient; use crate::api::requests::request_api; use crate::error::Result; use reqwest::header::HeaderMap; use reqwest::Method; use serde::Deserialize; use serde_json::Value; use urlencoding; #[derive(Debug, Deserialize)] pub struct HomeTimelineResponse { pub data: Option<HomeData>, } #[derive(Debug, Deserialize)] pub struct HomeData { pub home: Home, } #[derive(Debug, Deserialize)] pub struct Home { #[serde(rename = "home_timeline_urt")] pub home_timeline: HomeTimeline, } #[derive(Debug, Deserialize)] pub struct HomeTimeline { pub instructions: Vec<TimelineInstruction>, } #[derive(Debug, Deserialize)] #[serde(tag = "type")] pub enum TimelineInstruction { #[serde(rename = "TimelineAddEntries")] AddEntries { entries: Vec<TimelineEntry> }, // Add other variants as needed } #[derive(Debug, Deserialize)] pub struct TimelineEntry { pub content: EntryContent, } #[derive(Debug, Deserialize)] pub struct EntryContent { #[serde(rename = "itemContent")] pub item_content: Option<ItemContent>, } #[derive(Debug, Deserialize)] pub struct ItemContent { pub tweet_results: Option<TweetResults>, } #[derive(Debug, Deserialize)] pub struct TweetResults { pub result: Option<Value>, } pub async fn fetch_home_timeline( client: &TwitterClient, count: i32, seen_tweet_ids: Vec<String>, ) -> Result<Vec<Value>> { let variables = serde_json::json!({ "count": count, "includePromotedContent": true, "latestControlAvailable": true, "requestContext": "launch", "withCommunity": true, "seenTweetIds": seen_tweet_ids, }); let features = serde_json::json!({ "rweb_tipjar_consumption_enabled": true, "responsive_web_graphql_exclude_directive_enabled": true, "verified_phone_label_enabled": false, "creator_subscriptions_tweet_preview_api_enabled": true, "responsive_web_graphql_timeline_navigation_enabled": true, "responsive_web_graphql_skip_user_profile_image_extensions_enabled": false, "communities_web_enable_tweet_community_results_fetch": true, "c9s_tweet_anatomy_moderator_badge_enabled": true, "articles_preview_enabled": true, "responsive_web_edit_tweet_api_enabled": true, "graphql_is_translatable_rweb_tweet_is_translatable_enabled": true, "view_counts_everywhere_api_enabled": true, "longform_notetweets_consumption_enabled": true, "responsive_web_twitter_article_tweet_consumption_enabled": true, "tweet_awards_web_tipping_enabled": false, "creator_subscriptions_quote_tweet_preview_enabled": false, "freedom_of_speech_not_reach_fetch_enabled": true, "standardized_nudges_misinfo": true, "tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled": true, "rweb_video_timestamps_enabled": true, "longform_notetweets_rich_text_read_enabled": true, "longform_notetweets_inline_media_enabled": true, "responsive_web_enhance_cards_enabled": false, }); let url = format!( "https://x.com/i/api/graphql/HJFjzBgCs16TqxewQOeLNg/HomeTimeline?variables={}&features={}", urlencoding::encode(&variables.to_string()), urlencoding::encode(&features.to_string()) ); let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let (response, _) = request_api::<HomeTimelineResponse>(&client.client, &url, headers, Method::GET, None) .await?; let home = response .data .map(|data| data.home.home_timeline.instructions); let mut entries = Vec::new(); if let Some(instructions) = home { for instruction in instructions { match instruction { TimelineInstruction::AddEntries { entries: new_entries, } => { for entry in new_entries { if let Some(item_content) = entry.content.item_content { if let Some(tweet_results) = item_content.tweet_results { if let Some(result) = tweet_results.result { entries.push(result); } } } } } } } } Ok(entries) }
```

# plugins/twitter/src/timeline/mod.rs

```rs
pub mod home; pub mod search; pub mod tweet_utils; pub mod v1; pub mod v2; #[derive(Debug, Clone, Default)] pub struct TimelineParams { pub cursor: Option<String>, pub limit: Option<usize>, pub include_replies: bool, }
```

# plugins/twitter/src/timeline/search.rs

```rs
use crate::profile::parse_profile; use crate::timeline::v1::{QueryProfilesResponse, QueryTweetsResponse}; use crate::timeline::v2::{parse_legacy_tweet, SearchEntryRaw}; use lazy_static::lazy_static; use serde::Deserialize; lazy_static! { static ref EMPTY_INSTRUCTIONS: Vec<SearchInstruction> = Vec::new(); static ref EMPTY_ENTRIES: Vec<SearchEntryRaw> = Vec::new(); } #[derive(Debug, Deserialize)] pub struct SearchTimeline { pub data: Option<SearchData>, } #[derive(Debug, Deserialize)] pub struct SearchData { pub search_by_raw_query: Option<SearchByRawQuery>, } #[derive(Debug, Deserialize)] pub struct SearchByRawQuery { pub search_timeline: Option<SearchTimelineData>, } #[derive(Debug, Deserialize)] pub struct SearchTimelineData { pub timeline: Option<TimelineData>, } #[derive(Debug, Deserialize)] pub struct TimelineData { pub instructions: Option<Vec<SearchInstruction>>, } #[derive(Debug, Deserialize)] pub struct SearchInstruction { pub entries: Option<Vec<SearchEntryRaw>>, pub entry: Option<SearchEntryRaw>, #[serde(rename = "type")] pub instruction_type: Option<String>, } pub fn parse_search_timeline_tweets(timeline: &SearchTimeline) -> QueryTweetsResponse { let mut bottom_cursor = None; let mut top_cursor = None; let mut tweets = Vec::new(); let instructions = timeline .data .as_ref() .and_then(|data| data.search_by_raw_query.as_ref()) .and_then(|search| search.search_timeline.as_ref()) .and_then(|timeline| timeline.timeline.as_ref()) .and_then(|timeline| timeline.instructions.as_ref()) .unwrap_or(&EMPTY_INSTRUCTIONS); for instruction in instructions { if let Some(instruction_type) = &instruction.instruction_type { if instruction_type == "TimelineAddEntries" || instruction_type == "TimelineReplaceEntry" { if let Some(entry) = &instruction.entry { if let Some(content) = &entry.content { match content.cursor_type.as_deref() { Some("Bottom") => { bottom_cursor = content.value.clone(); continue; } Some("Top") => { top_cursor = content.value.clone(); continue; } _ => {} } } } // Process entries let entries = instruction.entries.as_ref().unwrap_or(&EMPTY_ENTRIES); for entry in entries { if let Some(content) = &entry.content { if let Some(item_content) = &content.item_content { if item_content.tweet_display_type.as_deref() == Some("Tweet") { if let Some(tweet_results) = &item_content.tweet_results { if let Some(result) = &tweet_results.result { let user_legacy = result .core .as_ref() .and_then(|core| core.user_results.as_ref()) .and_then(|user_results| user_results.result.as_ref()) .and_then(|result| result.legacy.as_ref()); if let Ok(tweet_result) = parse_legacy_tweet( user_legacy, result.legacy.as_deref(), ) { if tweet_result.views.is_none() { if let Some(views) = &result.views { if let Some(count) = &views.count { if let Ok(view_count) = count.parse::<i32>() { let mut tweet = tweet_result; tweet.views = Some(view_count); tweets.push(tweet); } } } } else { tweets.push(tweet_result); } } } } } } else if let Some(cursor_type) = &content.cursor_type { match cursor_type.as_str() { "Bottom" => bottom_cursor = content.value.clone(), "Top" => top_cursor = content.value.clone(), _ => {} } } } } } } } QueryTweetsResponse { tweets, next: bottom_cursor, previous: top_cursor, } } pub fn parse_search_timeline_users(timeline: &SearchTimeline) -> QueryProfilesResponse { let mut bottom_cursor = None; let mut top_cursor = None; let mut profiles = Vec::new(); let instructions = timeline .data .as_ref() .and_then(|data| data.search_by_raw_query.as_ref()) .and_then(|search| search.search_timeline.as_ref()) .and_then(|timeline| timeline.timeline.as_ref()) .and_then(|timeline| timeline.instructions.as_ref()) .unwrap_or(&EMPTY_INSTRUCTIONS); for instruction in instructions { if let Some(instruction_type) = &instruction.instruction_type { if instruction_type == "TimelineAddEntries" || instruction_type == "TimelineReplaceEntry" { if let Some(entry) = &instruction.entry { if let Some(content) = &entry.content { match content.cursor_type.as_deref() { Some("Bottom") => { bottom_cursor = content.value.clone(); continue; } Some("Top") => { top_cursor = content.value.clone(); continue; } _ => {} } } } // Process entries let entries = instruction.entries.as_ref().unwrap_or(&EMPTY_ENTRIES); for entry in entries { if let Some(content) = &entry.content { if let Some(item_content) = &content.item_content { if item_content.user_display_type.as_deref() == Some("User") { if let Some(user_results) = &item_content.user_results { if let Some(result) = &user_results.result { if let Some(legacy) = &result.legacy { let mut profile = parse_profile(legacy, result.is_blue_verified); if profile.id.is_empty() { profile.id = result.rest_id.clone().unwrap_or_default(); } profiles.push(profile); } } } } } else if let Some(cursor_type) = &content.cursor_type { match cursor_type.as_str() { "Bottom" => bottom_cursor = content.value.clone(), "Top" => top_cursor = content.value.clone(), _ => {} } } } } } } } QueryProfilesResponse { profiles, next: bottom_cursor, previous: top_cursor, } }
```

# plugins/twitter/src/timeline/tweet_utils.rs

```rs
use crate::models::{Photo, Video}; use crate::timeline::v1::{LegacyTweetRaw, TimelineMediaExtendedRaw}; use lazy_static::lazy_static; use regex::Regex; lazy_static! { static ref RE_HASHTAG: Regex = Regex::new(r"\B(\#\S+\b)").unwrap(); static ref RE_CASHTAG: Regex = Regex::new(r"\B(\$\S+\b)").unwrap(); static ref RE_TWITTER_URL: Regex = Regex::new(r"https:(\/\/t\.co\/([A-Za-z0-9]|[A-Za-z]){10})").unwrap(); static ref RE_USERNAME: Regex = Regex::new(r"\B(\@\S{1,15}\b)").unwrap(); } pub type NonNullableMediaFields = TimelineMediaExtendedRaw; pub fn parse_media_groups(media: &[TimelineMediaExtendedRaw]) -> (Vec<Photo>, Vec<Video>, bool) { let mut photos = Vec::new(); let mut videos = Vec::new(); let mut sensitive_content = false; for m in media .iter() .filter(|m| m.id_str.is_some() && m.media_url_https.is_some()) { match m.r#type.as_deref() { Some("photo") => { photos.push(Photo { id: m.id_str.clone().unwrap(), url: m.media_url_https.clone().unwrap(), alt_text: m.ext_alt_text.clone(), }); } Some("video") => { videos.push(parse_video(m)); } _ => {} } if let Some(warning) = &m.ext_sensitive_media_warning { sensitive_content = warning.adult_content.unwrap_or(false) || warning.graphic_violence.unwrap_or(false) || warning.other.unwrap_or(false); } } (photos, videos, sensitive_content) } fn parse_video(m: &NonNullableMediaFields) -> Video { let mut video = Video { id: m.id_str.clone().unwrap(), preview: m.media_url_https.clone().unwrap(), url: None, }; let mut max_bitrate = 0; if let Some(video_info) = &m.video_info { if let Some(variants) = &video_info.variants { for variant in variants { if let (Some(bitrate), Some(url)) = (&variant.bitrate, &variant.url) { if *bitrate > max_bitrate { let mut variant_url = url.clone(); if let Some(idx) = variant_url.find("?tag=10") { variant_url = variant_url[..idx + 1].to_string(); } video.url = Some(variant_url); max_bitrate = *bitrate; } } } } } video } pub fn reconstruct_tweet_html( tweet: &LegacyTweetRaw, photos: &[Photo], videos: &[Video], ) -> Option<String> { let mut html = tweet.full_text.clone().unwrap_or_default(); let mut media = Vec::new(); // Replace entities with HTML html = RE_HASHTAG .replace_all(&html, |caps: &regex::Captures| link_hashtag_html(&caps[0])) .to_string(); html = RE_CASHTAG .replace_all(&html, |caps: &regex::Captures| link_cashtag_html(&caps[0])) .to_string(); html = RE_USERNAME .replace_all(&html, |caps: &regex::Captures| link_username_html(&caps[0])) .to_string(); html = RE_TWITTER_URL .replace_all(&html, |caps: &regex::Captures| { unwrap_tco_url_html(tweet, &mut media, &caps[0]) }) .to_string(); // Add media for photo in photos { if !media.contains(&photo.url) { html.push_str(&format!("<br><img src=\"{}\"/>", photo.url)); } } for video in videos { if !media.contains(&video.preview) { html.push_str(&format!("<br><img src=\"{}\"/>", video.preview)); } } // Replace newlines with <br> html = html.replace('\n', "<br>"); Some(html) } fn link_hashtag_html(hashtag: &str) -> String { format!( "<a href=\"https://twitter.com/hashtag/{}\">{}</a>", &hashtag[1..], hashtag ) } fn link_cashtag_html(cashtag: &str) -> String { format!( "<a href=\"https://twitter.com/search?q=%24{}\">{}</a>", &cashtag[1..], cashtag ) } fn link_username_html(username: &str) -> String { format!( "<a href=\"https://twitter.com/{}\">{}</a>", &username[1..], username ) } fn unwrap_tco_url_html(tweet: &LegacyTweetRaw, found_media: &mut Vec<String>, tco: &str) -> String { if let Some(entities) = &tweet.entities { // Check URLs if let Some(urls) = &entities.urls { for entity in urls { if let (Some(url), Some(expanded)) = (&entity.url, &entity.expanded_url) { if url == tco { return format!("<a href=\"{}\">{}</a>", expanded, tco); } } } } // Check media if let Some(media) = &entities.media { for entity in media { if let (Some(url), Some(media_url)) = (&entity.url, &entity.media_url_https) { if url == tco { found_media.push(media_url.clone()); return format!("<br><a href=\"{}\"><img src=\"{}\"/></a>", tco, media_url); } } } } } tco.to_string() }
```

# plugins/twitter/src/timeline/v1.rs

```rs
use crate::models::tweets::Mention; use crate::models::tweets::PlaceRaw; use crate::models::{Profile, Tweet}; use crate::profile::LegacyUserRaw; use crate::timeline::tweet_utils::{parse_media_groups, reconstruct_tweet_html}; use chrono::DateTime; use chrono::Utc; use serde::{Deserialize, Serialize}; use std::collections::HashMap; #[derive(Debug, Deserialize, Serialize)] pub struct Hashtag { pub text: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineUserMentionBasicRaw { pub id_str: Option<String>, pub name: Option<String>, pub screen_name: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineMediaBasicRaw { pub media_url_https: Option<String>, pub r#type: Option<String>, pub url: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineUrlBasicRaw { pub expanded_url: Option<String>, pub url: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct ExtSensitiveMediaWarningRaw { pub adult_content: Option<bool>, pub graphic_violence: Option<bool>, pub other: Option<bool>, } #[derive(Debug, Deserialize, Serialize)] pub struct VideoVariant { pub bitrate: Option<i32>, pub url: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct VideoInfo { pub variants: Option<Vec<VideoVariant>>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineMediaExtendedRaw { pub id_str: Option<String>, pub media_url_https: Option<String>, pub ext_sensitive_media_warning: Option<ExtSensitiveMediaWarningRaw>, pub r#type: Option<String>, pub url: Option<String>, pub video_info: Option<VideoInfo>, pub ext_alt_text: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct SearchResultRaw { pub rest_id: Option<String>, pub __typename: Option<String>, pub core: Option<UserResultsCore>, pub views: Option<Views>, pub note_tweet: Option<NoteTweet>, pub quoted_status_result: Option<QuotedStatusResult>, pub legacy: Option<LegacyTweetRaw>, } #[derive(Debug, Deserialize, Serialize)] pub struct UserResultsCore { pub user_results: Option<UserResults>, } #[derive(Debug, Deserialize, Serialize)] pub struct UserResults { pub result: Option<UserResult>, } #[derive(Debug, Deserialize, Serialize)] pub struct UserResult { pub is_blue_verified: Option<bool>, pub legacy: Option<LegacyUserRaw>, } #[derive(Debug, Deserialize, Serialize)] pub struct Views { pub count: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct NoteTweet { pub note_tweet_results: Option<NoteTweetResults>, } #[derive(Debug, Deserialize, Serialize)] pub struct NoteTweetResults { pub result: Option<NoteTweetResult>, } #[derive(Debug, Deserialize, Serialize)] pub struct NoteTweetResult { pub text: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct QuotedStatusResult { pub result: Option<Box<SearchResultRaw>>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineResultRaw { pub result: Option<Box<TimelineResultRaw>>, pub rest_id: Option<String>, pub __typename: Option<String>, pub core: Option<TimelineCore>, pub views: Option<TimelineViews>, pub note_tweet: Option<TimelineNoteTweet>, pub quoted_status_result: Option<Box<TimelineQuotedStatus>>, pub legacy: Option<Box<LegacyTweetRaw>>, pub tweet: Option<Box<TimelineResultRaw>>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineCore { pub user_results: Option<TimelineUserResults>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineUserResults { pub result: Option<TimelineUserResult>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineUserResult { pub is_blue_verified: Option<bool>, pub legacy: Option<LegacyUserRaw>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineViews { pub count: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineNoteTweet { pub note_tweet_results: Option<TimelineNoteTweetResults>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineNoteTweetResults { pub result: Option<TimelineNoteTweetResult>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineNoteTweetResult { pub text: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineQuotedStatus { pub result: Option<Box<TimelineResultRaw>>, } #[derive(Debug, Deserialize, Serialize)] pub struct LegacyTweetRaw { pub bookmark_count: Option<i32>, pub conversation_id_str: Option<String>, pub created_at: Option<String>, pub favorite_count: Option<i32>, pub full_text: Option<String>, pub entities: Option<TweetEntities>, pub extended_entities: Option<TweetExtendedEntities>, pub id_str: Option<String>, pub in_reply_to_status_id_str: Option<String>, pub place: Option<PlaceRaw>, pub reply_count: Option<i32>, pub retweet_count: Option<i32>, pub retweeted_status_id_str: Option<String>, pub retweeted_status_result: Option<TimelineRetweetedStatus>, pub quoted_status_id_str: Option<String>, pub time: Option<String>, pub user_id_str: Option<String>, pub ext_views: Option<TweetExtViews>, } #[derive(Debug, Deserialize, Serialize)] pub struct TweetEntities { pub hashtags: Option<Vec<Hashtag>>, pub media: Option<Vec<TimelineMediaBasicRaw>>, pub urls: Option<Vec<TimelineUrlBasicRaw>>, pub user_mentions: Option<Vec<TimelineUserMentionBasicRaw>>, } #[derive(Debug, Deserialize, Serialize)] pub struct TweetExtendedEntities { pub media: Option<Vec<TimelineMediaExtendedRaw>>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineRetweetedStatus { pub result: Option<TimelineResultRaw>, } #[derive(Debug, Deserialize, Serialize)] pub struct TweetExtViews { pub state: Option<String>, pub count: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineGlobalObjectsRaw { pub tweets: Option<HashMap<String, Option<LegacyTweetRaw>>>, pub users: Option<HashMap<String, Option<LegacyUserRaw>>>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineDataRawCursor { pub value: Option<String>, pub cursor_type: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineDataRawEntity { pub id: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineDataRawModuleItem { pub client_event_info: Option<ClientEventInfo>, } #[derive(Debug, Deserialize, Serialize)] pub struct ClientEventInfo { pub details: Option<ClientEventDetails>, } #[derive(Debug, Deserialize, Serialize)] pub struct ClientEventDetails { pub guide_details: Option<GuideDetails>, } #[derive(Debug, Deserialize, Serialize)] pub struct GuideDetails { pub transparent_guide_details: Option<TransparentGuideDetails>, } #[derive(Debug, Deserialize, Serialize)] pub struct TransparentGuideDetails { pub trend_metadata: Option<TrendMetadata>, } #[derive(Debug, Deserialize, Serialize)] pub struct TrendMetadata { pub trend_name: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineDataRawAddEntry { pub content: Option<TimelineEntryContent>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineDataRawPinEntry { pub content: Option<TimelinePinContent>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelinePinContent { pub item: Option<TimelineItem>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineDataRawReplaceEntry { pub content: Option<TimelineReplaceContent>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineReplaceContent { pub operation: Option<TimelineOperation>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineDataRawInstruction { pub add_entries: Option<TimelineAddEntries>, pub pin_entry: Option<TimelineDataRawPinEntry>, pub replace_entry: Option<TimelineDataRawReplaceEntry>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineAddEntries { pub entries: Option<Vec<TimelineDataRawAddEntry>>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineDataRaw { pub instructions: Option<Vec<TimelineDataRawInstruction>>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineV1 { pub global_objects: Option<TimelineGlobalObjectsRaw>, pub timeline: Option<TimelineDataRaw>, } #[derive(Debug)] pub enum ParseTweetResult { Success { tweet: Tweet }, Error { err: String }, } #[derive(Debug, Serialize, Deserialize)] pub struct QueryTweetsResponse { pub tweets: Vec<Tweet>, pub next: Option<String>, pub previous: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct QueryProfilesResponse { pub profiles: Vec<Profile>, pub next: Option<String>, pub previous: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineEntryContent { pub item: Option<TimelineItem>, pub operation: Option<TimelineOperation>, pub timeline_module: Option<TimelineModule>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineItem { pub content: Option<TimelineContent>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineContent { pub tweet: Option<TimelineDataRawEntity>, pub user: Option<TimelineDataRawEntity>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineOperation { pub cursor: Option<TimelineDataRawCursor>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineModule { pub items: Option<Vec<TimelineModuleItemWrapper>>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineModuleItemWrapper { pub item: Option<TimelineDataRawModuleItem>, } #[derive(Debug)] pub struct UserMention { pub id: String, pub username: String, pub name: String, } pub fn parse_timeline_tweet(timeline: &TimelineV1, id: &str) -> ParseTweetResult { let empty_tweets = HashMap::new(); let tweets = match &timeline.global_objects { Some(go) => go.tweets.as_ref().unwrap_or(&empty_tweets), None => { return ParseTweetResult::Error { err: "No global objects found".to_string(), } } }; let tweet = match tweets.get(id) { Some(Some(t)) => t, _ => { return ParseTweetResult::Error { err: format!("Tweet \"{}\" was not found in the timeline object.", id), } } }; let user_id = match &tweet.user_id_str { Some(id) => id, None => { return ParseTweetResult::Error { err: "Tweet has no user ID".to_string(), } } }; let empty_users = HashMap::new(); let users = match &timeline.global_objects { Some(go) => go.users.as_ref().unwrap_or(&empty_users), None => { return ParseTweetResult::Error { err: "No users found".to_string(), } } }; let user = match users.get(user_id) { Some(Some(u)) => u, _ => { return ParseTweetResult::Error { err: format!("User \"{}\" has no username data.", user_id), } } }; let hashtags = tweet .entities .as_ref() .and_then(|e| e.hashtags.as_ref()) .map(|h| h.iter().filter_map(|tag| tag.text.clone()).collect()) .unwrap_or_default(); let mentions = tweet .entities .as_ref() .and_then(|e| e.user_mentions.as_ref()) .map(|m| { m.iter() .filter_map(|mention| { if let (Some(id), Some(screen_name), Some(name)) = (&mention.id_str, &mention.screen_name, &mention.name) { Some(Mention { id: id.clone(), username: Some(screen_name.clone()), name: Some(name.clone()), }) } else { None } }) .collect() }) .unwrap_or_default(); let empty_media = Vec::new(); let media = tweet .extended_entities .as_ref() .and_then(|e| e.media.as_ref()) .unwrap_or(&empty_media); let urls = tweet .entities .as_ref() .and_then(|e| e.urls.as_ref()) .map(|u| { u.iter() .filter_map(|url| url.expanded_url.clone()) .collect() }) .unwrap_or_default(); let (photos, videos, sensitive_content) = parse_media_groups(media); let mut tweet_obj = Tweet { conversation_id: tweet.conversation_id_str.clone(), id: Some(id.to_string()), hashtags, likes: tweet.favorite_count, mentions, name: user.name.clone(), permanent_url: Some(format!( "https://twitter.com/{}/status/{}", user.screen_name.as_ref().unwrap_or(&String::new()), id )), photos, replies: tweet.reply_count, retweets: tweet.retweet_count, text: tweet.full_text.clone(), thread: Vec::new(), urls, user_id: tweet.user_id_str.clone(), username: user.screen_name.clone(), videos, time_parsed: None, timestamp: None, place: None, is_quoted: Some(false), quoted_status_id: None, quoted_status: None, is_reply: Some(false), in_reply_to_status_id: None, in_reply_to_status: None, is_retweet: Some(false), retweeted_status_id: None, retweeted_status: None, views: None, is_pin: Some(false), sensitive_content: Some(sensitive_content), html: None, bookmark_count: None, is_self_thread: None, poll: None, created_at: None, ext_views: None, quote_count: None, reply_count: None, retweet_count: None, screen_name: None, thread_id: None, }; if let Some(created_at) = &tweet.created_at { if let Ok(parsed_time) = DateTime::parse_from_str(created_at, "%a %b %d %H:%M:%S %z %Y") { tweet_obj.time_parsed = Some(parsed_time.with_timezone(&Utc)); tweet_obj.timestamp = Some(parsed_time.timestamp()); } } if let Some(place) = &tweet.place { tweet_obj.place = Some(place.clone()); } if let Some(quoted_id) = &tweet.quoted_status_id_str { tweet_obj.is_quoted = Some(true); tweet_obj.quoted_status_id = Some(quoted_id.clone()); if let ParseTweetResult::Success { tweet: quoted_tweet, } = parse_timeline_tweet(timeline, quoted_id) { tweet_obj.quoted_status = Some(Box::new(quoted_tweet)); } } if let Some(ext_views) = &tweet.ext_views { if let Some(count) = &ext_views.count { if let Ok(views) = count.parse::<i32>() { tweet_obj.views = Some(views); } } } tweet_obj.html = reconstruct_tweet_html(tweet, &tweet_obj.photos, &tweet_obj.videos); ParseTweetResult::Success { tweet: tweet_obj } }
```

# plugins/twitter/src/timeline/v2.rs

```rs
use crate::error::Result; use crate::error::TwitterError; use crate::models::tweets::Mention; use crate::models::Tweet; use crate::profile::LegacyUserRaw; use crate::timeline::tweet_utils::parse_media_groups; use crate::timeline::v1::{LegacyTweetRaw, TimelineResultRaw}; use chrono::Utc; use lazy_static::lazy_static; use serde::{Deserialize, Serialize}; lazy_static! { static ref EMPTY_INSTRUCTIONS: Vec<TimelineInstruction> = Vec::new(); } #[derive(Debug, Deserialize, Serialize)] pub struct Timeline { pub timeline: Option<TimelineItems>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineContent { pub instructions: Option<Vec<TimelineInstruction>>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineData { pub user: Option<TimelineUser>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineEntities { pub hashtags: Option<Vec<Hashtag>>, pub user_mentions: Option<Vec<UserMention>>, pub urls: Option<Vec<UrlEntity>>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineEntry { #[serde(rename = "entryId")] pub entry_id: Option<String>, pub content: Option<EntryContent>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineEntryItemContent { pub item_type: Option<String>, pub tweet_display_type: Option<String>, pub tweet_result: Option<TweetResult>, pub tweet_results: Option<TweetResult>, pub user_display_type: Option<String>, pub user_results: Option<TimelineUserResult>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineEntryItemContentRaw { #[serde(rename = "itemType")] pub item_type: Option<String>, #[serde(rename = "tweetDisplayType")] pub tweet_display_type: Option<String>, #[serde(rename = "tweetResult")] pub tweet_result: Option<TweetResultRaw>, pub tweet_results: Option<TweetResultRaw>, #[serde(rename = "userDisplayType")] pub user_display_type: Option<String>, pub user_results: Option<TimelineUserResultRaw>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineItems { pub instructions: Option<Vec<TimelineInstruction>>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineUser { pub result: Option<TimelineUserResult>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineUserResult { pub rest_id: Option<String>, pub legacy: Option<LegacyUserRaw>, pub is_blue_verified: Option<bool>, pub timeline_v2: Option<Box<TimelineV2>>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineUserResultRaw { pub result: Option<TimelineUserResult>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineV2 { pub data: Option<TimelineData>, pub timeline: Option<TimelineItems>, } #[derive(Debug, Deserialize, Serialize)] pub struct ThreadedConversation { pub data: Option<ThreadedConversationData>, } #[derive(Debug, Deserialize, Serialize)] pub struct ThreadedConversationData { pub threaded_conversation_with_injections_v2: Option<TimelineContent>, } #[derive(Debug, Deserialize, Serialize)] pub struct TweetResult { pub result: Option<TimelineResultRaw>, } #[derive(Debug, Deserialize, Serialize)] pub struct TweetResultRaw { pub result: Option<TimelineResultRaw>, } #[derive(Debug, Deserialize, Serialize)] pub struct EntryContent { #[serde(rename = "cursorType")] pub cursor_type: Option<String>, pub value: Option<String>, pub items: Option<Vec<EntryItem>>, #[serde(rename = "itemContent")] pub item_content: Option<TimelineEntryItemContent>, } #[derive(Debug, Deserialize, Serialize)] pub struct EntryItem { #[serde(rename = "entryId")] pub entry_id: Option<String>, pub item: Option<ItemContent>, } #[derive(Debug, Deserialize, Serialize)] pub struct ItemContent { pub content: Option<TimelineEntryItemContent>, #[serde(rename = "itemContent")] pub item_content: Option<TimelineEntryItemContent>, } #[derive(Debug, Deserialize, Serialize)] pub struct Hashtag { pub text: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct UrlEntity { pub expanded_url: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct UserMention { pub id_str: Option<String>, pub name: Option<String>, pub screen_name: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct TimelineInstruction { pub entries: Option<Vec<TimelineEntry>>, pub entry: Option<TimelineEntry>, #[serde(rename = "type")] pub type_: Option<String>, } #[derive(Debug, Deserialize, Serialize)] pub struct SearchEntryRaw { #[serde(rename = "entryId")] pub entry_id: String, #[serde(rename = "sortIndex")] pub sort_index: String, pub content: Option<SearchEntryContentRaw>, } #[derive(Debug, Deserialize, Serialize)] pub struct SearchEntryContentRaw { #[serde(rename = "cursorType")] pub cursor_type: Option<String>, #[serde(rename = "entryType")] pub entry_type: Option<String>, #[serde(rename = "__typename")] pub typename: Option<String>, pub value: Option<String>, pub items: Option<Vec<SearchEntryItemRaw>>, #[serde(rename = "itemContent")] pub item_content: Option<TimelineEntryItemContentRaw>, } #[derive(Debug, Deserialize, Serialize)] pub struct SearchEntryItemRaw { pub item: Option<SearchEntryItemInnerRaw>, } #[derive(Debug, Deserialize, Serialize)] pub struct SearchEntryItemInnerRaw { pub content: Option<TimelineEntryItemContentRaw>, } pub fn parse_legacy_tweet( user: Option<&LegacyUserRaw>, tweet: Option<&LegacyTweetRaw>, ) -> Result<Tweet> { let tweet = tweet.ok_or(TwitterError::Api( "Tweet was not found in the timeline object".into(), ))?; let user = user.ok_or(TwitterError::Api( "User was not found in the timeline object".into(), ))?; let id_str = tweet .id_str .as_ref() .or(tweet.conversation_id_str.as_ref()) .ok_or(TwitterError::Api("Tweet ID was not found in object".into()))?; let hashtags = tweet .entities .as_ref() .and_then(|e| e.hashtags.as_ref()) .map(|h| h.iter().filter_map(|h| h.text.clone()).collect()) .unwrap_or_default(); let mentions = tweet .entities .as_ref() .and_then(|e| e.user_mentions.as_ref()) .map(|mentions| { mentions .iter() .map(|m| Mention { id: m.id_str.clone().unwrap_or_default(), name: m.name.clone(), username: m.screen_name.clone(), }) .collect() }) .unwrap_or_default(); let (photos, videos, _) = if let Some(extended_entities) = &tweet.extended_entities { if let Some(media) = &extended_entities.media { parse_media_groups(media) } else { (Vec::new(), Vec::new(), false) } } else { (Vec::new(), Vec::new(), false) }; let mut tweet = Tweet { bookmark_count: tweet.bookmark_count, conversation_id: tweet.conversation_id_str.clone(), id: Some(id_str.clone()), hashtags, likes: tweet.favorite_count, mentions, name: user.name.clone(), permanent_url: Some(format!( "https://twitter.com/{}/status/{}", user.screen_name.as_ref().unwrap_or(&String::new()), id_str )), photos, replies: tweet.reply_count, retweets: tweet.retweet_count, text: tweet.full_text.clone(), thread: Vec::new(), urls: tweet .entities .as_ref() .and_then(|e| e.urls.as_ref()) .map(|urls| urls.iter().filter_map(|u| u.expanded_url.clone()).collect()) .unwrap_or_default(), user_id: tweet.user_id_str.clone(), username: user.screen_name.clone(), videos, is_quoted: Some(false), is_reply: Some(false), is_retweet: Some(false), is_pin: Some(false), sensitive_content: Some(false), quoted_status: None, quoted_status_id: tweet.quoted_status_id_str.clone(), in_reply_to_status_id: tweet.in_reply_to_status_id_str.clone(), retweeted_status: None, retweeted_status_id: None, views: None, html: None, time_parsed: None, timestamp: None, place: tweet.place.clone(), in_reply_to_status: None, is_self_thread: None, poll: None, created_at: tweet.created_at.clone(), ext_views: None, quote_count: None, reply_count: None, retweet_count: None, screen_name: None, thread_id: None, }; if let Some(created_at) = &tweet.created_at { if let Ok(time) = chrono::DateTime::parse_from_str(created_at, "%a %b %d %H:%M:%S %z %Y") { tweet.time_parsed = Some(time.with_timezone(&Utc)); tweet.timestamp = Some(time.timestamp()); } } if let Some(views) = &tweet.ext_views { tweet.views = Some(*views); } // Set HTML // tweet.html = reconstruct_tweet_html(tweet, &photos, &videos); Ok(tweet) } pub fn parse_timeline_entry_item_content_raw( content: &TimelineEntryItemContent, _entry_id: &str, is_conversation: bool, ) -> Option<Tweet> { let result = content .tweet_results .as_ref() .or(content.tweet_result.as_ref()) .and_then(|r| r.result.as_ref())?; let tweet_result = parse_result(result); if tweet_result.success { let mut tweet = tweet_result.tweet?; if is_conversation && content.tweet_display_type.as_deref() == Some("SelfThread") { tweet.is_self_thread = Some(true); } return Some(tweet); } None } pub fn parse_and_push( tweets: &mut Vec<Tweet>, content: &TimelineEntryItemContent, entry_id: String, is_conversation: bool, ) { if let Some(tweet) = parse_timeline_entry_item_content_raw(content, &entry_id, is_conversation) { tweets.push(tweet); } } pub fn parse_result(result: &TimelineResultRaw) -> ParseTweetResult { let tweet_result = parse_legacy_tweet( result .core .as_ref() .and_then(|c| c.user_results.as_ref()) .and_then(|u| u.result.as_ref()) .and_then(|r| r.legacy.as_ref()), result.legacy.as_deref(), ); let mut tweet = match tweet_result { Ok(tweet) => tweet, Err(e) => { return ParseTweetResult { success: false, tweet: None, err: Some(e), } } }; if tweet.views.is_none() { if let Some(count) = result .views .as_ref() .and_then(|v| v.count.as_ref()) .and_then(|c| c.parse().ok()) { tweet.views = Some(count); } } if let Some(quoted) = result.quoted_status_result.as_ref() { if let Some(quoted_result) = quoted.result.as_ref() { let quoted_tweet_result = parse_result(quoted_result); if quoted_tweet_result.success { tweet.quoted_status = quoted_tweet_result.tweet.map(Box::new); } } } ParseTweetResult { success: true, tweet: Some(tweet), err: None, } } pub struct ParseTweetResult { pub success: bool, pub tweet: Option<Tweet>, pub err: Option<TwitterError>, } #[derive(Debug, Serialize, Deserialize)] pub struct QueryTweetsResponse { pub tweets: Vec<Tweet>, pub next: Option<String>, pub previous: Option<String>, } pub fn parse_timeline_tweets_v2(timeline: &TimelineV2) -> QueryTweetsResponse { let mut tweets = Vec::new(); let mut bottom_cursor = None; let mut top_cursor = None; let instructions = timeline .data .as_ref() .and_then(|data| data.user.as_ref()) .and_then(|user| user.result.as_ref()) .and_then(|result| result.timeline_v2.as_ref()) .and_then(|timeline| timeline.timeline.as_ref()) .and_then(|timeline| timeline.instructions.as_ref()) .unwrap_or(&EMPTY_INSTRUCTIONS); let expected_entry_types = ["tweet-", "profile-conversation-"]; for instruction in instructions { let entries = instruction.entries.as_deref().unwrap_or_else(|| { instruction .entry .as_ref() .map(std::slice::from_ref) .unwrap_or_default() }); for entry in entries { let content = match &entry.content { Some(content) => content, None => continue, }; if let Some(cursor_type) = &content.cursor_type { match cursor_type.as_str() { "Bottom" => { bottom_cursor = content.value.clone(); continue; } "Top" => { top_cursor = content.value.clone(); continue; } _ => {} } } let entry_id = match &entry.entry_id { Some(id) => id, None => continue, }; if !expected_entry_types .iter() .any(|prefix| entry_id.starts_with(prefix)) { continue; } if let Some(ref item_content) = content.item_content { parse_and_push(&mut tweets, item_content, entry_id.clone(), false); } if let Some(items) = &content.items { for item in items { if let Some(item) = &item.item { if let Some(item_content) = &item.item_content { parse_and_push(&mut tweets, item_content, entry_id.clone(), false); } } } } } } QueryTweetsResponse { tweets, next: bottom_cursor, previous: top_cursor, } } pub fn parse_threaded_conversation( conversation: &ThreadedConversation, ) -> (Option<Tweet>, Option<Vec<Tweet>>) { let mut main_tweet: Option<Tweet> = None; let mut replies: Vec<Tweet> = Vec::new(); let instructions = conversation .data .as_ref() .and_then(|data| data.threaded_conversation_with_injections_v2.as_ref()) .and_then(|conv| conv.instructions.as_ref()) .unwrap_or(&EMPTY_INSTRUCTIONS); for instruction in instructions { let entries = instruction.entries.as_deref().unwrap_or_default(); for entry in entries { if let Some(content) = &entry.content { if let Some(item_content) = &content.item_content { if let Some(tweet) = parse_timeline_entry_item_content_raw( item_content, entry.entry_id.as_deref().unwrap_or_default(), true, ) { if main_tweet.is_none() { main_tweet = Some(tweet); } else { replies.push(tweet); } } } if let Some(items) = &content.items { for item in items { if let Some(item) = &item.item { if let Some(item_content) = &item.item_content { if let Some(tweet) = parse_timeline_entry_item_content_raw( item_content, entry.entry_id.as_deref().unwrap_or_default(), true, ) { replies.push(tweet); } } } } } } } } if let Some(mut main_tweet) = main_tweet { for reply in &replies { if let Some(reply_id) = &reply.in_reply_to_status_id { if let Some(main_id) = &main_tweet.id { if reply_id == main_id { main_tweet.replies = Some(replies.len() as i32); break; } } } } if main_tweet.is_self_thread == Some(true) { let thread = replies .iter() .filter(|t| t.is_self_thread == Some(true)) .cloned() .collect::<Vec<_>>(); if thread.is_empty() { main_tweet.is_self_thread = Some(false); } else { main_tweet.thread = thread; } } // main_tweet.html = reconstruct_tweet_html(&main_tweet); (Some(main_tweet), Some(replies)) } else { (None, None) } }
```

# plugins/twitter/src/tweets.rs

```rs
use crate::api::client::TwitterClient; use crate::api::endpoints::Endpoints; use crate::api::requests::{request_api, request_multipart_api}; use crate::error::{Result, TwitterError}; use crate::models::tweets::Tweet; use crate::profile::get_user_id_by_screen_name; use crate::timeline::v2::parse_threaded_conversation; use crate::timeline::v2::parse_timeline_tweets_v2; use crate::timeline::v2::QueryTweetsResponse; use crate::timeline::v2::ThreadedConversation; use reqwest::header::HeaderMap; use reqwest::Method; use serde::{Deserialize, Serialize}; use serde_json::{json, Value}; pub const DEFAULT_EXPANSIONS: &[&str] = &[ "attachments.poll_ids", "attachments.media_keys", "author_id", "referenced_tweets.id", "in_reply_to_user_id", "edit_history_tweet_ids", "geo.place_id", "entities.mentions.username", "referenced_tweets.id.author_id", ]; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct Mention { pub id: String, pub username: Option<String>, pub name: Option<String>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct Photo { pub id: String, pub url: String, pub alt_text: Option<String>, } pub async fn fetch_tweets( client: &TwitterClient, user_id: &str, max_tweets: i32, cursor: Option<&str>, ) -> Result<Value> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let mut variables = json!({ "userId": user_id, "count": max_tweets.min(200), "includePromotedContent": false }); if let Some(cursor_val) = cursor { variables["cursor"] = json!(cursor_val); } let (value, _headers) = request_api( &client.client, "https://twitter.com/i/api/graphql/YNXM2DGuE2Sff6a2JD3Ztw/UserTweets", headers, Method::GET, Some(json!({ "variables": variables, "features": get_default_features() })), ) .await?; Ok(value) } pub async fn fetch_tweets_and_replies( client: &TwitterClient, username: &str, max_tweets: i32, cursor: Option<&str>, ) -> Result<QueryTweetsResponse> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let user_id = get_user_id_by_screen_name(client, username).await?; let endpoint = Endpoints::user_tweets_and_replies(&user_id, max_tweets.min(40), cursor); let (value, _headers) = request_api( &client.client, &endpoint.to_request_url(), headers, Method::GET, None, ) .await?; let parsed_response = parse_timeline_tweets_v2(&value); Ok(parsed_response) } pub async fn fetch_tweets_and_replies_by_user_id( client: &TwitterClient, user_id: &str, max_tweets: i32, cursor: Option<&str>, ) -> Result<QueryTweetsResponse> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let endpoint = Endpoints::user_tweets_and_replies(user_id, max_tweets.min(40), cursor); let (value, _headers) = request_api( &client.client, &endpoint.to_request_url(), headers, Method::GET, None, ) .await?; let parsed_response = parse_timeline_tweets_v2(&value); Ok(parsed_response) } pub async fn fetch_list_tweets( client: &TwitterClient, list_id: &str, max_tweets: i32, cursor: Option<&str>, ) -> Result<Value> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let mut variables = json!({ "listId": list_id, "count": max_tweets.min(200) }); if let Some(cursor_val) = cursor { variables["cursor"] = json!(cursor_val); } let (value, _headers) = request_api( &client.client, "https://twitter.com/i/api/graphql/LFKj1wqHNTsEJ4Oq7TzaNA/ListLatestTweetsTimeline", headers, Method::GET, Some(json!({ "variables": variables, "features": get_default_features() })), ) .await?; Ok(value) } pub async fn create_quote_tweet( client: &TwitterClient, text: &str, quoted_tweet_id: &str, media_data: Option<Vec<(Vec<u8>, String)>>, ) -> Result<Value> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let mut variables = json!({ "tweet_text": text, "dark_request": false, "attachment_url": format!("https://twitter.com/twitter/status/{}", quoted_tweet_id), "media": { "media_entities": [], "possibly_sensitive": false }, "semantic_annotation_ids": [] }); if let Some(media_files) = media_data { let mut media_entities = Vec::new(); for (file_data, media_type) in media_files { let media_id = upload_media(client, file_data, &media_type).await?; media_entities.push(json!({ "media_id": media_id, "tagged_users": [] })); } variables["media"]["media_entities"] = json!(media_entities); } let (value, _headers) = request_api( &client.client, "https://twitter.com/i/api/graphql/a1p9RWpkYKBjWv_I3WzS-A/CreateTweet", headers, Method::POST, Some(json!({ "variables": variables, "features": create_quote_tweet_features() })), ) .await?; Ok(value) } pub async fn like_tweet(client: &TwitterClient, tweet_id: &str) -> Result<Value> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let (value, _headers) = request_api( &client.client, "https://twitter.com/i/api/graphql/lI07N6Otwv1PhnEgXILM7A/FavoriteTweet", headers, Method::POST, Some(json!({ "variables": { "tweet_id": tweet_id } })), ) .await?; Ok(value) } pub async fn retweet(client: &TwitterClient, tweet_id: &str) -> Result<Value> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let (value, _headers) = request_api( &client.client, "https://twitter.com/i/api/graphql/ojPdsZsimiJrUGLR1sjUtA/CreateRetweet", headers, Method::POST, Some(json!({ "variables": { "tweet_id": tweet_id, "dark_request": false } })), ) .await?; Ok(value) } pub async fn create_long_tweet( client: &TwitterClient, text: &str, reply_to: Option<&str>, media_ids: Option<Vec<String>>, ) -> Result<Value> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let mut variables = json!({ "tweet_text": text, "dark_request": false, "media": { "media_entities": [], "possibly_sensitive": false }, "semantic_annotation_ids": [] }); if let Some(reply_id) = reply_to { variables["reply"] = json!({ "in_reply_to_tweet_id": reply_id }); } if let Some(media) = media_ids { variables["media"]["media_entities"] = json!(media .iter() .map(|id| json!({ "media_id": id, "tagged_users": [] })) .collect::<Vec<_>>()); } let (value, _headers) = request_api( &client.client, "https://twitter.com/i/api/graphql/YNXM2DGuE2Sff6a2JD3Ztw/CreateNoteTweet", headers, Method::POST, Some(json!({ "variables": variables, "features": get_long_tweet_features() })), ) .await?; Ok(value) } pub async fn fetch_liked_tweets( client: &TwitterClient, user_id: &str, max_tweets: i32, cursor: Option<&str>, ) -> Result<Value> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let mut variables = json!({ "userId": user_id, "count": max_tweets.min(200), "includePromotedContent": false }); if let Some(cursor_val) = cursor { variables["cursor"] = json!(cursor_val); } let (value, _headers) = request_api( &client.client, "https://twitter.com/i/api/graphql/YlkSUg4Czo2Zx7yRqpwDow/Likes", headers, Method::GET, Some(json!({ "variables": variables, "features": get_default_features() })), ) .await?; Ok(value) } pub async fn upload_media( client: &TwitterClient, file_data: Vec<u8>, media_type: &str, ) -> Result<String> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let upload_url = "https://upload.twitter.com/1.1/media/upload.json"; // Check if media is video let is_video = media_type.starts_with("video/"); if is_video { // Handle video upload using chunked upload upload_video_in_chunks(client, file_data, media_type, headers).await } else { // Handle image upload directly let form = reqwest::multipart::Form::new() .part("media", reqwest::multipart::Part::bytes(file_data)); let (response, _) = request_multipart_api::<Value>(&client.client, upload_url, headers, form).await?; response["media_id_string"] .as_str() .map(String::from) .ok_or_else(|| TwitterError::Api("Failed to get media_id".into())) } } async fn upload_video_in_chunks( client: &TwitterClient, file_data: Vec<u8>, media_type: &str, headers: HeaderMap, ) -> Result<String> { let upload_url = "https://upload.twitter.com/1.1/media/upload.json"; // INIT command let (init_response, _) = request_api::<Value>( &client.client, upload_url, headers.clone(), Method::POST, Some(json!({ "command": "INIT", "total_bytes": file_data.len(), "media_type": media_type })), ) .await?; let media_id = init_response["media_id_string"] .as_str() .ok_or_else(|| TwitterError::Api("Failed to get media_id".into()))? .to_string(); // APPEND command - upload in chunks let chunk_size = 5 * 1024 * 1024; // 5MB chunks let mut segment_index = 0; for chunk in file_data.chunks(chunk_size) { let form = reqwest::multipart::Form::new() .text("command", "APPEND") .text("media_id", media_id.clone()) .text("segment_index", segment_index.to_string()) .part("media", reqwest::multipart::Part::bytes(chunk.to_vec())); let (_, _) = request_multipart_api::<Value>(&client.client, upload_url, headers.clone(), form) .await?; segment_index += 1; } // FINALIZE command let (finalize_response, _) = request_api::<Value>( &client.client, &format!("{}?command=FINALIZE&media_id={}", upload_url, media_id), headers.clone(), Method::POST, None, ) .await?; // Check processing status for videos if finalize_response.get("processing_info").is_some() { check_upload_status(client, &media_id, &headers).await?; } Ok(media_id) } async fn check_upload_status( client: &TwitterClient, media_id: &str, headers: &HeaderMap, ) -> Result<()> { let upload_url = "https://upload.twitter.com/1.1/media/upload.json"; for _ in 0..20 { // Maximum 20 attempts tokio::time::sleep(tokio::time::Duration::from_secs(5)).await; // Wait 5 seconds let (status_response, _) = request_api::<Value>( &client.client, &format!("{}?command=STATUS&media_id={}", upload_url, media_id), headers.clone(), Method::GET, None, ) .await?; if let Some(processing_info) = status_response.get("processing_info") { match processing_info["state"].as_str() { Some("succeeded") => return Ok(()), Some("failed") => return Err(TwitterError::Api("Video processing failed".into())), _ => continue, } } } Err(TwitterError::Api("Video processing timeout".into())) } pub async fn get_tweet( client: &TwitterClient, id: &str, ) -> Result<(Option<Tweet>, Option<Vec<Tweet>>)> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let tweet_detail_request = Endpoints::tweet_detail(id); let url = tweet_detail_request.to_request_url(); let (response, _) = request_api::<Value>(&client.client, &url, headers, Method::GET, None).await?; let data = response.clone(); let conversation: ThreadedConversation = serde_json::from_value(data)?; let (main_tweet, replies) = parse_threaded_conversation(&conversation); Ok((main_tweet, replies)) } fn create_tweet_features() -> Value { json!({ "interactive_text_enabled": true, "longform_notetweets_inline_media_enabled": false, "responsive_web_text_conversations_enabled": false, "tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled": false, "vibe_api_enabled": false, "rweb_lists_timeline_redesign_enabled": true, "responsive_web_graphql_exclude_directive_enabled": true, "verified_phone_label_enabled": false, "creator_subscriptions_tweet_preview_api_enabled": true, "responsive_web_graphql_timeline_navigation_enabled": true, "responsive_web_graphql_skip_user_profile_image_extensions_enabled": false, "tweetypie_unmention_optimization_enabled": true, "responsive_web_edit_tweet_api_enabled": true, "graphql_is_translatable_rweb_tweet_is_translatable_enabled": true, "view_counts_everywhere_api_enabled": true, "longform_notetweets_consumption_enabled": true, "tweet_awards_web_tipping_enabled": false, "freedom_of_speech_not_reach_fetch_enabled": true, "standardized_nudges_misinfo": true, "longform_notetweets_rich_text_read_enabled": true, "responsive_web_enhance_cards_enabled": false, "subscriptions_verification_info_enabled": true, "subscriptions_verification_info_reason_enabled": true, "subscriptions_verification_info_verified_since_enabled": true, "super_follow_badge_privacy_enabled": false, "super_follow_exclusive_tweet_notifications_enabled": false, "super_follow_tweet_api_enabled": false, "super_follow_user_api_enabled": false, "android_graphql_skip_api_media_color_palette": false, "creator_subscriptions_subscription_count_enabled": false, "blue_business_profile_image_shape_enabled": false, "unified_cards_ad_metadata_container_dynamic_card_content_query_enabled": false, "rweb_video_timestamps_enabled": false, "c9s_tweet_anatomy_moderator_badge_enabled": false, "responsive_web_twitter_article_tweet_consumption_enabled": false }) } fn get_default_features() -> Value { json!({ "interactive_text_enabled": true, "longform_notetweets_inline_media_enabled": false, "responsive_web_text_conversations_enabled": false, "tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled": false, "vibe_api_enabled": false, "rweb_lists_timeline_redesign_enabled": true, "responsive_web_graphql_exclude_directive_enabled": true, "verified_phone_label_enabled": false, "creator_subscriptions_tweet_preview_api_enabled": true, "responsive_web_graphql_timeline_navigation_enabled": true, "responsive_web_graphql_skip_user_profile_image_extensions_enabled": false, "tweetypie_unmention_optimization_enabled": true, "responsive_web_edit_tweet_api_enabled": true, "graphql_is_translatable_rweb_tweet_is_translatable_enabled": true, "view_counts_everywhere_api_enabled": true, "longform_notetweets_consumption_enabled": true, "tweet_awards_web_tipping_enabled": false, "freedom_of_speech_not_reach_fetch_enabled": true, "standardized_nudges_misinfo": true, "longform_notetweets_rich_text_read_enabled": true, "responsive_web_enhance_cards_enabled": false, "subscriptions_verification_info_enabled": true, "subscriptions_verification_info_reason_enabled": true, "subscriptions_verification_info_verified_since_enabled": true, "super_follow_badge_privacy_enabled": false, "super_follow_exclusive_tweet_notifications_enabled": false, "super_follow_tweet_api_enabled": false, "super_follow_user_api_enabled": false, "android_graphql_skip_api_media_color_palette": false, "creator_subscriptions_subscription_count_enabled": false, "blue_business_profile_image_shape_enabled": false, "unified_cards_ad_metadata_container_dynamic_card_content_query_enabled": false, "rweb_video_timestamps_enabled": true, "c9s_tweet_anatomy_moderator_badge_enabled": true, "responsive_web_twitter_article_tweet_consumption_enabled": false, "creator_subscriptions_quote_tweet_preview_enabled": false, "profile_label_improvements_pcf_label_in_post_enabled": false, "rweb_tipjar_consumption_enabled": true, "articles_preview_enabled": true }) } // Helper function for long tweet features fn get_long_tweet_features() -> Value { json!({ "premium_content_api_read_enabled": false, "communities_web_enable_tweet_community_results_fetch": true, "c9s_tweet_anatomy_moderator_badge_enabled": true, "responsive_web_grok_analyze_button_fetch_trends_enabled": true, "responsive_web_edit_tweet_api_enabled": true, "graphql_is_translatable_rweb_tweet_is_translatable_enabled": true, "view_counts_everywhere_api_enabled": true, "longform_notetweets_consumption_enabled": true, "responsive_web_twitter_article_tweet_consumption_enabled": true, "tweet_awards_web_tipping_enabled": false, "longform_notetweets_rich_text_read_enabled": true, "longform_notetweets_inline_media_enabled": true, "responsive_web_graphql_exclude_directive_enabled": true, "verified_phone_label_enabled": false, "freedom_of_speech_not_reach_fetch_enabled": true, "standardized_nudges_misinfo": true, "tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled": true, "responsive_web_graphql_timeline_navigation_enabled": true, "responsive_web_enhance_cards_enabled": false }) } pub async fn create_tweet_request( client: &TwitterClient, text: &str, reply_to: Option<&str>, media_data: Option<Vec<(Vec<u8>, String)>>, ) -> Result<Value> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; // Prepare variables let mut variables = json!({ "tweet_text": text, "dark_request": false, "media": { "media_entities": [], "possibly_sensitive": false }, "semantic_annotation_ids": [] }); // Add reply information if provided if let Some(reply_id) = reply_to { variables["reply"] = json!({ "in_reply_to_tweet_id": reply_id }); } // Handle media uploads if provided if let Some(media_files) = media_data { let mut media_entities = Vec::new(); // Upload each media file and collect media IDs for (file_data, media_type) in media_files { let media_id = upload_media(client, file_data, &media_type).await?; media_entities.push(json!({ "media_id": media_id, "tagged_users": [] })); } variables["media"]["media_entities"] = json!(media_entities); } let features = create_tweet_features(); // Make the create tweet request let (value, _headers) = request_api( &client.client, "https://twitter.com/i/api/graphql/a1p9RWpkYKBjWv_I3WzS-A/CreateTweet", headers, Method::POST, Some(json!({ "variables": variables, "features": features, "fieldToggles": {} })), ) .await?; Ok(value) } fn create_quote_tweet_features() -> Value { json!({ "interactive_text_enabled": true, "longform_notetweets_inline_media_enabled": false, "responsive_web_text_conversations_enabled": false, "tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled": false, "vibe_api_enabled": false, "rweb_lists_timeline_redesign_enabled": true, "responsive_web_graphql_exclude_directive_enabled": true, "verified_phone_label_enabled": false, "creator_subscriptions_tweet_preview_api_enabled": true, "responsive_web_graphql_timeline_navigation_enabled": true, "responsive_web_graphql_skip_user_profile_image_extensions_enabled": false, "tweetypie_unmention_optimization_enabled": true, "responsive_web_edit_tweet_api_enabled": true, "graphql_is_translatable_rweb_tweet_is_translatable_enabled": true, "view_counts_everywhere_api_enabled": true, "longform_notetweets_consumption_enabled": true, "tweet_awards_web_tipping_enabled": false, "freedom_of_speech_not_reach_fetch_enabled": true, "standardized_nudges_misinfo": true, "longform_notetweets_rich_text_read_enabled": true, "responsive_web_enhance_cards_enabled": false, "subscriptions_verification_info_enabled": true, "subscriptions_verification_info_reason_enabled": true, "subscriptions_verification_info_verified_since_enabled": true, "super_follow_badge_privacy_enabled": false, "super_follow_exclusive_tweet_notifications_enabled": false, "super_follow_tweet_api_enabled": false, "super_follow_user_api_enabled": false, "android_graphql_skip_api_media_color_palette": false, "creator_subscriptions_subscription_count_enabled": false, "blue_business_profile_image_shape_enabled": false, "unified_cards_ad_metadata_container_dynamic_card_content_query_enabled": false, "rweb_video_timestamps_enabled": true, "c9s_tweet_anatomy_moderator_badge_enabled": true, "responsive_web_twitter_article_tweet_consumption_enabled": false }) } pub async fn fetch_user_tweets( client: &TwitterClient, user_id: &str, max_tweets: i32, cursor: Option<&str>, ) -> Result<QueryTweetsResponse> { let mut headers = HeaderMap::new(); client.auth.install_headers(&mut headers).await?; let endpoint = Endpoints::user_tweets(user_id, max_tweets.min(200), cursor); let (value, _headers) = request_api( &client.client, &endpoint.to_request_url(), headers, Method::GET, None, ) .await?; let parsed_response = parse_timeline_tweets_v2(&value); Ok(parsed_response) }
```

# README.md

```md
# Cainam Core Core functionality for the Cainam project - A decentralized network of autonomous AI trading agents for the $CAINAM token platform on Solana. ## Overview Cainam Core is a Rust-based system that implements autonomous AI trading agents, market monitoring, and data analysis for the Solana blockchain. The system features real-time market data processing, automated trading execution, and advanced risk management capabilities. ### Key Features - Real-time market monitoring via Birdeye API - Blockchain transaction monitoring using Helius webhooks - Autonomous trading agents with AI-driven decision making - Advanced risk management and position sizing - Time-series data storage with TimescaleDB - Vector similarity search using Qdrant - Discord and Twitter integration ## Prerequisites - Rust 1.75+ (2021 edition) - PostgreSQL 15+ with TimescaleDB extension - Solana CLI tools - Node.js and npm (for development tools) ## Installation 1. Clone the repository: \`\`\`bash git clone https://github.com/cainamventures/cainam-core cd cainam-core \`\`\` 2. Copy the environment template and configure your variables: \`\`\`bash cp .env.example .env # Edit .env with your configuration \`\`\` 3. Install development dependencies: \`\`\`bash # Install pre-commit hooks pre-commit install # Install required database extensions psql -c 'CREATE EXTENSION IF NOT EXISTS timescaledb;' \`\`\` 4. Build the project: \`\`\`bash cargo build \`\`\` ## Configuration The following environment variables are required: \`\`\`env # Database DATABASE_URL=postgresql://user:password@localhost/dbname # Solana SOLANA_RPC_URL=your_rpc_url HELIUS_API_KEY=your_helius_key # APIs BIRDEYE_API_KEY=your_birdeye_key # Optional integrations DISCORD_TOKEN=your_discord_token TWITTER_API_KEY=your_twitter_key \`\`\` ## Project Structure \`\`\` src/ ├── actions/ # External API interactions ├── agent/ # Agent implementations ├── trading/ # Trading logic ├── models/ # Data models └── services/ # Business logic \`\`\` ## Development ### Running Tests \`\`\`bash # Run all tests cargo test # Run specific test suite cargo test --package cainam-core \`\`\` ### Database Migrations \`\`\`bash # Apply migrations sqlx migrate run # Create new migration sqlx migrate add <name> \`\`\` ### Code Style The project uses rustfmt and clippy for code formatting and linting: \`\`\`bash # Format code cargo fmt # Run clippy cargo clippy \`\`\` ## Performance Requirements - Trade execution: < 500ms end-to-end - Market data updates: < 1s refresh rate - Signal processing: < 200ms - Database queries: < 100ms response time ## Dependencies Core dependencies include: - tokio (async runtime) - solana-client & solana-sdk (blockchain interaction) - serde (serialization) - tokio-postgres (database) - qdrant-client (vector store) - rig-core (framework) ## Contributing Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on contributing to the project. ## License This project is licensed under the MIT License - see the LICENSE file for details. ## Contact - Author: Matt Gunnin - Email: <matt@cainamventures.com> - Repository: <https://github.com/cainamventures/cainam-core>
```

# scripts/add-template.sh

```sh
#!/bin/bash # Check if the name is provided if [ "$#" -ne 3 ]; then echo "Usage: $0 <plugin_name> <rig_name> <example_name>" exit 1 fi PLUGIN_NAME=$1 RIG_NAME=$2 EXAMPLE_NAME=$3 # Create cainam-plugins cargo new --lib cainam-plugins/$PLUGIN_NAME # Rename package name in Cargo.toml sed -i "s/name = \"$PLUGIN_NAME\"/name = \"cainam-plugin-$PLUGIN_NAME\"/" cainam-plugins/$PLUGIN_NAME/Cargo.toml # Create examples cargo new examples/$EXAMPLE_NAME
```

# scripts/init_mongodb.rs

```rs
use rig_mongodb::{MongoDbPool, bson::doc, options::ClientOptions}; use anyhow::Result; use tracing::info; #[tokio::main] async fn main() -> Result<()> { dotenv::dotenv().ok(); let mongodb_uri = std::env::var("MONGODB_URI").expect("MONGODB_URI must be set"); info!("Connecting to MongoDB..."); let client_options = ClientOptions::parse(&mongodb_uri).await?; let pool = MongoDbPool::new_with_options(&mongodb_uri, client_options).await?; let db = pool.database("cainam"); info!("Creating collections and indexes..."); // Create token_analytics collection with indexes db.create_collection("token_analytics", None).await?; db.collection("token_analytics").create_index( doc! { "token_address": 1, "timestamp": -1 }, None, ).await?; // Create market_signals collection with indexes db.create_collection("market_signals", None).await?; db.collection("market_signals").create_index( doc! { "asset_address": 1, "timestamp": -1 }, None, ).await?; // Create vector_store collection with indexes db.create_collection("vectors", None).await?; db.collection("vectors").create_index( doc! { "vector": "2dsphere", "metadata.timestamp": -1 }, None, ).await?; info!("MongoDB setup completed successfully!"); Ok(()) }
```

# scripts/init_vector_store.rs

```rs
use anyhow::Result; use cainam_core::config::mongodb::MongoConfig; use mongodb::bson::Document; use mongodb::{bson::doc, Client, IndexModel}; use tracing::info; use tracing_subscriber::fmt; #[tokio::main] async fn main() -> Result<()> { // Initialize tracing fmt() .with_target(false) .with_thread_ids(false) .with_thread_names(false) .with_file(true) .with_line_number(true) .init(); // Load environment variables dotenvy::dotenv().ok(); info!("Initializing vector store..."); // Initialize MongoDB connection let config = MongoConfig::from_env(); let uri = std::env::var("MONGODB_URI").expect("MONGODB_URI must be set"); let client = Client::with_uri_str(&uri).await?; let db = client.database(&config.database); // Create token_analytics collection info!("Creating token_analytics collection..."); match db.create_collection("token_analytics").await { Ok(_) => info!("Created token_analytics collection"), Err(e) if e.to_string().contains("already exists") => { info!("Collection token_analytics already exists") } Err(e) => return Err(e.into()), } // Create metadata index for token_analytics info!("Creating metadata index for token_analytics..."); let metadata_index = doc! { "metadata": 1 }; match db .collection::<Document>("token_analytics") .create_index(IndexModel::builder().keys(metadata_index).build()) .await { Ok(_) => info!("Created metadata index for token_analytics"), Err(e) if e.to_string().contains("already exists") => { info!("Metadata index already exists for token_analytics") } Err(e) => return Err(e.into()), } // Create metadata index for market_signals info!("Creating metadata index for market_signals..."); let index_model = IndexModel::builder() .keys(doc! { "asset_address": 1, "timestamp": -1 }) .build(); let collection = db.collection::<Document>("market_signals"); match collection.create_index(index_model).await { Ok(_) => info!("Created metadata index for market_signals"), Err(e) if e.to_string().contains("already exists") => { info!("Metadata index for market_signals already exists") } Err(e) => return Err(e.into()), } info!("Vector store initialization complete"); Ok(()) }
```

# scripts/run_migrations.rs

```rs
use anyhow::Result; use tracing::info; use crate::config::mongodb::MongoConfig; mod mongodb { pub mod m01_setup; pub mod m02_schema; pub mod m03_trade_status; pub mod m04_allocations; pub mod m05_vector_store; } #[tokio::main] async fn main() -> Result<()> { dotenv::dotenv().ok(); info!("Starting migrations..."); // Initialize MongoDB configuration let config = MongoConfig::from_env(); // Run MongoDB migrations in order info!("Running MongoDB migrations..."); mongodb::m01_setup::run(&config).await?; mongodb::m02_schema::run(&config).await?; mongodb::m03_trade_status::run(&config).await?; mongodb::m04_allocations::run(&config).await?; mongodb::m05_vector_store::run(&config).await?; info!("All migrations completed successfully!"); Ok(()) }
```

# scripts/setup_mongodb.rs

```rs
use cainam_core::config::mongodb::MongoConfig; use mongodb::{ bson::Document, options::{ClientOptions, IndexOptions}, Client, IndexModel, }; use std::error::Error; #[tokio::main] async fn main() -> Result<(), Box<dyn Error>> { // Load environment variables first dotenvy::dotenv().ok(); // Initialize MongoDB client using configuration let config = MongoConfig::from_env(); let mut client_options = ClientOptions::parse(&config.uri).await?; client_options.server_api = Some( mongodb::options::ServerApi::builder() .version(mongodb::options::ServerApiVersion::V1) .build(), ); let client = Client::with_options(client_options)?; let db = client.database(&config.database); println!("Connected to MongoDB successfully"); // Create collections if they don't exist println!("Creating collections..."); let collections = db.list_collection_names().await?; if !collections.contains(&"token_analytics".to_string()) { db.create_collection("token_analytics").await?; println!("Created token_analytics collection"); } else { println!("Collection token_analytics already exists"); } if !collections.contains(&"market_signals".to_string()) { db.create_collection("market_signals").await?; println!("Created market_signals collection"); } else { println!("Collection market_signals already exists"); } // Get collections let token_analytics = db.collection::<Document>("token_analytics"); let market_signals = db.collection::<Document>("market_signals"); // Create indexes for token_analytics collection println!("Creating indexes for token_analytics collection..."); // Compound index on token_address and timestamp let compound_index_options = IndexOptions::builder().build(); let compound_index = IndexModel::builder() .keys(mongodb::bson::doc! { "token_address": 1, "timestamp": 1 }) .options(compound_index_options) .build(); match token_analytics.create_index(compound_index).await { Ok(_) => println!("Created compound index for token_analytics"), Err(e) if e.to_string().contains("already exists") => { println!("Compound index already exists for token_analytics"); } Err(e) => return Err(e.into()), } // Create vector search index for embeddings let vector_search_command = mongodb::bson::doc! { "createSearchIndexes": "token_analytics", "indexes": [{ "name": "vector_index", "definition": { "mappings": { "dynamic": true, "fields": { "id": { "type": "string" }, "token_address": { "type": "string" }, "token_name": { "type": "string" }, "token_symbol": { "type": "string" }, "embedding": { "type": "knnVector", "dimensions": 1536, "similarity": "cosine" } } } } }] }; match db.run_command(vector_search_command).await { Ok(_) => println!("Created vector search index for token_analytics collection"), Err(e) if e.to_string().contains("already defined") => { println!("Vector search index already exists for token_analytics collection"); } Err(e) => return Err(e.into()), } // Create indexes for market_signals collection println!("Creating indexes for market_signals collection..."); let market_index_options = IndexOptions::builder().build(); let market_index = IndexModel::builder() .keys(mongodb::bson::doc! { "asset_address": 1, "timestamp": 1 }) .options(market_index_options) .build(); match market_signals.create_index(market_index).await { Ok(_) => println!("Created index for market_signals"), Err(e) if e.to_string().contains("already exists") => { println!("Index already exists for market_signals"); } Err(e) => return Err(e.into()), } println!("MongoDB setup completed successfully!"); Ok(()) }
```

# scripts/test_vector_search.rs

```rs
use anyhow::Result; use cainam_core::config::mongodb::MongoConfig; use cainam_core::config::mongodb::{MongoDbPool, TokenAnalyticsData, TokenAnalyticsDataExt}; use mongodb::bson::doc; use rig::providers::openai::{Client as OpenAiClient, TEXT_EMBEDDING_3_SMALL}; use std::env; use tracing::info; use tracing_subscriber::fmt; use rig::embeddings::embed::{Embed, TextEmbedder, EmbedError}; // Add a local wrapper for TokenAnalyticsData to bypass the orphan rule. #[derive(serde::Serialize)] struct WrappedTokenAnalyticsData(TokenAnalyticsData); impl From<TokenAnalyticsData> for WrappedTokenAnalyticsData { fn from(data: TokenAnalyticsData) -> Self { WrappedTokenAnalyticsData(data) } } impl Embed for WrappedTokenAnalyticsData { fn embed(&self, embedder: &mut TextEmbedder) -> Result<(), EmbedError> { let text = format!( "Name: {}, Symbol: {}, Address: {}", self.0.token_name, self.0.token_symbol, self.0.token_address ); embedder.embed(text); Ok(()) } } #[tokio::main] async fn main() -> Result<()> { // Initialize tracing (with file and line numbers for easier debugging) fmt() .with_target(false) .with_thread_ids(false) .with_thread_names(false) .with_file(true) .with_line_number(true) .init(); // Load environment variables from .env file dotenvy::dotenv().ok(); info!("Starting vector search test..."); // Initialize MongoDB connection using the configuration from the environment. let config = MongoConfig::from_env(); let pool = MongoDbPool::create_pool(config).await?; // Clear the collection before inserting test data pool.client() .database("cainam") .collection::<TokenAnalyticsData>("token_analytics") .delete_many(doc! {}) .await?; // Initialize the OpenAI client and create an embedding model using TEXT_EMBEDDING_3_SMALL. let openai_api_key = env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY must be set"); let openai_client = OpenAiClient::new(&openai_api_key); let embedding_model = openai_client.embedding_model(TEXT_EMBEDDING_3_SMALL); // Define sample test token data. // Here we leave the embedding vector empty so that insert_token_analytics_documents can generate it. let test_tokens = vec![ WrappedTokenAnalyticsData(TokenAnalyticsData { id: "1".to_string(), token_address: "So11111111111111111111111111111111111111112".to_string(), token_name: "Wrapped SOL".to_string(), token_symbol: "SOL".to_string(), embedding: vec![], }), WrappedTokenAnalyticsData(TokenAnalyticsData { id: "2".to_string(), token_address: "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v".to_string(), token_name: "USD Coin".to_string(), token_symbol: "USDC".to_string(), embedding: vec![], }), WrappedTokenAnalyticsData(TokenAnalyticsData { id: "3".to_string(), token_address: "DezXAZ8z7PnrnRJjz3wXBoRgixCa6xjnB7YaB1pPB263".to_string(), token_name: "Bonk".to_string(), token_symbol: "BONK".to_string(), embedding: vec![], }), ]; // Insert test token documents. // Because the trait is implemented for MongoDbPool, and pool is an Arc<MongoDbPool>, // we call the methods on &*pool (which dereferences the Arc). info!("Inserting test token data..."); (&*pool) .insert_token_analytics_documents("token_analytics", embedding_model.clone(), test_tokens) .await?; info!("Test data inserted successfully"); // Define the vector search queries. let test_queries = vec![ "Find me a stablecoin", "What's the native token of Solana", "Show me a meme token", ]; // Execute each query and print search results. for query in test_queries { info!("Testing search with query: {}", query); let results = (&*pool) .top_n("token_analytics", embedding_model.clone(), query, 2) .await?; info!("Search results for '{}': {:#?}", query, results); } info!("Vector search test completed successfully"); Ok(()) }
```

# src/_logging.rs

```rs
use serde::Serialize; use std::time::Instant; use tracing::{info, warn, error}; use tracing_subscriber::{fmt, EnvFilter}; use chrono::{DateTime, Utc}; use uuid::Uuid; use anyhow::Result; #[derive(Debug, Serialize)] pub struct RequestLog { pub request_id: String, pub service: String, pub operation: String, pub start_time: DateTime<Utc>, pub duration_ms: u64, pub status: String, pub error: Option<String>, } #[derive(Debug, Serialize)] pub struct MarketMetrics { pub symbol: String, pub price: f64, pub volume_24h: Option<f64>, pub signal_type: Option<String>, pub confidence: Option<f64>, } #[derive(Debug, Serialize)] pub struct MarketSignalLog { pub id: Uuid, pub token_address: String, pub token_symbol: String, pub signal_type: String, pub price: f64, pub price_change_24h: Option<f64>, pub volume_change_24h: Option<f64>, pub confidence: f64, pub risk_score: f64, pub timestamp: DateTime<Utc>, pub created_at: DateTime<Utc> } #[derive(Debug, Serialize)] pub struct PerformanceMetrics { pub operation: String, pub duration_ms: u64, pub success: bool, pub timestamp: DateTime<Utc>, } pub struct RequestLogger { start_time: Instant, request_id: String, service: String, operation: String, } impl RequestLogger { pub fn new(service: &str, operation: &str) -> Self { Self { start_time: Instant::now(), request_id: uuid::Uuid::new_v4().to_string(), service: service.to_string(), operation: operation.to_string(), } } pub fn success(self) { let duration = self.start_time.elapsed(); let log = RequestLog { request_id: self.request_id, service: self.service, operation: self.operation, start_time: Utc::now() - chrono::Duration::from_std(duration).unwrap(), duration_ms: duration.as_millis() as u64, status: "success".to_string(), error: None, }; info!(target: "request", "{}", serde_json::to_string(&log).unwrap()); } pub fn error(self, error_msg: &str) { let duration = self.start_time.elapsed(); let log = RequestLog { request_id: self.request_id, service: self.service, operation: self.operation, start_time: Utc::now() - chrono::Duration::from_std(duration).unwrap(), duration_ms: duration.as_millis() as u64, status: "error".to_string(), error: Some(error_msg.to_string()), }; error!(target: "request", "{}", serde_json::to_string(&log).unwrap()); } } pub fn log_market_metrics(metrics: MarketMetrics) { info!( target: "market_metrics", "{}", serde_json::to_string(&metrics).unwrap() ); } pub fn log_market_signal(signal: MarketSignalLog) { info!( target: "market_signal", "Market signal detected: {}", serde_json::to_string(&signal).unwrap() ); } pub fn log_performance(metrics: PerformanceMetrics) { if metrics.success { info!( target = "performance", "{}", serde_json::to_string(&metrics).unwrap() ); } else { warn!( target = "performance", "{}", serde_json::to_string(&metrics).unwrap() ); } } pub fn init_logging() -> Result<()> { let env_filter = EnvFilter::try_from_default_env() .unwrap_or_else(|_| EnvFilter::new("info")); fmt() .with_env_filter(env_filter) .with_target(false) .with_thread_ids(false) .with_thread_names(false) .with_file(false) .with_line_number(false) .with_level(true) .with_ansi(true) .compact() .init(); info!("Logging initialized"); Ok(()) } #[cfg(test)] mod tests { use super::*; use serde_json::Value; #[test] fn test_request_logger() { let logger = RequestLogger::new("test_service", "test_operation"); logger.success(); // Verify log format would be tested in integration tests } #[test] fn test_market_metrics_serialization() { let metrics = MarketMetrics { symbol: "SOL".to_string(), price: 100.0, volume_24h: Some(1000000.0), signal_type: Some("BUY".to_string()), confidence: Some(0.8), }; let json = serde_json::to_string(&metrics).unwrap(); let parsed: Value = serde_json::from_str(&json).unwrap(); assert_eq!(parsed["symbol"], "SOL"); assert_eq!(parsed["price"], 100.0); assert_eq!(parsed["volume_24h"], 1000000.0); assert_eq!(parsed["signal_type"], "BUY"); assert_eq!(parsed["confidence"], 0.8); } #[test] fn test_performance_metrics_serialization() { let metrics = PerformanceMetrics { operation: "market_analysis".to_string(), duration_ms: 100, success: true, timestamp: Utc::now(), }; let json = serde_json::to_string(&metrics).unwrap(); let parsed: Value = serde_json::from_str(&json).unwrap(); assert_eq!(parsed["operation"], "market_analysis"); assert_eq!(parsed["duration_ms"], 100); assert_eq!(parsed["success"], true); assert!(parsed["timestamp"].is_string()); } }
```

# src/actions/helius/create_webhook.rs

```rs
use crate::SolanaAgentKit; use serde::{Deserialize, Serialize}; #[derive(Deserialize, Serialize)] pub struct HeliusWebhookResponse { pub webhook_url: String, pub webhook_id: String, } pub async fn create_webhook( agent: &SolanaAgentKit, account_addresses: Vec<String>, webhook_url: String, ) -> Result<HeliusWebhookResponse, Box<dyn std::error::Error>> { // Get the Helius API key from the agent's configuration let api_key = match agent.config.helius_api_key.as_ref() { Some(key) => key, None => return Err("Missing Helius API key in agent.config.HELIUS_API_KEY".into()), }; let url = format!("https://api.helius.xyz/v0/webhooks?api-key={}", api_key); let body = serde_json::json!({ "webhookURL": webhook_url, "transactionTypes": ["Any"], "accountAddresses": account_addresses, "webhookType": "enhanced", "txnStatus": "all", }); let client = reqwest::Client::new(); let response = client.post(url).header("Content-Type", "application/json").json(&body).send().await?; let data = response.json::<serde_json::Value>().await?; let webhook_url = data.get("webhookURL").expect("webhookURL field").as_str().expect("webhookURL text"); let webhook_id = data.get("webhookID").expect("webhookID field").as_str().expect("webhookID text"); Ok(HeliusWebhookResponse { webhook_url: webhook_url.to_string(), webhook_id: webhook_id.to_string() }) }
```

# src/actions/helius/delete_webhook.rs

```rs
use crate::SolanaAgentKit; /// Deletes a Helius Webhook by its ID. /// /// # Arguments /// * `agent` - An instance of SolanaAgentKit (with .config.HELIUS_API_KEY) /// * `webhook_id` - The unique ID of the webhook to delete /// /// # Returns /// The response body from the Helius API (which may contain status or other info) pub async fn delete_webhook( agent: &SolanaAgentKit, webhook_id: &str, ) -> Result<serde_json::Value, Box<dyn std::error::Error>> { // Get the Helius API key from the agent's configuration let api_key = match agent.config.helius_api_key.as_ref() { Some(key) => key, None => return Err("Missing Helius API key in agent.config.HELIUS_API_KEY".into()), }; // Construct the URL for the DELETE request let url = format!("https://api.helius.xyz/v0/webhooks/{}?api-key={}", webhook_id, api_key); // Create an HTTP client let client = reqwest::Client::new(); // Send the DELETE request let response = client.delete(&url).header("Content-Type", "application/json").send().await?; // Check if the request was successful if !response.status().is_success() { return Err(format!( "Failed to delete webhook: {} {}", response.status(), response.status().canonical_reason().unwrap_or("Unknown") ) .into()); } // Handle different response status codes if response.status().as_u16() == 204 { return Ok(serde_json::json!({"message": "Webhook deleted successfully (no content returned)"})); } // Check if the response body is empty let content_length = response.headers().get("Content-Length"); if content_length.is_none() || content_length.expect("HeaderValue").to_str()? == "0" { return Ok(serde_json::json!({"message": "Webhook deleted successfully (empty body)"})); } // Parse the response body as JSON let data: serde_json::Value = response.json().await?; Ok(data) }
```

# src/actions/helius/get_assets_by_owner.rs

```rs
use crate::SolanaAgentKit; use serde_json::json; pub async fn get_assets_by_owner( agent: &SolanaAgentKit, owner_public_key: &str, limit: u32, ) -> Result<serde_json::Value, Box<dyn std::error::Error>> { // Get the Helius API key from the agent's configuration let api_key = match agent.config.helius_api_key.as_ref() { Some(key) => key, None => return Err("Missing Helius API key in agent.config.HELIUS_API_KEY".into()), }; let url = format!("https://mainnet.helius-rpc.com/?api-key={}", api_key); let client = reqwest::Client::new(); let request_body = json!({ "jsonrpc": "2.0", "id": "get-assets", "method": "getAssetsByOwner", "params": json!({ "ownerAddress": owner_public_key, "page": 3, "limit": limit, "displayOptions": { "showFungible": true }, }), }); let response = client.post(&url).header("Content-Type", "application/json").json(&request_body).send().await?; if !response.status().is_success() { return Err(format!( "Failed to fetch: {} - {}", response.status(), response.status().canonical_reason().unwrap_or("Unknown") ) .into()); } let data: serde_json::Value = response.json().await?; Ok(data) }
```

# src/actions/helius/get_webhook.rs

```rs
use crate::SolanaAgentKit; use serde::{Deserialize, Serialize}; #[derive(Debug, Serialize, Deserialize)] pub struct HeliusWebhookIdResponse { pub wallet: String, pub webhook_url: String, pub transaction_types: Vec<String>, pub account_addresses: Vec<String>, pub webhook_type: String, } /// Retrieves a Helius Webhook by ID, returning only the specified fields. /// /// # Arguments /// * `agent` - An instance of SolanaAgentKit (with .config.HELIUS_API_KEY) /// * `webhook_id` - The unique ID of the webhook to delete /// /// # Returns /// A HeliusWebhook object containing { wallet, webhookURL, transactionTypes, accountAddresses, webhookType } pub async fn get_webhook( agent: &SolanaAgentKit, webhook_id: &str, ) -> Result<HeliusWebhookIdResponse, Box<dyn std::error::Error>> { // Get the Helius API key from the agent's configuration let api_key = match agent.config.helius_api_key.as_ref() { Some(key) => key, None => return Err("Missing Helius API key in agent.config.HELIUS_API_KEY".into()), }; let client = reqwest::Client::new(); let url = format!("https://api.helius.xyz/v0/webhooks/{}?api-key={}", webhook_id, api_key); let response = client.get(url).header("Content-Type", "application/json").send().await?; let data = response.json::<HeliusWebhookIdResponse>().await?; Ok(data) }
```

# src/actions/helius/mod.rs

```rs
mod create_webhook; pub use create_webhook::{create_webhook, HeliusWebhookResponse}; mod delete_webhook; pub use delete_webhook::delete_webhook; mod get_webhook; pub use get_webhook::{get_webhook, HeliusWebhookIdResponse}; mod transaction_parsing; pub use transaction_parsing::transaction_parse; mod get_assets_by_owner; pub use get_assets_by_owner::get_assets_by_owner;
```

# src/actions/helius/transaction_parsing.rs

```rs
use crate::SolanaAgentKit; use serde::{Deserialize, Serialize}; use serde_json::json; #[derive(Debug, Serialize, Deserialize)] pub struct HeliusWebhookIdResponse { pub wallet: String, pub webhook_url: String, pub transaction_types: Vec<String>, pub account_addresses: Vec<String>, pub webhook_type: String, } /// Parse a Solana transaction using the Helius Enhanced Transactions API /// /// # Arguments /// * `agent` - An instance of SolanaAgentKit (with .config.HELIUS_API_KEY) /// * `transaction_id` - The transaction ID to parse /// /// # Returns /// Parsed transaction data pub async fn transaction_parse( agent: &SolanaAgentKit, transaction_id: &str, ) -> Result<serde_json::Value, Box<dyn std::error::Error>> { // Get the Helius API key from the agent's configuration let api_key = match agent.config.helius_api_key.as_ref() { Some(key) => key, None => return Err("Missing Helius API key in agent.config.HELIUS_API_KEY".into()), }; let client = reqwest::Client::new(); let url = format!("https://api.helius.xyz/v0/transactions/?api-key={}", api_key); let body = json!( { "transactions": vec![transaction_id.to_string()], }); let response = client.post(url).header("Content-Type", "application/json").json(&body).send().await?; let data = response.json().await?; Ok(data) }
```

# src/actions/solana/close_empty_token_accounts.rs

```rs
use crate::{primitives::USDC, SolanaAgentKit}; use solana_client::rpc_request::TokenAccountsFilter; use solana_sdk::{instruction::Instruction, pubkey::Pubkey, transaction::Transaction}; use spl_token::instruction::close_account; use serde::{Deserialize, Serialize}; #[derive(serde::Deserialize)] pub struct Parsed { pub info: SplToken, } #[derive(serde::Deserialize)] pub struct SplToken { pub mint: String, #[serde(rename(deserialize = "tokenAmount"))] pub token_amount: Amount, } #[allow(dead_code)] #[derive(serde::Deserialize)] pub struct Amount { pub amount: String, #[serde(rename(deserialize = "uiAmountString"))] ui_amount_string: String, #[serde(rename(deserialize = "uiAmount"))] pub ui_amount: f64, pub decimals: u8, } #[derive(Serialize, Deserialize, Debug, Default)] pub struct CloseEmptyTokenAccountsData { pub signature: String, pub closed_size: usize, } impl CloseEmptyTokenAccountsData { pub fn new(signature: String, closed_size: usize) -> Self { CloseEmptyTokenAccountsData { signature, closed_size } } } /// Close Empty SPL Token accounts of the agent. /// /// # Parameters /// /// - `agent`: An instance of `SolanaAgentKit`. /// /// # Returns /// /// Transaction signature and total number of accounts closed or an error if the account doesn't exist. pub async fn close_empty_token_accounts( agent: &SolanaAgentKit, ) -> Result<CloseEmptyTokenAccountsData, Box<dyn std::error::Error>> { let max_instructions = 40_u32; let mut transaction: Vec<Instruction> = vec![]; let mut closed_size = 0; let token_programs = vec![spl_token::ID, spl_token_2022::ID]; for token_program in token_programs { let accounts = agent .connection .get_token_accounts_by_owner( &agent.wallet.address, TokenAccountsFilter::ProgramId(token_program.to_owned()), ) .expect("get_token_accounts_by_owner"); closed_size += accounts.len(); for account in accounts { if transaction.len() >= max_instructions as usize { break; } if let solana_account_decoder::UiAccountData::Json(d) = &account.account.data { if let Ok(parsed) = serde_json::from_value::<Parsed>(d.parsed.clone()) { if parsed.info.token_amount.amount.parse::<u32>().unwrap_or_default() == 0_u32 && parsed.info.mint != USDC { let account_pubkey = Pubkey::from_str_const(&account.pubkey); if let Ok(instruct) = close_account( &token_program, &account_pubkey, &agent.wallet.address, &agent.wallet.address, &[&agent.wallet.address], ) { transaction.push(instruct); } } } } } } if transaction.is_empty() { return Ok(CloseEmptyTokenAccountsData::default()); } // Create and send transaction let recent_blockhash = agent.connection.get_latest_blockhash()?; let transaction = Transaction::new_signed_with_payer( &transaction, Some(&agent.wallet.address), &[&agent.wallet.wallet], recent_blockhash, ); let signature = agent.connection.send_and_confirm_transaction(&transaction)?; let data = CloseEmptyTokenAccountsData::new(signature.to_string(), closed_size); Ok(data) }
```

# src/actions/solana/get_balance.rs

```rs
use crate::SolanaAgentKit; use solana_client::client_error::ClientError; use solana_sdk::{native_token::LAMPORTS_PER_SOL, pubkey::Pubkey}; use std::str::FromStr; /// Gets the balance of SOL or an SPL token for the agent's wallet. /// /// # Parameters /// /// - `agent`: An instance of `SolanaAgentKit`. /// - `token_address`: An optional SPL token mint address. If not provided, returns the SOL balance. /// /// # Returns /// /// A `Result` that resolves to the balance as a number (in UI units) or an error if the account doesn't exist. pub async fn get_balance(agent: &SolanaAgentKit, token_address: Option<String>) -> Result<f64, ClientError> { if let Some(token_address) = token_address { // Get SPL token account balance if let Ok(pubkey) = Pubkey::from_str(&token_address) { let token_account = agent.connection.get_token_account_balance(&pubkey)?; let ui_amount = token_account.ui_amount.unwrap_or(0.0); return Ok(ui_amount); } } // Get SOL balance let balance = agent.connection.get_balance(&agent.wallet.address)?; Ok(balance as f64 / LAMPORTS_PER_SOL as f64) }
```

# src/actions/solana/get_tps.rs

```rs
use crate::SolanaAgentKit; use solana_client::client_error::ClientError; /// Gets the transactions per second (TPS) from the Solana network. /// /// # Parameters /// /// - `agent`: An instance of `SolanaAgentKit` that connects to the Solana cluster. /// /// # Returns /// /// A `Result` containing the TPS as a `f64`, or an error if fetching performance samples fails. pub async fn get_tps(agent: &SolanaAgentKit) -> Result<f64, ClientError> { // Fetch recent performance samples let limit = 1; let perf_samples = agent.connection.get_recent_performance_samples(Some(limit))?; // Check if there are any samples available if !perf_samples.is_empty() { // Calculate TPS let num_transactions = perf_samples[0].num_transactions; let sample_period_secs = perf_samples[0].sample_period_secs; let tps = num_transactions as f64 / sample_period_secs as f64; return Ok(tps); } Ok(0.0) }
```

# src/actions/solana/mod.rs

```rs
mod close_empty_token_accounts; pub use close_empty_token_accounts::{close_empty_token_accounts, CloseEmptyTokenAccountsData}; mod get_balance; pub use get_balance::get_balance; mod request_faucet_funds; pub use request_faucet_funds::request_faucet_funds; mod get_tps; pub use get_tps::get_tps; mod transfer; pub use transfer::transfer;
```

# src/actions/solana/request_faucet_funds.rs

```rs
use crate::SolanaAgentKit; use solana_client::client_error::ClientError; use solana_sdk::native_token::LAMPORTS_PER_SOL; /// Requests SOL from the Solana faucet (devnet/testnet only). /// /// # Parameters /// /// - `agent`: An instance of `SolanaAgentKit`. /// /// # Returns /// /// A transaction signature as a `String`. /// /// # Errors /// /// Returns an error if the request fails or times out. pub async fn request_faucet_funds(agent: &SolanaAgentKit) -> Result<String, ClientError> { // Request airdrop of 5 SOL (5 * LAMPORTS_PER_SOL) let tx = agent.connection.request_airdrop(&agent.wallet.address, 5 * LAMPORTS_PER_SOL)?; // Confirm the transaction agent.connection.confirm_transaction(&tx)?; Ok(tx.to_string()) }
```

# src/actions/solana/transfer.rs

```rs
use crate::SolanaAgentKit; use solana_client::client_error::ClientError; use solana_sdk::{program_pack::Pack, pubkey::Pubkey, system_instruction, transaction::Transaction}; use spl_associated_token_account::get_associated_token_address; use spl_token::{instruction::transfer as transfer_instruct, state::Mint}; /// Transfer SOL or SPL tokens to a recipient /// /// `agent` - SolanaAgentKit instance /// `to` - Recipient's public key /// `amount` - Amount to transfer /// `mint` - Optional mint address for SPL tokens /// /// Returns the transaction signature. pub async fn transfer( agent: &SolanaAgentKit, to: &str, amount: u64, mint: Option<String>, ) -> Result<String, ClientError> { match mint { Some(mint) => { // Transfer SPL Token let mint = Pubkey::from_str_const(&mint); let to = Pubkey::from_str_const(to); let from_ata = get_associated_token_address(&mint, &agent.wallet.address); let to_ata = get_associated_token_address(&mint, &to); let account_info = &agent.connection.get_account(&mint).expect("get_account"); let mint_info = Mint::unpack_from_slice(&account_info.data).expect("unpack_from_slice"); let adjusted_amount = amount * 10u64.pow(mint_info.decimals as u32); let transfer_instruction = transfer_instruct( &spl_token::id(), &from_ata, &to_ata, &from_ata, &[&agent.wallet.address], adjusted_amount, ) .expect("transfer_instruct"); let transaction = Transaction::new_signed_with_payer( &[transfer_instruction], Some(&agent.wallet.address), &[&agent.wallet.wallet], agent.connection.get_latest_blockhash().expect("new_signed_with_payer"), ); let signature = agent.connection.send_and_confirm_transaction(&transaction).expect("send_and_confirm_transaction"); Ok(signature.to_string()) } None => { let transfer_instruction = system_instruction::transfer(&agent.wallet.address, &Pubkey::from_str_const(to), amount); let transaction = Transaction::new_signed_with_payer( &[transfer_instruction], Some(&agent.wallet.address), &[&agent.wallet.wallet], agent.connection.get_latest_blockhash().expect("get_latest_blockhash"), ); let signature = agent.connection.send_and_confirm_transaction(&transaction).expect("send_and_confirm_transaction"); Ok(signature.to_string()) } } }
```

# src/agent/analyst.rs

```rs
use crate::config::mongodb::MongoDbPool; use crate::models::market_signal::MarketSignal; use crate::services::token_analytics::TokenAnalyticsService; use anyhow::Result; use bson::DateTime; use chrono::{Duration, TimeZone, Utc}; use std::sync::Arc; use thiserror::Error; #[derive(Error, Debug)] pub enum Error { #[error("MongoDB error: {0}")] Mongo(#[from] mongodb::error::Error), #[error("Other error: {0}")] Other(String), } pub struct AnalystAgent { analytics_service: Arc<TokenAnalyticsService>, db: Arc<MongoDbPool>, } impl AnalystAgent { pub fn new(analytics_service: Arc<TokenAnalyticsService>, db: Arc<MongoDbPool>) -> Self { Self { analytics_service, db, } } pub async fn analyze_token(&self, symbol: &str, address: &str) -> Result<Option<MarketSignal>> { // First fetch and store current token info let analytics = self .analytics_service .fetch_and_store_token_info(symbol, address) .await .map_err(|e| anyhow::anyhow!(e))?; // Get historical data for analysis let now = DateTime::now(); let timestamp_millis = now.timestamp_millis(); let chrono_now = Utc.timestamp_millis_opt(timestamp_millis).unwrap(); let start_time_chrono = chrono_now - Duration::days(7); let new_timestamp_millis = start_time_chrono.timestamp_millis(); let start_time = DateTime::from_millis(new_timestamp_millis); // let start_time = DateTime::now() - chrono::Duration::days(7); let end_time = DateTime::now(); let _history = self .analytics_service .get_token_history(address, start_time, end_time, 100, 0) .await .map_err(|e| anyhow::anyhow!(e))?; // Get latest analytics for comparison let latest = self .analytics_service .get_latest_token_analytics(address) .await .map_err(|e| anyhow::anyhow!(e))?; if let Some(latest) = latest { // Calculate volume change if let Some(current_volume) = analytics.volume_24h.clone() { if let Some(_volume_change) = self .analytics_service .calculate_volume_change(&current_volume, &latest) { // Generate market signals based on the analysis return self .analytics_service .generate_market_signals(&analytics) .await .map_err(|e| anyhow::anyhow!(e)); } } } Ok(None) } // async fn store_analysis(&self, analysis: &Analysis) -> Result<(), Error> { // let collection = self.db.database("cainam").collection("market_analysis"); // collection // .insert_one(analysis, None) // .await // .map_err(|e| Error::Mongo(e))?; // Ok(()) // } } // #[cfg(test)] // mod tests { // use super::*; // use crate::birdeye::{BirdeyeApi, MockBirdeyeApi, TokenInfo}; // use crate::config::MarketConfig; // use rig_mongodb::MongoDbPool; // async fn setup_test_db() -> Arc<MongoDbPool> { // MongoDbPool::new_from_uri("mongodb://localhost:32770", "cainam_test") // .await // .expect("Failed to create test database pool") // .into() // } // fn setup_mock_birdeye() -> (Arc<dyn BirdeyeApi>, Arc<BirdeyeClient>) { // let mut mock = MockBirdeyeApi::new(); // mock.expect_get_token_info().returning(|_| { // Ok(TokenInfo { // price: 100.0, // volume_24h: 1000000.0, // price_change_24h: 5.0, // liquidity: 500000.0, // trade_24h: 1000, // }) // }); // ( // Arc::new(mock), // Arc::new(BirdeyeClient::new("test_key".to_string())), // ) // } // #[tokio::test] // async fn test_analyze_token() -> Result<()> { // let db = setup_test_db().await; // let (birdeye, birdeye_extended) = setup_mock_birdeye(); // let market_config = MarketConfig::default(); // let analytics_service = Arc::new(TokenAnalyticsService::new( // db, // birdeye, // birdeye_extended, // Some(market_config), // )); // let analyst = AnalystAgent::new(analytics_service); // let signal = analyst.analyze_token("SOL", "test_address").await?; // assert!(signal.is_some()); // Ok(()) // } // }
```

# src/agent/mod.rs

```rs
pub mod trader; // pub mod risk_manager; // pub mod portfolio_optimizer; pub mod analyst; use serde::{Deserialize, Serialize}; use std::time::Duration; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct AgentConfig { pub openai_api_key: String, pub birdeye_api_key: String, pub twitter_email: String, pub twitter_username: String, pub twitter_password: String, pub analysis_interval: Duration, pub trade_min_confidence: f64, pub trade_max_amount: f64, } impl Default for AgentConfig { fn default() -> Self { Self { openai_api_key: String::new(), birdeye_api_key: String::new(), twitter_email: String::new(), twitter_username: String::new(), twitter_password: String::new(), analysis_interval: Duration::from_secs(300), // 5 minutes trade_min_confidence: 0.7, trade_max_amount: 1000.0, } } } // Re-export common types
```

# src/agent/portfolio_optimizer.rs

```rs
use anyhow::Result; use bigdecimal::{BigDecimal, ToPrimitive}; use crate::models::market_signal::MarketSignal; use crate::models::token_analytics::TokenAnalytics; use crate::utils::f64_to_decimal; use std::sync::Arc; use rig_mongodb::{MongoDbPool, bson::doc}; use crate::error::Error; use crate::models::allocation::Allocation; pub struct PortfolioOptimizer { db: Arc<MongoDbPool>, } impl PortfolioOptimizer { pub fn new(db: Arc<MongoDbPool>) -> Self { Self { db } } pub async fn get_allocation(&self, _token: &TokenAnalytics, _signal: &MarketSignal) -> Result<BigDecimal> { // For now, return a default allocation Ok(f64_to_decimal(0.1)) // 10% allocation } pub async fn get_position_allocation(&self, address: &str) -> Result<BigDecimal> { let collection = self.db.collection("allocations"); let filter = doc! { "token_address": address, }; let doc = collection.find_one(filter, None) .await?; let allocation = doc .and_then(|d| d.get_f64("allocation")) .unwrap_or(0.0); Ok(f64_to_decimal(allocation)) } async fn get_allocation(&self, token_address: &str) -> Result<Option<Allocation>, Error> { let collection = self.db.database("cainam").collection("allocations"); let filter = doc! { "token_address": token_address, }; collection.find_one(filter, None) .await .map_err(|e| Error::Database(e.to_string())) } }
```

# src/agent/risk_manager.rs

```rs
use anyhow::Result; use crate::models::market_signal::MarketSignal; use crate::utils::{decimal_to_f64, f64_to_decimal}; use std::sync::Arc; use rig_mongodb::MongoDbPool; pub struct RiskManagerAgent { db: Arc<MongoDbPool>, max_position_size: f64, max_drawdown: f64, } impl RiskManagerAgent { pub fn new(db: Arc<MongoDbPool>, max_position_size: f64, max_drawdown: f64) -> Self { Self { db, max_position_size, max_drawdown, } } pub async fn validate_trade(&self, signal: &MarketSignal) -> Result<bool> { // TODO: Implement risk validation logic // - Check current exposure // - Validate against max drawdown // - Check correlation with existing positions // - Verify position sizing let min_confidence = f64_to_decimal(0.5); let max_risk = f64_to_decimal(0.7); if signal.confidence < min_confidence || signal.risk_score > max_risk { return Ok(false); } Ok(true) } pub async fn calculate_position_size(&self, signal: &MarketSignal) -> Result<f64> { // Calculate optimal position size based on: // - Current portfolio value // - Risk metrics // - Signal confidence let max_size = f64_to_decimal(self.max_position_size); let base_size = max_size.clone() * signal.confidence.clone(); let one = f64_to_decimal(1.0); let risk_factor = one - signal.risk_score.clone(); let risk_adjusted_size = base_size * risk_factor; Ok(decimal_to_f64(&risk_adjusted_size.min(max_size))) } }
```

# src/agent/trader.rs

```rs
// use crate::models::trade::Trade; use crate::{ birdeye::BirdeyeClient, config::mongodb::MongoDbPool, config::AgentConfig, config::MarketConfig, error::{AgentError, AgentResult}, models::market_signal::{MarketSignal, SignalType}, services::token_analytics::TokenAnalyticsService, trading::trading_engine::TradingEngine, trading::SolanaAgentKit, utils::f64_to_decimal, }; use bigdecimal::BigDecimal; use rig::{ agent::Agent, providers::openai::{Client as OpenAIClient, CompletionModel}, }; use std::sync::atomic::{AtomicBool, Ordering}; use std::sync::Arc; use tokio::time::sleep; use tracing::{error, info}; const MAX_RETRIES: u32 = 3; const RETRY_DELAY: u64 = 1000; // 1 second pub struct TradingAgent { agent: Agent<CompletionModel>, trading_engine: TradingEngine, analytics_service: Arc<TokenAnalyticsService>, config: AgentConfig, running: Arc<AtomicBool>, db: Arc<MongoDbPool>, } impl TradingAgent { pub async fn new( config: AgentConfig, db: Arc<MongoDbPool>, solana_agent: SolanaAgentKit, ) -> AgentResult<Self> { info!("Initializing TradingAgent..."); // Initialize OpenAI client let openai_client = OpenAIClient::new(&config.openai_api_key); info!("Creating GPT-4 agent..."); let agent = openai_client .agent(crate::config::get_openai_model()) .preamble(include_str!("../prompts/system.txt")) .build(); // Initialize components let trading_engine = TradingEngine::new( config.trade_min_confidence, config.trade_max_amount, solana_agent, ); // info!("Initializing Twitter client..."); // let mut twitter_client = TwitterClient::new( // config.twitter_email.clone(), // config.twitter_username.clone(), // config.twitter_password.clone(), // ); // // Retry Twitter login with exponential backoff // let mut retry_count = 0; // loop { // match twitter_client.login().await { // Ok(_) => { // info!("Successfully logged in to Twitter"); // break; // } // Err(e) => { // retry_count += 1; // if retry_count >= MAX_RETRIES { // error!("Failed to login to Twitter after {} attempts", MAX_RETRIES); // return Err(AgentError::TwitterApi(format!("Login failed: {}", e))); // } // warn!( // "Failed to login to Twitter (attempt {}), retrying...", // retry_count // ); // sleep(Duration::from_millis(RETRY_DELAY * 2u64.pow(retry_count))).await; // } // } // } info!("Initializing Birdeye clients..."); let birdeye = Arc::new(BirdeyeClient::new(config.birdeye_api_key.clone())); let birdeye_extended = Arc::new(BirdeyeClient::new(config.birdeye_api_key.clone())); // Initialize market config let market_config = MarketConfig::new_from_env()?; // Initialize analytics service let analytics_service = Arc::new(TokenAnalyticsService::new(db.clone(), birdeye, Some(market_config)).await?); Ok(Self { agent, trading_engine, analytics_service, config, running: Arc::new(AtomicBool::new(false)), db, }) } // async fn store_trade(&self, trade: &Trade) -> Result<(), Error> { // let collection = self.db.database("cainam").collection("trades"); // collection // .insert_one(trade) // .await // .map_err(|e| Error::Mongo(e))?; // Ok(()) // } pub async fn analyze_market( &self, symbol: &str, address: &str, ) -> AgentResult<Option<MarketSignal>> { info!("Starting market analysis for {}", symbol); // Fetch and store token analytics let analytics = self .analytics_service .fetch_and_store_token_info(symbol, address) .await .map_err(|e| { AgentError::MarketAnalysis(format!("Failed to fetch token info: {}", e)) })?; info!("Market Analysis for {}:", symbol); info!("Current Price: ${:.4}", analytics.price); if let Some(ref volume) = analytics.volume_24h { info!("24h Volume: ${:.2}", volume); } // Generate market signals let signal = self .analytics_service .generate_market_signals(&analytics) .await .map_err(|e| { AgentError::MarketAnalysis(format!("Failed to generate signals: {}", e)) })?; if let Some(signal) = &signal { info!( "Market signal generated: {:?} (confidence: {:.2})", signal.signal_type, signal.confidence ); } Ok(signal) } pub async fn process_signal(&self, signal: &MarketSignal) -> AgentResult<Option<String>> { let zero = BigDecimal::from(0); let action = match signal.signal_type { SignalType::PriceSpike if signal.price > zero => "BUY", SignalType::StrongBuy => "BUY", SignalType::Buy => "BUY", SignalType::VolumeSurge if signal.volume_change > zero => "BUY", SignalType::PriceDrop => "SELL", SignalType::StrongSell => "SELL", SignalType::Sell => "SELL", SignalType::Hold => "HOLD", _ => return Ok(None), }; // Convert f64 config values to BigDecimal let threshold = f64_to_decimal(self.config.trade_min_confidence); let max_amount = f64_to_decimal(self.config.trade_max_amount); if signal.confidence >= threshold { let amount = (max_amount.clone() * signal.confidence.clone()).min(max_amount.clone()); match action { "BUY" | "SELL" => { info!( "Executing {} trade for {} with amount {}", action, signal.asset_address, amount ); self.trading_engine .execute_trade(signal) .await .map_err(|e| { AgentError::Trading(format!("Trade execution failed: {}", e)) })?; } _ => {} } } Ok(Some(action.to_string())) } pub async fn execute_trade(&self, _symbol: &str, signal: &MarketSignal) -> AgentResult<String> { self.trading_engine .execute_trade(signal) .await .map_err(|e| AgentError::Trading(format!("Trade execution failed: {}", e))) } pub async fn post_trade_update( &self, _symbol: &str, _action: &str, _amount: f64, _signal_type: &SignalType, ) -> AgentResult<()> { // TODO: Implement post-trade updates // - Update portfolio state // - Log trade details // - Send notifications Ok(()) } pub async fn run(&self) -> AgentResult<()> { info!("Starting trading agent..."); self.running.store(true, Ordering::SeqCst); let tokens = [ ("SOL", "So11111111111111111111111111111111111111112"), ("BONK", "DezXAZ8z7PnrnRJjz3wXBoRgixCa6xjnB7YaB1pPB263"), ]; while self.running.load(Ordering::SeqCst) { for (symbol, address) in tokens.iter() { match self.analyze_market(symbol, address).await { Ok(Some(signal)) => { let min_confidence = f64_to_decimal(self.config.trade_min_confidence); if signal.confidence >= min_confidence { if let Err(e) = self.process_signal(&signal).await { error!("Error processing signal: {}", e); } } else { info!("Signal confidence too low for trading"); } } Ok(None) => { info!("No trading signals generated"); } Err(e) => { error!("Market analysis failed for {}: {}", symbol, e); } } } info!( "Waiting for next analysis interval ({:?})...", self.config.analysis_interval ); sleep(self.config.analysis_interval).await; info!("Starting next analysis cycle"); } info!("Trading agent stopped"); Ok(()) } pub fn stop(&self) { info!("Stopping trading agent..."); self.running.store(false, Ordering::SeqCst); } } // #[cfg(test)] // mod tests { // use super::*; // use crate::birdeye::MockBirdeyeApi; // use crate::twitter::MockTwitterApi; // async fn setup_test_db() -> Arc<MongoDbPool> { // MongoDbPool::new_from_uri("mongodb://localhost:32770", "cainam_test") // .await // .expect("Failed to create test database pool") // .into() // } // async fn setup_mocks() -> (Box<MockTwitterApi>, Box<MockBirdeyeApi>) { // let mut twitter_mock = Box::new(MockTwitterApi::new()); // twitter_mock // .expect_login() // .times(1) // .returning(|| Box::pin(async { Ok(()) })); // let mut birdeye_mock = Box::new(MockBirdeyeApi::new()); // birdeye_mock.expect_get_token_info().returning(|_| { // Box::pin(async { // Ok(crate::birdeye::TokenInfo { // price: 100.0, // volume_24h: 1000000.0, // price_change_24h: 5.0, // liquidity: 500000.0, // trade_24h: 1000, // }) // }) // }); // (twitter_mock, birdeye_mock) // } // #[tokio::test] // async fn test_market_analysis() -> AgentResult<()> { // let db = setup_test_db().await; // let solana_agent = SolanaAgentKit::new_from_env()?; // let config = AgentConfig::new_from_env()?; // let agent = TradingAgent::new(config, db, solana_agent).await?; // let signal = agent // .analyze_market("SOL", "So11111111111111111111111111111111111111112") // .await?; // assert!(signal.is_some()); // Ok(()) // } // }
```

# src/birdeye/mod.rs

```rs
use crate::error::{AgentError, AgentResult}; use async_trait::async_trait; use reqwest::Client; use serde::{Deserialize, Serialize}; use std::sync::atomic::{AtomicU64, Ordering}; use std::sync::Arc; use tokio::time::{sleep, Duration}; const BIRDEYE_API_BASE: &str = "https://public-api.birdeye.so"; const RATE_LIMIT_DELAY: u64 = 500; // 500ms between requests // Common token addresses const TOKEN_ADDRESSES: &[(&str, &str)] = &[ ("SOL", "So11111111111111111111111111111111111111112"), ("USDC", "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v"), ("BONK", "DezXAZ8z7PnrnRJjz3wXBoRgixCa6xjnB7YaB1pPB263"), ]; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TokenInfo { pub price: f64, pub volume_24h: f64, pub price_change_24h: f64, pub liquidity: f64, pub trade_24h: i64, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TokenOverview { pub name: String, pub market_cap: f64, pub total_supply: f64, pub circulating_supply: Option<f64>, } #[derive(Debug, Serialize, Deserialize)] struct TokenMarketResponse { success: bool, data: TokenMarketData, } #[derive(Debug, Serialize, Deserialize)] struct TokenMarketData { address: String, price: f64, volume_24h: f64, decimals: u8, price_sol: f64, market_cap: f64, fully_diluted_market_cap: Option<f64>, circulating_supply: Option<f64>, total_supply: Option<f64>, price_change_24h: Option<f64>, volume_change_24h: Option<f64>, } impl Default for TokenMarketData { fn default() -> Self { Self { address: String::new(), price: 0.0, volume_24h: 0.0, decimals: 0, price_sol: 0.0, market_cap: 0.0, fully_diluted_market_cap: None, circulating_supply: None, total_supply: None, price_change_24h: None, volume_change_24h: None, } } } #[cfg_attr(test, mockall::automock)] #[async_trait] pub trait BirdeyeApi: Send + Sync { async fn get_token_info(&self, symbol: &str) -> AgentResult<TokenInfo>; async fn get_token_info_by_address(&self, address: &str) -> AgentResult<TokenInfo>; } pub struct BirdeyeClient { client: Client, api_key: String, last_request: Arc<AtomicU64>, } impl BirdeyeClient { pub fn new(api_key: String) -> Self { Self { client: Client::builder() .timeout(Duration::from_secs(10)) .build() .unwrap_or_else(|_| Client::new()), api_key, last_request: Arc::new(AtomicU64::new(0)), } } fn get_token_address(symbol: &str) -> AgentResult<&'static str> { let symbol = symbol.trim_start_matches('$').to_uppercase(); TOKEN_ADDRESSES .iter() .find(|(s, _)| *s == symbol) .map(|(_, addr)| *addr) .ok_or_else(|| AgentError::InvalidInput(format!("Unknown token symbol: {}", symbol))) } async fn rate_limit(&self) { let now = std::time::SystemTime::now() .duration_since(std::time::UNIX_EPOCH) .unwrap() .as_millis() as u64; let last = self.last_request.load(Ordering::SeqCst); let elapsed = now.saturating_sub(last); if elapsed < RATE_LIMIT_DELAY { sleep(Duration::from_millis(RATE_LIMIT_DELAY - elapsed)).await; } self.last_request.store(now, Ordering::SeqCst); } async fn get_market_data(&self, address: &str) -> AgentResult<TokenMarketResponse> { self.rate_limit().await; let url = format!( "{}/v2/tokens/token_data?address={}", BIRDEYE_API_BASE, address ); tracing::info!("Requesting Birdeye data: {}", url); let response = self .client .get(&url) .header("X-API-KEY", &self.api_key) .send() .await .map_err(|e| AgentError::ApiError(format!("Birdeye request failed: {}", e)))?; let status = response.status(); let text = response .text() .await .map_err(|e| AgentError::ApiError(format!("Failed to read Birdeye response: {}", e)))?; tracing::info!("Birdeye response status: {}", status); tracing::info!("Birdeye response body: {}", text); if !status.is_success() { if let Ok(error_response) = serde_json::from_str::<TokenMarketResponse>(&text) { return Err(AgentError::ApiError(format!( "Token not found: {}", error_response.data.address ))); } return Err(AgentError::ApiError(format!( "HTTP error {}: {}", status, text ))); } serde_json::from_str(&text).map_err(|e| { AgentError::ApiError(format!( "Failed to parse market data: {}\nResponse: {}", e, text )) }) } } #[async_trait] impl BirdeyeApi for BirdeyeClient { async fn get_token_info(&self, symbol: &str) -> AgentResult<TokenInfo> { let address = Self::get_token_address(symbol)?; self.get_token_info_by_address(address).await } async fn get_token_info_by_address(&self, address: &str) -> AgentResult<TokenInfo> { let response = self.get_market_data(address).await?; if !response.success { return Err(AgentError::ApiError(format!( "Birdeye API error for token {}", address ))); } let data = response.data; Ok(TokenInfo { price: data.price, volume_24h: data.volume_24h, price_change_24h: data.price_change_24h.unwrap_or_default(), liquidity: 0.0, // Not available in this endpoint trade_24h: 0, // Not available in this endpoint }) } }
```

# src/character/mod.rs

```rs
use serde::{Deserialize, Serialize}; use std::collections::HashMap; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct Character { pub name: String, pub username: String, pub clients: Vec<String>, pub model_provider: String, pub image_model_provider: String, pub plugins: Vec<String>, pub settings: Settings, pub system: String, pub bio: Vec<String>, pub lore: Vec<String>, pub knowledge: Vec<String>, pub message_examples: Vec<Vec<MessageExample>>, pub post_examples: Vec<String>, pub topics: Vec<String>, pub style: Style, pub adjectives: Vec<String>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct Settings { pub secrets: HashMap<String, String>, pub voice: VoiceSettings, pub rag_knowledge: bool, pub model_config: ModelConfig, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct VoiceSettings { pub model: String, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct ModelConfig { pub temperature: f32, pub max_tokens: u32, pub frequency_penalty: f32, pub presence_penalty: f32, pub top_p: f32, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MessageExample { pub user: String, pub content: MessageContent, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MessageContent { pub text: String, pub action: Option<String>, pub content: Option<serde_json::Value>, } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct Style { pub tone: String, pub writing: String, pub personality: String, pub quirks: Vec<String>, pub all: Vec<String>, pub chat: Vec<String>, pub post: Vec<String>, } impl Character { pub fn load(path: &str) -> anyhow::Result<Self> { let content = std::fs::read_to_string(path)?; let character = serde_json::from_str(&content)?; Ok(character) } pub fn get_system_prompt(&self) -> String { let mut prompt = String::new(); // Add system description prompt.push_str(&self.system); prompt.push_str("\n\n"); // Add style guidelines prompt.push_str("Style Guidelines:\n"); for guideline in &self.style.all { prompt.push_str(&format!("- {}\n", guideline)); } prompt.push_str("\n"); // Add knowledge base summary prompt.push_str("Knowledge Base:\n"); for knowledge in &self.knowledge { prompt.push_str(&format!("- {}\n", knowledge)); } prompt } pub fn get_post_style(&self) -> Vec<String> { self.style.post.clone() } pub fn get_chat_style(&self) -> Vec<String> { self.style.chat.clone() } } #[cfg(test)] mod tests { use super::*; #[test] fn test_character_deserialization() { let json = r#"{ "name": "Vergen", "username": "vergen", "clients": ["direct", "discord", "telegram", "twitter"], "modelProvider": "anthropic", "imageModelProvider": "openai", "plugins": [], "settings": { "secrets": {}, "voice": { "model": "en_US-hfc_male-medium" }, "ragKnowledge": true, "modelConfig": { "temperature": 0.7, "maxTokens": 2048, "frequencyPenalty": 0.0, "presencePenalty": 0.0, "topP": 0.95 } }, "system": "Test system prompt", "bio": ["Test bio"], "lore": ["Test lore"], "knowledge": ["Test knowledge"], "messageExamples": [], "postExamples": [], "topics": ["Test topic"], "style": { "tone": "professional", "writing": "clear", "personality": "confident", "quirks": ["test quirk"], "all": ["test guideline"], "chat": ["test chat style"], "post": ["test post style"] }, "adjectives": ["analytical"] }"#; let character: Character = serde_json::from_str(json).unwrap(); assert_eq!(character.name, "Vergen"); assert_eq!(character.username, "vergen"); } }
```

# src/characteristics/adjectives.rs

```rs
use std::fs; use std::io; use crate::core::characteristics::Characteristic; pub struct Adjectives; impl Characteristic for Adjectives { fn get_header(&self) -> String { "These are the adjectives.".to_string() } fn get_traits(&self, character_name: &str) -> io::Result<String> { let path = format!("./characters/{}/adjectives.txt", character_name); fs::read_to_string(&path) } }
```

# src/characteristics/bio.rs

```rs
use std::fs; use std::io; use crate::core::characteristics::Characteristic; pub struct Bio; impl Characteristic for Bio { fn get_header(&self) -> String { "This is your background.".to_string() } fn get_traits(&self, character_name: &str) -> io::Result<String> { let path = format!("./characters/{}/bio.txt", character_name); fs::read_to_string(&path) } }
```

# src/characteristics/lore.rs

```rs
use std::fs; use std::io; use crate::core::characteristics::Characteristic; pub struct Lore; impl Characteristic for Lore { fn get_header(&self) -> String { "This is your lore.".to_string() } fn get_traits(&self, character_name: &str) -> io::Result<String> { let path = format!("./characters/{}/lore.txt", character_name); fs::read_to_string(&path) } }
```

# src/characteristics/mod.rs

```rs
pub mod adjectives; pub mod bio; pub mod lore; pub mod post_examples; pub mod previous_messages; pub mod topics; pub mod styles;
```

# src/characteristics/post_examples.rs

```rs
use std::fs; use std::io; use crate::core::characteristics::Characteristic; pub struct PostExamples; impl Characteristic for PostExamples { fn get_header(&self) -> String { "These are previous post examples.".to_string() } fn get_traits(&self, character_name: &str) -> io::Result<String> { let path = format!("./characters/{}/post_examples.txt", character_name); fs::read_to_string(&path) } }
```

# src/characteristics/previous_messages.rs

```rs
use std::fs; use std::io; use crate::core::characteristics::Characteristic; pub struct PreviousMessages; impl Characteristic for PreviousMessages { fn get_header(&self) -> String { "These are examples of your previous messages.".to_string() } fn get_traits(&self, character_name: &str) -> io::Result<String> { let path = format!("./characters/{}/previous_messages.txt", character_name); fs::read_to_string(&path) } }
```

# src/characteristics/styles.rs

```rs
use std::fs; use std::io; use crate::core::characteristics::Characteristic; pub struct Styles; impl Characteristic for Styles { fn get_header(&self) -> String { "This is the style you use to talk in".to_string() } fn get_traits(&self, character_name: &str) -> io::Result<String> { let path = format!("./characters/{}/styles.txt", character_name); fs::read_to_string(&path) } }
```

# src/characteristics/topics.rs

```rs
use std::fs; use std::io; use crate::core::characteristics::Characteristic; pub struct Topics; impl Characteristic for Topics { fn get_header(&self) -> String { "These are the topics you should talk about.".to_string() } fn get_traits(&self, character_name: &str) -> io::Result<String> { let path = format!("./characters/{}/topics.txt", character_name); fs::read_to_string(&path) } }
```

# src/clients/twitter.rs

```rs
use anyhow::{Result, anyhow}; use async_trait::async_trait; use reqwest::{Client, cookie::Jar}; use serde::{Deserialize, Serialize}; use std::sync::Arc; use tokio::sync::RwLock; use url::Url; const TWITTER_API_URL: &str = "https://api.twitter.com"; const TWITTER_LOGIN_URL: &str = "https://twitter.com/i/flow/login"; #[derive(Debug, Clone)] pub struct TwitterClient { client: Arc<Client>, session: Arc<RwLock<Option<TwitterSession>>>, } #[derive(Debug, Clone, Serialize, Deserialize)] struct TwitterSession { auth_token: String, csrf_token: String, cookies: Vec<(String, String)>, } #[async_trait] pub trait SocialMediaClient: Send + Sync { async fn post(&self, content: &str) -> Result<String>; async fn reply(&self, parent_id: &str, content: &str) -> Result<String>; async fn delete(&self, post_id: &str) -> Result<()>; } impl TwitterClient { pub fn new() -> Self { let cookie_store = Arc::new(Jar::default()); let client = Client::builder() .cookie_provider(cookie_store.clone()) .build() .unwrap(); Self { client: Arc::new(client), session: Arc::new(RwLock::new(None)), } } pub async fn login(&self, email: &str, username: &str, password: &str) -> Result<()> { // First, get the guest token and initial cookies let guest_token = self.get_guest_token().await?; // Start login flow let flow_token = self.start_login_flow(&guest_token).await?; // Submit username/email let account_flow_token = self.submit_username(&flow_token, username, email).await?; // Submit password let auth_token = self.submit_password(&account_flow_token, password).await?; // Store session let session = TwitterSession { auth_token, csrf_token: self.get_csrf_token().await?, cookies: self.extract_cookies(), }; *self.session.write().await = Some(session); Ok(()) } async fn get_guest_token(&self) -> Result<String> { let response = self.client .post(&format!("{}/1.1/guest/activate.json", TWITTER_API_URL)) .send() .await?; #[derive(Deserialize)] struct GuestToken { guest_token: String, } let token: GuestToken = response.json().await?; Ok(token.guest_token) } async fn start_login_flow(&self, guest_token: &str) -> Result<String> { let response = self.client .get(TWITTER_LOGIN_URL) .header("x-guest-token", guest_token) .send() .await?; // Extract flow_token from response // This is a placeholder - actual implementation would need to parse the HTML/JS Ok("flow_token".to_string()) } async fn submit_username(&self, flow_token: &str, username: &str, email: &str) -> Result<String> { // Submit username/email to the login flow // This is a placeholder - actual implementation would need to handle the specific endpoints Ok("account_flow_token".to_string()) } async fn submit_password(&self, flow_token: &str, password: &str) -> Result<String> { // Submit password and get auth token // This is a placeholder - actual implementation would need to handle the specific endpoints Ok("auth_token".to_string()) } async fn get_csrf_token(&self) -> Result<String> { // Get CSRF token from cookies or make a request to get it Ok("csrf_token".to_string()) } fn extract_cookies(&self) -> Vec<(String, String)> { // Extract relevant cookies from the cookie store vec![] } async fn ensure_authenticated(&self) -> Result<()> { if self.session.read().await.is_none() { return Err(anyhow!("Not authenticated")); } Ok(()) } } #[async_trait] impl SocialMediaClient for TwitterClient { async fn post(&self, content: &str) -> Result<String> { self.ensure_authenticated().await?; let session = self.session.read().await; let session = session.as_ref().unwrap(); let response = self.client .post(&format!("{}/2/tweets", TWITTER_API_URL)) .header("authorization", &format!("Bearer {}", session.auth_token)) .header("x-csrf-token", &session.csrf_token) .json(&serde_json::json!({ "text": content })) .send() .await?; #[derive(Deserialize)] struct TweetResponse { data: TweetData, } #[derive(Deserialize)] struct TweetData { id: String, } let tweet: TweetResponse = response.json().await?; Ok(tweet.data.id) } async fn reply(&self, parent_id: &str, content: &str) -> Result<String> { self.ensure_authenticated().await?; let session = self.session.read().await; let session = session.as_ref().unwrap(); let response = self.client .post(&format!("{}/2/tweets", TWITTER_API_URL)) .header("authorization", &format!("Bearer {}", session.auth_token)) .header("x-csrf-token", &session.csrf_token) .json(&serde_json::json!({ "text": content, "reply": { "in_reply_to_tweet_id": parent_id } })) .send() .await?; #[derive(Deserialize)] struct TweetResponse { data: TweetData, } #[derive(Deserialize)] struct TweetData { id: String, } let tweet: TweetResponse = response.json().await?; Ok(tweet.data.id) } async fn delete(&self, post_id: &str) -> Result<()> { self.ensure_authenticated().await?; let session = self.session.read().await; let session = session.as_ref().unwrap(); self.client .delete(&format!("{}/2/tweets/{}", TWITTER_API_URL, post_id)) .header("authorization", &format!("Bearer {}", session.auth_token)) .header("x-csrf-token", &session.csrf_token) .send() .await?; Ok(()) } }
```

# src/config/agent_config.rs

```rs
use crate::error::{AgentError, AgentResult}; use std::env; use std::time::Duration; #[derive(Debug, Clone)] pub struct AgentConfig { pub openai_api_key: String, pub birdeye_api_key: String, // pub twitter_email: String, // pub twitter_username: String, // pub twitter_password: String, pub analysis_interval: Duration, pub trade_min_confidence: f64, pub trade_max_amount: f64, } impl AgentConfig { /// Creates a new AgentConfig from environment variables with validation pub fn new_from_env() -> AgentResult<Self> { let config = Self { openai_api_key: get_env_var("OPENAI_API_KEY")?, birdeye_api_key: get_env_var("BIRDEYE_API_KEY")?, // twitter_email: get_env_var("TWITTER_EMAIL")?, // twitter_username: get_env_var("TWITTER_USERNAME")?, // twitter_password: get_env_var("TWITTER_PASSWORD")?, analysis_interval: parse_duration_secs("ANALYSIS_INTERVAL", 300)?, trade_min_confidence: parse_f64("TRADE_MIN_CONFIDENCE", 0.7)?, trade_max_amount: parse_f64("TRADE_MAX_AMOUNT", 1000.0)?, }; config.validate()?; Ok(config) } /// Validates the configuration values fn validate(&self) -> AgentResult<()> { // Validate API keys are not empty if self.openai_api_key.is_empty() { return Err(AgentError::Config("OpenAI API key cannot be empty".into())); } if self.birdeye_api_key.is_empty() { return Err(AgentError::Config("Birdeye API key cannot be empty".into())); } // // Validate Twitter credentials // if self.twitter_email.is_empty() || !self.twitter_email.contains('@') { // return Err(AgentError::Config("Invalid Twitter email".into())); // } // if self.twitter_username.is_empty() { // return Err(AgentError::Config("Twitter username cannot be empty".into())); // } // if self.twitter_password.is_empty() { // return Err(AgentError::Config("Twitter password cannot be empty".into())); // } // Validate trading parameters if !(0.0..=1.0).contains(&self.trade_min_confidence) { return Err(AgentError::InvalidConfig( "trade_min_confidence".into(), "must be between 0.0 and 1.0".into(), )); } if self.trade_max_amount <= 0.0 { return Err(AgentError::InvalidConfig( "trade_max_amount".into(), "must be greater than 0".into(), )); } Ok(()) } } /// Helper function to get an environment variable fn get_env_var(key: &str) -> AgentResult<String> { env::var(key).map_err(|_| AgentError::MissingEnvVar(key.to_string())) } /// Helper function to parse a duration from seconds fn parse_duration_secs(key: &str, default: u64) -> AgentResult<Duration> { let secs = env::var(key) .map(|v| v.parse::<u64>()) .unwrap_or(Ok(default)) .map_err(|_| { AgentError::InvalidConfig( key.to_string(), "must be a valid number of seconds".to_string(), ) })?; Ok(Duration::from_secs(secs)) } /// Helper function to parse an f64 value fn parse_f64(key: &str, default: f64) -> AgentResult<f64> { let value = env::var(key) .map(|v| v.parse::<f64>()) .unwrap_or(Ok(default)) .map_err(|_| { AgentError::InvalidConfig(key.to_string(), "must be a valid number".to_string()) })?; Ok(value) } #[cfg(test)] mod tests { use super::*; use std::env; #[test] fn test_config_validation() { // Set required environment variables env::set_var("OPENAI_API_KEY", "test_key"); env::set_var("BIRDEYE_API_KEY", "test_key"); env::set_var("TWITTER_EMAIL", "test@example.com"); env::set_var("TWITTER_USERNAME", "test_user"); env::set_var("TWITTER_PASSWORD", "test_pass"); let config = AgentConfig::new_from_env().unwrap(); assert_eq!(config.trade_min_confidence, 0.7); // Default value assert_eq!(config.trade_max_amount, 1000.0); // Default value // Test invalid confidence env::set_var("TRADE_MIN_CONFIDENCE", "2.0"); assert!(AgentConfig::new_from_env().is_err()); // Test invalid amount env::set_var("TRADE_MAX_AMOUNT", "-100"); assert!(AgentConfig::new_from_env().is_err()); // Test invalid email env::set_var("TWITTER_EMAIL", "invalid_email"); assert!(AgentConfig::new_from_env().is_err()); } }
```

# src/config/market_config.rs

```rs
use crate::error::{AgentError, AgentResult}; use crate::utils::f64_to_decimal; use bigdecimal::BigDecimal; use std::env; #[derive(Debug, Clone)] pub struct MarketConfig { pub price_change_threshold: BigDecimal, pub volume_surge_threshold: BigDecimal, pub base_confidence: BigDecimal, pub price_weight: BigDecimal, pub volume_weight: BigDecimal, } impl MarketConfig { pub fn new_from_env() -> AgentResult<Self> { Ok(Self { price_change_threshold: parse_decimal_env("PRICE_CHANGE_THRESHOLD", 0.05)?, volume_surge_threshold: parse_decimal_env("VOLUME_SURGE_THRESHOLD", 1.0)?, base_confidence: parse_decimal_env("BASE_CONFIDENCE", 0.5)?, price_weight: parse_decimal_env("PRICE_WEIGHT", 0.3)?, volume_weight: parse_decimal_env("VOLUME_WEIGHT", 0.2)?, }) } pub fn validate(&self) -> AgentResult<()> { // Validate thresholds are positive if self.price_change_threshold <= BigDecimal::from(0) { return Err(AgentError::InvalidConfig( "price_change_threshold".into(), "must be greater than 0".into(), )); } if self.volume_surge_threshold <= BigDecimal::from(0) { return Err(AgentError::InvalidConfig( "volume_surge_threshold".into(), "must be greater than 0".into(), )); } // Validate weights sum to less than or equal to 1 let total_weight = &self.price_weight + &self.volume_weight; if total_weight > BigDecimal::from(1) { return Err(AgentError::InvalidConfig( "weights".into(), "sum of weights must not exceed 1.0".into(), )); } Ok(()) } } impl Default for MarketConfig { fn default() -> Self { Self { price_change_threshold: f64_to_decimal(0.05), volume_surge_threshold: f64_to_decimal(1.0), base_confidence: f64_to_decimal(0.5), price_weight: f64_to_decimal(0.3), volume_weight: f64_to_decimal(0.2), } } } fn parse_decimal_env(key: &str, default: f64) -> AgentResult<BigDecimal> { match env::var(key) { Ok(val) => val .parse::<f64>() .map_err(|_| { AgentError::InvalidConfig( key.to_string(), "must be a valid decimal number".to_string(), ) }) .map(f64_to_decimal), Err(_) => Ok(f64_to_decimal(default)), } } #[cfg(test)] mod tests { use super::*; #[test] fn test_market_config_defaults() { let config = MarketConfig::default(); assert_eq!(config.price_change_threshold, f64_to_decimal(0.05)); assert_eq!(config.volume_surge_threshold, f64_to_decimal(1.0)); assert_eq!(config.base_confidence, f64_to_decimal(0.5)); } #[test] fn test_market_config_validation() { // Valid config let config = MarketConfig::default(); assert!(config.validate().is_ok()); // Invalid: negative threshold let mut invalid_config = MarketConfig::default(); invalid_config.price_change_threshold = f64_to_decimal(-0.1); assert!(invalid_config.validate().is_err()); // Invalid: weights sum > 1 let mut invalid_weights = MarketConfig::default(); invalid_weights.price_weight = f64_to_decimal(0.6); invalid_weights.volume_weight = f64_to_decimal(0.5); assert!(invalid_weights.validate().is_err()); } }
```

# src/config/mod.rs

```rs
mod agent_config; mod market_config; pub mod mongodb; pub use self::agent_config::AgentConfig; pub use self::market_config::MarketConfig; use rig::providers::openai::{GPT_4O, GPT_4O_MINI, O1_MINI, O1_PREVIEW}; pub const DEFAULT_MODEL: &str = "gpt-4o-mini"; pub fn get_openai_model() -> &'static str { match std::env::var("OPENAI_MODEL").as_deref() { Ok("gpt-4o") => GPT_4O, Ok("gpt-4o-mini") => GPT_4O_MINI, Ok("o3-mini") => O1_MINI, Ok("o1-preview") => O1_PREVIEW, _ => DEFAULT_MODEL, } }
```

# src/config/mongodb.rs

```rs
use anyhow::{Result, Context, anyhow}; use mongodb::{ bson::{doc, Document}, options::ClientOptions, Client, Database, }; use rig::{ embeddings::{EmbeddingsBuilder, Embed}, providers::openai::EmbeddingModel, vector_store::VectorStoreIndexDyn, }; use rig_mongodb::{MongoDbVectorIndex, SearchParams}; use serde::{Deserialize, Serialize, Deserializer}; use serde_json::Value; use std::{env, sync::Arc, time::Duration}; use async_trait::async_trait; #[derive(Debug, Clone)] pub struct MongoPoolConfig { pub min_pool_size: u32, pub max_pool_size: u32, pub connect_timeout: Duration, } impl Default for MongoPoolConfig { fn default() -> Self { Self { min_pool_size: 5, max_pool_size: 10, connect_timeout: Duration::from_secs(20), } } } impl MongoPoolConfig { pub fn from_env() -> Self { Self { min_pool_size: std::env::var("MONGODB_MIN_POOL_SIZE") .ok() .and_then(|s| s.parse().ok()) .unwrap_or(5), max_pool_size: std::env::var("MONGODB_MAX_POOL_SIZE") .ok() .and_then(|s| s.parse().ok()) .unwrap_or(10), connect_timeout: Duration::from_millis( std::env::var("MONGODB_CONNECT_TIMEOUT_MS") .ok() .and_then(|s| s.parse().ok()) .unwrap_or(20000), ), } } pub fn apply_to_options(&self, options: &mut ClientOptions) { options.min_pool_size = Some(self.min_pool_size); options.max_pool_size = Some(self.max_pool_size); options.connect_timeout = Some(self.connect_timeout); } } #[derive(Debug, Clone)] pub struct MongoConfig { pub uri: String, pub database: String, pub app_name: Option<String>, pub pool_config: MongoPoolConfig, } impl Default for MongoConfig { fn default() -> Self { Self { uri: "mongodb://localhost:32770".to_string(), database: "cainam".to_string(), app_name: Some("cainam-core".to_string()), pool_config: MongoPoolConfig::default(), } } } impl MongoConfig { pub fn from_env() -> Self { let uri = env::var("MONGODB_URI").expect("MONGODB_URI must be set"); let database = env::var("MONGODB_DATABASE").expect("MONGODB_DATABASE must be set"); Self { uri, database, app_name: None, pool_config: MongoPoolConfig::default(), } } } #[derive(Debug, Clone, Deserialize, Serialize)] pub struct TokenAnalyticsData { #[serde(rename = "_id", deserialize_with = "deserialize_object_id")] pub id: String, pub token_address: String, pub token_name: String, pub token_symbol: String, pub embedding: Vec<f32>, } fn deserialize_object_id<'de, D>(deserializer: D) -> Result<String, D::Error> where D: Deserializer<'de>, { let value = Value::deserialize(deserializer)?; match value { Value::String(s) => Ok(s), Value::Object(map) => { if let Some(Value::String(oid)) = map.get("$oid") { Ok(oid.to_string()) } else { Err(serde::de::Error::custom( "Expected $oid field with string value", )) } } _ => Err(serde::de::Error::custom( "Expected string or object with $oid field", )), } } #[derive(Clone)] pub struct MongoDbPool { client: Client, config: MongoConfig, db: Database, } impl MongoDbPool { pub async fn create_pool(config: MongoConfig) -> Result<Arc<MongoDbPool>> { let mut client_options = ClientOptions::parse(&config.uri).await?; if let Some(app_name) = &config.app_name { client_options.app_name = Some(app_name.clone()); } // Set server API version to ensure compatibility client_options.server_api = Some( mongodb::options::ServerApi::builder() .version(mongodb::options::ServerApiVersion::V1) .build(), ); // Apply pool configuration config.pool_config.apply_to_options(&mut client_options); let client = Client::with_options(client_options)?; let db = client.database(&config.database); // Test the connection client .database("admin") .run_command(doc! {"ping": 1}) .await?; Ok(Arc::new(MongoDbPool { client, config, db })) } pub fn database(&self, name: &str) -> mongodb::Database { self.db.clone() } pub fn get_config(&self) -> &MongoConfig { &self.config } pub fn client(&self) -> &Client { &self.client } } #[async_trait] pub trait TokenAnalyticsDataExt { async fn insert_token_analytics_documents<T>( &self, collection_name: &str, embedding_model: EmbeddingModel, documents: Vec<T>, ) -> Result<()> where T: Serialize + Send + Sync + Embed; async fn top_n( &self, collection_name: &str, embedding_model: EmbeddingModel, query: &str, limit: i64, ) -> Result<Vec<Document>>; } #[async_trait] impl TokenAnalyticsDataExt for MongoDbPool { async fn insert_token_analytics_documents<T>( &self, collection_name: &str, embedding_model: EmbeddingModel, documents: Vec<T>, ) -> Result<()> where T: Serialize + Send + Sync + Embed, { let collection = self.db.collection::<Document>(collection_name); /* let index_options = IndexOptions::builder() .name("vector_index".to_string()) .build(); let index_model = IndexModel::builder() .keys(doc! { "embedding": "vector" }) .options(index_options) .build(); if let Err(e) = collection.create_index(index_model).await { if !e.to_string().contains("Index already exists") { return Err(e.into()); } } */ let embeddings = EmbeddingsBuilder::new(embedding_model.clone()) .documents(documents)? .build() .await?; for (doc, embedding) in embeddings { let token_data_doc = bson::to_document(&doc) .map_err(|e| anyhow!("Serialization error: {}", e))?; let mut doc_with_embedding = token_data_doc; doc_with_embedding.insert("embedding", bson::to_bson(&embedding)?); collection.insert_one(doc_with_embedding).await?; } Ok(()) } async fn top_n( &self, collection_name: &str, embedding_model: EmbeddingModel, query: &str, limit: i64, ) -> Result<Vec<Document>> { let collection = self.db.collection::<TokenAnalyticsData>(collection_name); let index = MongoDbVectorIndex::<_, TokenAnalyticsData>::new( collection, embedding_model, "vector_index", SearchParams::new() .filter(doc! { "fields": ["embedding"] }) .exact(true) .num_candidates(100), ).await?; let results = index .top_n(query, limit as usize) .await .context("Failed to perform vector search")?; let documents = results .into_iter() .map(|(_, _, doc)| bson::to_document(&doc)) .collect::<Result<Vec<_>, _>>()?; Ok(documents) } }
```

# src/core/agent.rs

```rs
use rig::agent::Agent as RigAgent; use rig::providers::openai::{Client as OpenAIClient, CompletionModel, GPT_4_TURBO}; use rig::{completion::Prompt, providers}; use anyhow::Result; pub struct Agent { agent: RigAgent<CompletionModel>, } impl Agent { pub fn new(openai_api_key: &str, prompt: &str) -> Self { let openai_client = OpenAIClient::new(openai_api_key); let agent = openai_client .agent(GPT_4_TURBO) .preamble(prompt) .temperature(1.0) .build(); Agent { agent } } pub async fn prompt(&self, input: &str) -> Result<String> { let response = self.agent.prompt(input).await?; Ok(response) } }
```

# src/core/characteristics.rs

```rs
use std::io; use crate::characteristics::{ adjectives::Adjectives, bio::Bio, lore::Lore, post_examples::PostExamples, previous_messages::PreviousMessages, styles::Styles, topics::Topics, }; // Trait to simulate each characteristic module pub trait Characteristic { fn get_header(&self) -> String; fn get_traits(&self, character_name: &str) -> io::Result<String>; } pub struct Characteristics; impl Characteristics { // Simulate getCharacteristics pub fn get_characteristics() -> Vec<Box<dyn Characteristic>> { vec![ Box::new(Bio), Box::new(Lore), Box::new(PreviousMessages), Box::new(PostExamples), Box::new(Adjectives), Box::new(Topics), Box::new(Styles), ] } // Simulate buildCharacteristicsInstructions pub fn build_characteristics_instructions(character_name: &str) -> String { let mut chars_instruction = String::new(); let characteristics = Self::get_characteristics(); for characteristic in characteristics { chars_instruction += &characteristic.get_header(); chars_instruction += "\n"; chars_instruction += &characteristic.get_traits(character_name).unwrap(); chars_instruction += "\n"; } chars_instruction } // Simulate getCharacterInstructions pub fn get_character_instructions(chars_instruction: &String) -> &String { chars_instruction } }
```

# src/core/instruction_builder.rs

```rs
use std::fs; use std::io::{self}; use super::characteristics::Characteristics; pub struct InstructionBuilder { instructions: String, } impl InstructionBuilder { pub fn new() -> Self { Self { instructions: String::new(), } } // Read base instructions from a file pub fn get_base(character_name: &str) -> io::Result<String> { let path = format!("./characters/{}/instructions/base.txt", character_name); fs::read_to_string(&path) } // Read suffix instructions from a file pub fn get_suffix(character_name: &str) -> io::Result<String> { let path = format!("./characters/{}/instructions/suffix.txt", character_name); fs::read_to_string(&path) } // Add instruction to the internal buffer pub fn add_instruction(&mut self, instruction: &str) { self.instructions.push_str(instruction); } // Add multiple instructions (array equivalent) pub fn add_instructions(&mut self, instructions: Vec<String>) { for instruction in instructions { self.add_instruction(&instruction); } } // Build the complete instructions pub fn build_instructions(&mut self, character_name: &str) -> io::Result<()> { self.instructions.clear(); let characteristics = Characteristics::build_characteristics_instructions(character_name); // Add base instructions if let Ok(base) = Self::get_base(character_name) { self.add_instruction(&base); } // Add characteristics instructions self.add_instruction(&characteristics); // Add suffix instructions if let Ok(suffix) = Self::get_suffix(character_name) { self.add_instruction(&suffix); } Ok(()) } // Get the complete instructions pub fn get_instructions(&self) -> &str { &self.instructions } }
```

# src/core/mod.rs

```rs
pub mod agent; pub mod characteristics; pub mod instruction_builder; pub mod runtime;
```

# src/core/runtime.rs

```rs
use rand::Rng; use tokio::time::{sleep, Duration}; use crate::{ core::agent::Agent, memory::MemoryStore, providers::{ai16z_twitter::Ai16zTwitter, discord::Discord, twitter::Twitter}, }; pub enum TwitterType { ApiKeys(Twitter), Ai16zTwitter(Ai16zTwitter), } impl TwitterType { pub async fn tweet(&self, text: &str) -> Result<(), anyhow::Error> { match self { TwitterType::ApiKeys(twitter) => { // Call the tweet method for Twitter API twitter.tweet(text.to_string()).await } TwitterType::Ai16zTwitter(ai6z_twitter) => { // Call the tweet method for Ai6zTwitter ai6z_twitter.tweet(text.to_string()).await } } } } pub struct Runtime { openai_api_key: String, twitter: TwitterType, discord: Discord, agents: Vec<Agent>, memory: Vec<String>, } impl Runtime { pub fn new( openai_api_key: &str, discord_webhook_url: &str, twitter_consumer_key: Option<&str>, twitter_consumer_secret: Option<&str>, twitter_access_token: Option<&str>, twitter_access_token_secret: Option<&str>, twitter_username: Option<&str>, twitter_password: Option<&str>, ) -> Self { let twitter = match (twitter_username, twitter_password) { (Some(username), Some(password)) => { // If both username and password are provided, prioritize Ai6zTwitter TwitterType::Ai16zTwitter(Ai16zTwitter::new(username, password)) } (_, _) => { // Otherwise, fall back to Twitter API keys if available match ( twitter_consumer_key, twitter_consumer_secret, twitter_access_token, twitter_access_token_secret, ) { ( Some(consumer_key), Some(consumer_secret), Some(access_token), Some(access_token_secret), ) => TwitterType::ApiKeys(Twitter::new( consumer_key, consumer_secret, access_token, access_token_secret, )), _ => panic!("You must provide either Twitter username/password or API keys."), } } }; let discord = Discord::new(discord_webhook_url); let agents = Vec::new(); let memory: Vec<String> = MemoryStore::load_memory().unwrap_or_else(|_| Vec::new()); Runtime { discord, memory, openai_api_key: openai_api_key.to_string(), agents, twitter, } } pub fn add_agent(&mut self, prompt: &str) { let agent = Agent::new(&self.openai_api_key, prompt); self.agents.push(agent); } pub async fn run(&mut self) -> Result<(), anyhow::Error> { if self.agents.is_empty() { return Err(anyhow::anyhow!("No agents available")).map_err(Into::into); } let mut rng = rand::thread_rng(); let selected_agent = &self.agents[rng.gen_range(0..self.agents.len())]; let response = selected_agent.prompt("tweet").await?; match MemoryStore::add_to_memory(&mut self.memory, &response) { Ok(_) => println!("Response saved to memory."), Err(e) => eprintln!("Failed to save response to memory: {}", e), } println!("AI Response: {}", response); self.discord.send_channel_message(&response.clone()).await; self.twitter.tweet(&response).await?; Ok(()) } pub async fn run_periodically(&mut self) -> Result<(), anyhow::Error> { let mut rng = rand::thread_rng(); loop { let random_sleep_duration = rng.gen_range(300..=1800); sleep(Duration::from_secs(random_sleep_duration)).await; if let Err(e) = self.run().await { eprintln!("Error running process: {}", e); } } } }
```

# src/database/mod.rs

```rs
use std::sync::Arc; use async_trait::async_trait; pub use mongodb::{ Collection, options::{FindOptions, FindOneOptions}, bson::{self, doc, Document, DateTime}, }; use crate::config::mongodb::{MongoConfig, MongoDbPool}; use anyhow::Result; use serde::{de::DeserializeOwned, Serialize}; // pub mod sync; #[derive(Clone)] pub struct DatabaseManager { pool: Arc<MongoDbPool>, } impl DatabaseManager { pub async fn new(config: MongoConfig) -> Result<Self> { let pool = MongoDbPool::create_pool(config).await?; Ok(Self { pool }) } pub fn get_pool(&self) -> &MongoDbPool { &self.pool } pub fn get_database(&self, name: &str) -> mongodb::Database { self.pool.database(name) } } #[async_trait] pub trait MongoDbExtensions { fn get_collection<T>(&self, name: &str) -> Collection<T> where T: Serialize + DeserializeOwned + Unpin + Send + Sync + 'static; async fn find_one_by_id<T>(&self, collection: &str, id: bson::oid::ObjectId) -> Result<Option<T>> where T: Serialize + DeserializeOwned + Unpin + Send + Sync + 'static; async fn find_one_by_filter<T>(&self, collection: &str, filter: bson::Document) -> Result<Option<T>> where T: Serialize + DeserializeOwned + Unpin + Send + Sync + 'static; async fn find_with_sort<T>(&self, collection: &str, filter: bson::Document, sort: bson::Document, limit: Option<i64>) -> Result<Vec<T>> where T: Serialize + DeserializeOwned + Unpin + Send + Sync + 'static; } // impl MongoDbExtensions for mongodb::Database { // fn get_collection<T>(&self, name: &str) -> Collection<T> // where // T: Serialize + DeserializeOwned + Unpin + Send + Sync + 'static // { // self.collection(name) // } // async fn find_one_by_id<T>(&self, collection: &str, id: bson::oid::ObjectId) -> Result<Option<T>> // where // T: Serialize + DeserializeOwned + Unpin + Send + Sync + 'static, // Crucial change // { // let filter = doc! { "_id": id }; // let collection: Collection<T> = self.collection(collection); // Type hint for clarity // Ok(collection.find_one(filter).await?) // } // async fn find_one_by_filter<T>(&self, collection: &str, filter: bson::Document) -> Result<Option<T>> // where // T: Serialize + DeserializeOwned + Unpin + Send + Sync + 'static, // Crucial change // { // Ok(self.collection(collection).find_one(filter).await?) // } // async fn find_with_sort<T>(&self, collection: &str, filter: bson::Document, sort: bson::Document, limit: Option<i64>) -> Result<Vec<T>> // where // T: Serialize + DeserializeOwned + Unpin + Send + Sync + 'static, // Crucial change // { // let options = FindOptions::builder() // .sort(sort) // .limit(limit) // .build(); // let mut cursor = self.collection(collection).find(filter).await?; // let mut results = Vec::new(); // while let Some(doc) = cursor.try_next().await? { // results.push(doc); // } // Ok(results) // } // } // Vector store configuration helper // pub fn create_vector_search_params() -> SearchParams { // SearchParams::new() // .with_distance_metric("cosine") // .with_embedding_field("vector") // .with_index_type("hnsw") // } // #[cfg(test)] // mod tests { // use super::*; // use crate::test_utils::setup_test_db; // #[tokio::test] // async fn test_database_extensions() { // let (pool, db_name) = setup_test_db().await.unwrap(); // let db = pool.database(&db_name); // // Test find_one_by_filter // let filter = doc! { "test_field": "test_value" }; // let result = db.find_one_by_filter::<Document>("test_collection", filter).await; // assert!(result.is_ok()); // } // }
```

# src/database/sync.rs

```rs
use anyhow::Result; use bson::doc; use chrono::{DateTime, Utc}; use mongodb::Database; use serde::{Serialize, Deserialize}; use std::sync::Arc; use tracing::{info, warn}; use rig::completion::CompletionModel; use solana_sdk::signature::Keypair; use crate::error::Error; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TokenState { pub address: String, pub symbol: String, pub name: String, pub price_usd: f64, pub price_sol: f64, pub volume_24h: f64, pub market_cap: f64, pub price_change_24h: f64, pub volume_change_24h: f64, pub timestamp: DateTime<Utc>, } pub struct DataSyncService<M: CompletionModel> { db: Arc<Database>, data_provider: Arc<dyn DataProvider>, twitter: Arc<TwitterClient>, trading_strategy: Arc<TradingStrategy<M>>, dex: JupiterDex, personality: StoicPersonality, wallet: Arc<Keypair>, sync_interval: u64, } impl<M: CompletionModel> DataSyncService<M> { pub fn new( db: Arc<Database>, data_provider: Arc<dyn DataProvider>, twitter: Arc<TwitterClient>, trading_strategy: Arc<TradingStrategy<M>>, dex: JupiterDex, wallet: Arc<Keypair>, sync_interval: u64, ) -> Self { Self { db, data_provider, twitter, trading_strategy, dex, personality: StoicPersonality::new(), wallet, sync_interval, } } pub async fn sync_market_data(&self) -> Result<()> { info!("Starting market data sync cycle"); // Fetch trending tokens info!("Fetching trending tokens from BirdEye"); let trends = self.data_provider.get_trending_tokens(20).await?; info!("Found {} trending tokens", trends.len()); // Insert token states and analyze trading opportunities for trend in trends { info!( "Processing token {} ({}) - Price: ${:.4}, 24h Change: {:.2}%, Volume: ${:.2}M", trend.metadata.name, trend.metadata.symbol, trend.metadata.price_usd, trend.price_change_24h, trend.metadata.volume_24h / 1_000_000.0 ); let state = self.market_trend_to_token_state(trend.clone()); info!("Inserting token state into MongoDB"); self.db.insert_one("token_states", &state).await?; // Format market data for LLM analysis let prompt = format!( "Analyze trading opportunity for {} ({}). Price: ${:.4}, 24h Change: {:.2}%, Volume: ${:.2}M", trend.metadata.name, trend.metadata.symbol, trend.metadata.price_usd, trend.price_change_24h, trend.metadata.volume_24h / 1_000_000.0 ); // Analyze trading opportunity info!("Analyzing trading opportunity with LLM"); if let Ok(analysis) = self.trading_strategy.analyze_trading_opportunity(prompt, 1.0).await { // Parse the analysis into a trade recommendation if let Ok(trade) = serde_json::from_str::<TradeRecommendation>(&analysis) { info!( "Received trade recommendation: Action={:?}, Amount={} SOL, Confidence={:.2}, Risk={}", trade.action, trade.amount_in_sol, trade.confidence, trade.risk_assessment ); // Execute trade if confidence is high enough if trade.confidence >= 0.8 { match trade.action { TradeAction::Buy => { info!("Executing BUY order for {} SOL worth of {}", trade.amount_in_sol, trend.metadata.symbol); if let Ok(signature) = self.dex.execute_swap( "So11111111111111111111111111111111111111112", // SOL &trade.token_address, trade.amount_in_sol as u64, &self.wallet, ).await { info!("Trade executed successfully. Signature: {}", signature); // Generate and post tweet about the trade info!("Generating tweet for successful buy"); let tweet = self.personality.generate_trade_tweet( &self.trading_strategy.agent, &format!( "Action: Buy\nAmount: {} SOL\nToken: {}\nPrice: ${:.4}\nMarket Cap: ${:.2}M\n24h Volume: ${:.2}M\n24h Change: {:.2}%\nContract: {}\nTransaction: {}\nAnalysis: {}\nRisk Assessment: {}\nMarket Analysis:\n- Volume: {}\n- Price Trend: {}\n- Liquidity: {}\n- Momentum: {}", trade.amount_in_sol, trend.metadata.symbol, trend.metadata.price_usd, trend.metadata.market_cap / 1_000_000.0, trend.metadata.volume_24h / 1_000_000.0, trend.price_change_24h, trend.token_address, signature, trade.reasoning, trade.risk_assessment, trade.market_analysis.volume_analysis, trade.market_analysis.price_trend, trade.market_analysis.liquidity_assessment, trade.market_analysis.momentum_indicators ), ).await?; info!("Posting tweet: {}", tweet); if let Err(e) = self.twitter.post_tweet(&tweet).await { warn!("Failed to post trade tweet: {}", e); } } else { warn!("Failed to execute buy order"); } }, TradeAction::Sell => { info!("Skipping SELL action - not implemented yet"); }, TradeAction::Hold => { info!("Decision: HOLD {} - {}", trend.metadata.symbol, trade.reasoning); } } } else { info!("Skipping trade due to low confidence: {:.2}", trade.confidence); } } else { warn!("Failed to parse trade recommendation"); } } else { warn!("Failed to get trading analysis from LLM"); } } info!("Market data sync cycle complete"); Ok(()) } pub async fn get_token_state(&self, token_address: &str) -> Result<Option<TokenState>> { let collection = self.db .database() .collection("token_states"); let filter = doc! { "address": token_address }; let options = rig_mongodb::options::FindOneOptions::builder() .sort(doc! { "timestamp": -1 }) .build(); collection.find_one(filter, options) .await .map_err(anyhow::Error::from) } pub async fn get_token_history( &self, token_address: &str, start_time: DateTime<Utc>, end_time: DateTime<Utc>, ) -> Result<Vec<TokenState>> { let collection = self.db .database() .collection("token_states"); let filter = doc! { "address": token_address, "timestamp": { "$gte": start_time, "$lte": end_time } }; let options = rig_mongodb::options::FindOptions::builder() .sort(doc! { "timestamp": -1 }) .build(); let cursor = collection.find(filter, options).await?; cursor.try_collect().await.map_err(anyhow::Error::from) } } pub fn sync_databases(source: &Database, target: &Database) -> Result<(), Error> { // ...existing code... }
```

# src/error.rs

```rs
use mongodb::error::Error as MongoError; use std::error::Error as StdError; use std::fmt; use std::num::ParseFloatError; use thiserror::Error; #[derive(Error, Debug)] pub enum Error { #[error("MongoDB error: {0}")] Mongo(#[from] MongoError), #[error("ParseFloat error: {0}")] ParseFloat(#[from] ParseFloatError), #[error("Other error: {0}")] Other(String), } #[derive(Debug)] pub enum AgentError { Config(String), MissingEnvVar(String), InvalidConfig(String, String), TwitterApi(String), Trading(String), Database(MongoError), MarketAnalysis(String), VectorStore(String), BirdeyeApi(String), Transaction(String), Validation(String), Parse(String), RateLimit(String), Authentication(String), Network(String), Timeout(String), Conversion(String), Other(anyhow::Error), Mongo(mongodb::error::Error), InvalidInput(String), ApiError(String), } impl fmt::Display for AgentError { fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result { match self { AgentError::Config(msg) => write!(f, "Configuration error: {}", msg), AgentError::MissingEnvVar(var) => write!(f, "Environment variable '{}' not found", var), AgentError::InvalidConfig(field, msg) => { write!(f, "Invalid value for {}: {}", field, msg) } AgentError::TwitterApi(msg) => write!(f, "Twitter API error: {}", msg), AgentError::Trading(msg) => write!(f, "Trading error: {}", msg), AgentError::Database(err) => write!(f, "Database error: {}", err), AgentError::MarketAnalysis(msg) => write!(f, "Market analysis error: {}", msg), AgentError::VectorStore(msg) => write!(f, "Vector store error: {}", msg), AgentError::BirdeyeApi(msg) => write!(f, "Birdeye API error: {}", msg), AgentError::Transaction(msg) => write!(f, "Transaction error: {}", msg), AgentError::Validation(msg) => write!(f, "Validation error: {}", msg), AgentError::Parse(msg) => write!(f, "Parse error: {}", msg), AgentError::RateLimit(service) => write!(f, "Rate limit exceeded for {}", service), AgentError::Authentication(msg) => write!(f, "Authentication error: {}", msg), AgentError::Network(msg) => write!(f, "Network error: {}", msg), AgentError::Timeout(msg) => write!(f, "Timeout error: {}", msg), AgentError::Conversion(msg) => write!(f, "Conversion error: {}", msg), AgentError::Other(err) => write!(f, "Other error: {}", err), AgentError::Mongo(err) => write!(f, "MongoDB error: {}", err), AgentError::InvalidInput(err) => write!(f, "Input error: {}", err), AgentError::ApiError(err) => write!(f, "Api error: {}", err), } } } impl StdError for AgentError { fn source(&self) -> Option<&(dyn StdError + 'static)> { match self { AgentError::Database(err) => Some(err), AgentError::Mongo(err) => Some(err), _ => None, } } } impl From<MongoError> for AgentError { fn from(err: MongoError) -> Self { AgentError::Mongo(err) } } impl From<ParseFloatError> for AgentError { fn from(err: ParseFloatError) -> Self { AgentError::Parse(err.to_string()) } } impl From<tracing_subscriber::filter::ParseError> for AgentError { fn from(err: tracing_subscriber::filter::ParseError) -> Self { AgentError::Parse(err.to_string()) } } impl From<reqwest::Error> for AgentError { fn from(err: reqwest::Error) -> Self { if err.is_timeout() { AgentError::Timeout(err.to_string()) } else if err.is_connect() { AgentError::Network(err.to_string()) } else { AgentError::Other(err.into()) } } } pub type AgentResult<T> = Result<T, AgentError>; // Helper functions for common error cases impl AgentError { pub fn missing_env(var: &str) -> Self { AgentError::MissingEnvVar(var.to_string()) } pub fn invalid_config<T: std::fmt::Display>(field: &str, message: T) -> Self { AgentError::InvalidConfig(field.to_string(), message.to_string()) } pub fn validation<T: std::fmt::Display>(message: T) -> Self { AgentError::Validation(message.to_string()) } pub fn transaction<T: std::fmt::Display>(message: T) -> Self { AgentError::Transaction(message.to_string()) } pub fn rate_limit<T: std::fmt::Display>(service: T) -> Self { AgentError::RateLimit(service.to_string()) } pub fn auth<T: std::fmt::Display>(message: T) -> Self { AgentError::Authentication(message.to_string()) } } #[cfg(test)] mod tests { use super::*; #[test] fn test_error_conversions() { // Test ParseFloatError conversion let parse_err: AgentError = "invalid float".parse::<f64>().unwrap_err().into(); assert!(matches!(parse_err, AgentError::Parse(_))); // Test helper functions let missing_env = AgentError::missing_env("TEST_VAR"); assert!(matches!(missing_env, AgentError::MissingEnvVar(_))); let invalid_config = AgentError::invalid_config("threshold", "must be positive"); assert!(matches!(invalid_config, AgentError::InvalidConfig(_, _))); let validation = AgentError::validation("invalid input"); assert!(matches!(validation, AgentError::Validation(_))); let transaction = AgentError::transaction("commit failed"); assert!(matches!(transaction, AgentError::Transaction(_))); let rate_limit = AgentError::rate_limit("Birdeye API"); assert!(matches!(rate_limit, AgentError::RateLimit(_))); let auth = AgentError::auth("invalid credentials"); assert!(matches!(auth, AgentError::Authentication(_))); } #[test] fn test_error_display() { let err = AgentError::missing_env("TEST_VAR"); assert_eq!(err.to_string(), "Environment variable 'TEST_VAR' not found"); let err = AgentError::invalid_config("threshold", "must be positive"); assert_eq!( err.to_string(), "Invalid value for threshold: must be positive" ); let err = AgentError::validation("invalid input"); assert_eq!(err.to_string(), "Validation error: invalid input"); } }
```

# src/lib.rs

```rs
pub mod agent; pub mod birdeye; pub mod config; pub mod error; pub mod logging; pub mod models; pub mod services; pub mod trading; pub mod twitter; pub mod utils;
```

# src/logging/mod.rs

```rs
use chrono::{DateTime, Utc}; use serde::Serialize; use tracing::{error, info, warn}; use uuid::Uuid; #[derive(Debug, Clone, Serialize)] pub struct MarketMetrics { pub symbol: String, pub price: f64, pub volume_24h: Option<f64>, pub signal_type: Option<String>, pub confidence: Option<f64>, } #[derive(Debug, Clone, Serialize)] pub struct MarketSignalLog { pub id: Uuid, pub timestamp: DateTime<Utc>, pub token_address: String, pub token_symbol: String, pub signal_type: String, pub price: f64, pub price_change_24h: Option<f64>, pub volume_change_24h: Option<f64>, pub confidence: f64, pub risk_score: f64, pub created_at: DateTime<Utc>, } pub struct RequestLogger { module: String, action: String, } impl RequestLogger { pub fn new(module: &str, action: &str) -> Self { Self { module: module.to_string(), action: action.to_string(), } } pub fn info(&self, message: &str) { info!(module = %self.module, action = %self.action, "{}", message); } pub fn warn(&self, message: &str) { warn!(module = %self.module, action = %self.action, "{}", message); } pub fn error(&self, message: &str) { error!(module = %self.module, action = %self.action, "{}", message); } } pub fn log_market_metrics(metrics: MarketMetrics) { info!( symbol = %metrics.symbol, price = %metrics.price, volume_24h = ?metrics.volume_24h, signal_type = ?metrics.signal_type, confidence = ?metrics.confidence, "Market metrics recorded" ); } pub fn log_market_signal(signal: MarketSignalLog) { info!( token = %signal.token_symbol, signal_type = %signal.signal_type, price_change = ?signal.price_change_24h, volume_change = ?signal.volume_change_24h, confidence = %signal.confidence, risk_score = %signal.risk_score, "Market signal generated" ); }
```

# src/main.rs

```rs
use crate::{ agent::trader::TradingAgent, config::AgentConfig, models::market_signal::{MarketSignal, SignalType}, trading::SolanaAgentKit, utils::f64_to_decimal, }; use anyhow::Result; use bson::DateTime; use config::mongodb::{MongoConfig, MongoDbPool, MongoPoolConfig}; use solana_sdk::signature::Keypair; use std::io::{self, Write}; use std::sync::atomic::{AtomicBool, Ordering}; use std::sync::Arc; use tokio; use tracing::{error, info}; mod agent; mod birdeye; mod config; mod error; mod logging; mod models; mod services; mod trading; mod twitter; mod utils; async fn handle_user_input( trader: Arc<TradingAgent>, config: AgentConfig, running: Arc<AtomicBool>, ) { println!("\n=== Cainam Trading Agent ==="); println!("The agent is running autonomously in the background."); println!("\nAvailable commands:"); println!(" analyze <symbol> <address> - Analyze market for a token"); println!(" trade <symbol> <buy|sell> <amount> - Execute a trade"); println!(" status - Get current trading status"); println!(" exit - Exit the program"); println!("\nType a command and press Enter.\n"); loop { if !running.load(Ordering::SeqCst) { break; } print!("> "); io::stdout().flush().unwrap_or_default(); let mut input = String::new(); match io::stdin().read_line(&mut input) { Ok(_) => { let parts: Vec<String> = input.trim().split_whitespace().map(String::from).collect(); if parts.is_empty() { continue; } match parts[0].as_str() { "analyze" => { if parts.len() != 3 { println!("Usage: analyze <symbol> <address>"); continue; } println!("Analyzing market for {}...", parts[1]); tokio::spawn({ let trader = trader.clone(); let symbol = parts[1].clone(); let address = parts[2].clone(); async move { match trader.analyze_market(&symbol, &address).await { Ok(Some(signal)) => { println!("\nMarket Analysis Result:"); println!(" Signal: {:?}", signal.signal_type); println!(" Confidence: {:.2}", signal.confidence); println!(" Risk Score: {:.2}", signal.risk_score); } Ok(None) => println!("\nNo trading signals generated"), Err(e) => println!("\nAnalysis failed: {}", e), } } }); } "trade" => { if parts.len() != 4 { println!("Usage: trade <symbol> <buy|sell> <amount>"); continue; } let amount = match parts[3].parse::<f64>() { Ok(val) => val, Err(_) => { println!("Invalid amount. Please provide a valid number."); continue; } }; let signal_type = match parts[2].to_uppercase().as_str() { "BUY" => SignalType::StrongBuy, "SELL" => SignalType::StrongSell, _ => { println!("Invalid trade type. Use 'buy' or 'sell'"); continue; } }; println!("Executing {} trade for {}...", parts[2], parts[1]); tokio::spawn({ let trader = trader.clone(); let symbol = parts[1].clone(); async move { let signal = MarketSignal { id: None, asset_address: symbol.clone(), signal_type: signal_type.clone(), confidence: f64_to_decimal(0.8), risk_score: f64_to_decimal(0.2), sentiment_score: Some(f64_to_decimal(0.6)), volume_change_24h: Some(f64_to_decimal(0.15)), price_change_24h: Some(f64_to_decimal( if signal_type == SignalType::StrongBuy { 0.05 } else { -0.05 }, )), price: f64_to_decimal(10.0), volume_change: f64_to_decimal(0.2), timestamp: DateTime::now(), metadata: None, created_at: None, }; let min_confidence = f64_to_decimal(config.trade_min_confidence); if signal.confidence >= min_confidence { match trader.execute_trade(&symbol, &signal).await { Ok(signature) => { println!("\nTrade executed successfully!"); println!("Transaction: {}", signature); if let Err(e) = trader .post_trade_update( &symbol, &parts[2], amount, &signal_type, ) .await { println!("Failed to post trade update: {}", e); } } Err(e) => println!("\nTrade execution failed: {}", e), } } } }); } "status" => { println!("\nTrading Agent Status:"); println!(" State: Active"); println!(" Analysis Interval: {:?}", config.analysis_interval); println!(" Min Confidence: {:.2}", config.trade_min_confidence); println!(" Max Trade Amount: {:.2}", config.trade_max_amount); } "exit" => { println!("\nShutting down trading agent..."); running.store(false, Ordering::SeqCst); break; } _ => println!("Unknown command. Type 'help' for available commands."), } } Err(e) => { error!("Error reading input: {}", e); break; } } } } async fn init_mongodb() -> Result<Arc<MongoDbPool>> { info!("Initializing MongoDB connection..."); let config = MongoConfig { uri: std::env::var("MONGODB_URI") .unwrap_or_else(|_| "mongodb://localhost:32770".to_string()), database: std::env::var("MONGODB_DATABASE").unwrap_or_else(|_| "cainam".to_string()), app_name: std::env::var("MONGODB_APP_NAME").ok(), pool_config: MongoPoolConfig::from_env(), }; info!("Connecting to MongoDB at {}", config.uri); let pool = MongoDbPool::create_pool(config).await?; info!("Successfully connected to MongoDB"); Ok(pool) } #[tokio::main] async fn main() -> Result<()> { // Initialize logging // logging::init_logging()?; println!("Starting Cainam Core..."); // Load environment variables from .env file dotenvy::dotenv().ok(); println!("loadi env file..."); // Initialize MongoDB connection pool using rig-mongodb let db_pool = init_mongodb().await?; println!("init pool..."); // TODO: zTgx hardcoded // Initialize Solana agent let rpc_url = "https://api.devnet.solana.com"; let keypair = Keypair::new(); let solana_agent = SolanaAgentKit::new(rpc_url, keypair); // Load configuration from environment let config = AgentConfig::new_from_env()?; // Initialize trading agent let trader = Arc::new(TradingAgent::new(config.clone(), db_pool, solana_agent).await?); let running = Arc::new(AtomicBool::new(true)); // Initialize services with MongoDB pool // let token_analytics_service = TokenAnalyticsService::new( // db_pool.clone(), // birdeye.clone(), // birdeye_extended.clone(), // Some(market_config.clone()), // ).await?; // let portfolio_optimizer = PortfolioOptimizer::new(db_pool.clone()); // // Initialize vector store // let vector_store = VectorStore::new().await?; // // Spawn the autonomous trading agent // let trader_clone = trader.clone(); // let running_clone = running.clone(); // let trading_handle = tokio::spawn(async move { // info!("Starting autonomous trading..."); // if let Err(e) = trader_clone.run().await { // error!("Trading agent error: {}", e); // running_clone.store(false, Ordering::SeqCst); // } // }); // Handle user input in a separate task let input_handle = tokio::spawn(handle_user_input(trader.clone(), config, running.clone())); // Wait for either task to complete tokio::select! { // _ = trading_handle => { // info!("Trading task completed"); // } _ = input_handle => { info!("User input task completed"); } } // Wait for clean shutdown info!("Shutting down trading agent..."); running.store(false, Ordering::SeqCst); trader.stop(); Ok(()) }
```

# src/market_data/birdeye.rs

```rs
#[derive(Debug, Deserialize)] pub struct TokenMarketResponse { pub data: TokenMarketData, pub success: bool, } #[derive(Debug, Deserialize, Default)] pub struct TokenMarketData { pub address: String, pub price: f64, pub volume_24h: f64, pub decimals: u8, pub price_sol: f64, pub market_cap: f64, pub fully_diluted_market_cap: f64, pub circulating_supply: f64, pub total_supply: f64, pub price_change_24h: f64, pub volume_change_24h: f64, } impl BirdeyeClient { pub fn new(api_key: String) -> Self { Self { api_key, client: Client::new(), } } pub async fn get_market_data(&self, token_address: &str) -> Result<TokenMarketData, AgentError> { let url = format!( "https://public-api.birdeye.so/public/market_data?address={}", token_address ); let response = self .client .get(&url) .header("X-API-KEY", &self.api_key) .send() .await .map_err(|e| AgentError::ApiError(e.to_string()))?; if !response.status().is_success() { return Err(AgentError::ApiError(format!( "Failed to get market data: {}", response.status() ))); } let market_data = response .json::<TokenMarketResponse>() .await .map_err(|e| AgentError::ApiError(e.to_string()))?; if !market_data.success { return Err(AgentError::ApiError("Token not found".to_string())); } Ok(market_data.data) } pub async fn get_token_info_by_address(&self, token_address: &str) -> Result<TokenInfo, AgentError> { let market_data = self.get_market_data(token_address).await?; Ok(TokenInfo { address: market_data.address, price: market_data.price, volume_24h: market_data.volume_24h, decimals: market_data.decimals, price_sol: market_data.price_sol, market_cap: market_data.market_cap, fully_diluted_market_cap: market_data.fully_diluted_market_cap, circulating_supply: market_data.circulating_supply, total_supply: market_data.total_supply, price_change_24h: market_data.price_change_24h, volume_change_24h: market_data.volume_change_24h, }) } } #[async_trait] impl BirdeyeApi for BirdeyeClient { async fn get_token_info(&self, token_address: &str) -> Result<TokenInfo, AgentError> { self.get_token_info_by_address(token_address).await } }
```

# src/memory.rs

```rs
use serde_json; use std::fs; use std::io::{self, Write}; use std::path::Path; pub struct MemoryStore; impl MemoryStore { const FILE_PATH: &'static str = "./storage/memory.json"; // Load memory from file pub fn load_memory() -> io::Result<Vec<String>> { if Path::new(Self::FILE_PATH).exists() { let data = fs::read_to_string(Self::FILE_PATH)?; let memory: Vec<String> = serde_json::from_str(&data)?; Ok(memory) } else { Ok(Vec::new()) // Return an empty vector if file doesn't exist } } // Add to memory pub fn add_to_memory(memory: &mut Vec<String>, item: &str) -> Result<(), String> { if !memory.contains(&item.to_string()) { memory.push(item.to_string()); let _ = Self::save_memory(memory); Ok(()) } else { Err("Memory Exists!".to_string()) } } // Wipe memory pub fn wipe_memory(memory: &mut Vec<String>) -> io::Result<()> { memory.clear(); Self::save_memory(memory) } // Count memories pub fn count_memories(memory: &Vec<String>) -> usize { memory.len() } // Save memory to file pub fn save_memory(memory: &Vec<String>) -> io::Result<()> { let data = serde_json::to_string(memory)?; let mut file = fs::File::create(Self::FILE_PATH)?; file.write_all(data.as_bytes())?; Ok(()) } // Get current memory pub fn get_memory() -> io::Result<Vec<String>> { Self::load_memory() } }
```

# src/models/market_config.rs

```rs
use bigdecimal::BigDecimal; use serde::{Serialize, Deserialize}; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MarketConfig { pub price_change_threshold: BigDecimal, pub volume_surge_threshold: BigDecimal, pub base_confidence: BigDecimal, pub price_weight: BigDecimal, pub volume_weight: BigDecimal, } impl Default for MarketConfig { fn default() -> Self { Self { price_change_threshold: BigDecimal::from(0.05), volume_surge_threshold: BigDecimal::from(0.2), base_confidence: BigDecimal::from(0.5), price_weight: BigDecimal::from(0.3), volume_weight: BigDecimal::from(0.2), } } }
```

# src/models/market_signal.rs

```rs
use bigdecimal::BigDecimal; // use bson::{Document, oid::ObjectId}; // use chrono::DateTime; use crate::utils::f64_to_decimal; use bson::{self, DateTime, Document}; use serde::{Deserialize, Serialize}; use serde_json::Value as JsonValue; use std::fmt; #[derive(Debug, Clone, PartialEq, Serialize, Deserialize)] pub enum SignalType { Buy, Sell, Hold, StrongBuy, StrongSell, PriceSpike, PriceDrop, VolumeSurge, } impl fmt::Display for SignalType { fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result { match self { SignalType::Buy => write!(f, "buy"), SignalType::Sell => write!(f, "sell"), SignalType::Hold => write!(f, "hold"), _ => write!(f, "unknown"), } } } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct MarketSignal { #[serde(rename = "_id", skip_serializing_if = "Option::is_none")] pub id: Option<bson::oid::ObjectId>, pub asset_address: String, pub signal_type: SignalType, pub price: BigDecimal, pub confidence: BigDecimal, pub risk_score: BigDecimal, pub sentiment_score: Option<BigDecimal>, pub price_change_24h: Option<BigDecimal>, pub volume_change_24h: Option<BigDecimal>, pub volume_change: BigDecimal, pub created_at: Option<DateTime>, pub timestamp: DateTime, #[serde(skip_serializing_if = "Option::is_none")] pub metadata: Option<Document>, } pub struct MarketSignalBuilder { asset_address: String, signal_type: SignalType, confidence: Option<BigDecimal>, risk_score: Option<BigDecimal>, sentiment_score: Option<BigDecimal>, volume_change_24h: Option<BigDecimal>, price_change_24h: Option<BigDecimal>, price: BigDecimal, volume_change: Option<BigDecimal>, timestamp: Option<DateTime>, metadata: Option<JsonValue>, } impl MarketSignalBuilder { pub fn new(asset_address: String, signal_type: SignalType, price: BigDecimal) -> Self { Self { asset_address, signal_type, confidence: None, risk_score: None, sentiment_score: None, volume_change_24h: None, price_change_24h: None, price, volume_change: None, timestamp: None, metadata: None, } } pub fn confidence(mut self, confidence: BigDecimal) -> Self { self.confidence = Some(confidence); self } pub fn risk_score(mut self, risk_score: BigDecimal) -> Self { self.risk_score = Some(risk_score); self } pub fn sentiment_score(mut self, sentiment_score: BigDecimal) -> Self { self.sentiment_score = Some(sentiment_score); self } pub fn volume_change_24h(mut self, volume_change: BigDecimal) -> Self { self.volume_change_24h = Some(volume_change); self } pub fn price_change_24h(mut self, price_change: BigDecimal) -> Self { self.price_change_24h = Some(price_change); self } pub fn volume_change(mut self, volume_change: BigDecimal) -> Self { self.volume_change = Some(volume_change); self } pub fn timestamp(mut self, timestamp: DateTime) -> Self { self.timestamp = Some(timestamp); self } pub fn metadata(mut self, metadata: JsonValue) -> Self { self.metadata = Some(metadata); self } pub fn build(self) -> MarketSignal { MarketSignal { id: None, asset_address: self.asset_address, signal_type: self.signal_type, confidence: self.confidence.unwrap_or_else(|| f64_to_decimal(0.5)), risk_score: self.risk_score.unwrap_or_else(|| f64_to_decimal(0.5)), sentiment_score: self.sentiment_score, volume_change_24h: self.volume_change_24h, price_change_24h: self.price_change_24h, price: self.price, volume_change: self.volume_change.unwrap_or_else(|| BigDecimal::from(0)), timestamp: DateTime::from(self.timestamp.unwrap_or_else(DateTime::now)), metadata: self.metadata.map(|v| bson::to_document(&v).unwrap()), created_at: None, } } } #[cfg(test)] mod tests { use super::*; use serde_json::json; #[test] fn test_market_signal_builder() { let price = f64_to_decimal(100.0); let signal = MarketSignalBuilder::new( "test_address".to_string(), SignalType::PriceSpike, price.clone(), ) .confidence(f64_to_decimal(0.8)) .risk_score(f64_to_decimal(0.3)) .volume_change_24h(f64_to_decimal(0.15)) .price_change_24h(f64_to_decimal(0.05)) .metadata(json!({"source": "test"})) .build(); assert_eq!(signal.asset_address, "test_address"); assert_eq!(signal.price, price); assert_eq!(signal.confidence, f64_to_decimal(0.8)); assert_eq!(signal.risk_score, f64_to_decimal(0.3)); assert!(signal.metadata.is_some()); } #[test] fn test_market_signal_builder_defaults() { let price = f64_to_decimal(100.0); let signal = MarketSignalBuilder::new("test_address".to_string(), SignalType::Hold, price.clone()) .build(); assert_eq!(signal.confidence, f64_to_decimal(0.5)); // Default confidence assert_eq!(signal.risk_score, f64_to_decimal(0.5)); // Default risk score assert_eq!(signal.volume_change, BigDecimal::from(0)); // Default volume change assert!(signal.metadata.is_none()); } }
```

# src/models/mod.rs

```rs
use bson::{self, oid::ObjectId, DateTime}; use serde::{Deserialize, Serialize}; pub mod market_signal; pub mod token_analytics; // pub mod market_config; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TradeStatus; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TokenMetrics { #[serde(rename = "_id", skip_serializing_if = "Option::is_none")] pub id: Option<ObjectId>, pub token_address: String, pub metrics: bson::Document, pub timestamp: DateTime, } // Add typed collection helpers impl TokenMetrics { pub fn collection_name() -> &'static str { "token_metrics" } } #[derive(Debug, Clone, Serialize, Deserialize)] pub struct VectorDocument { #[serde(rename = "_id", skip_serializing_if = "Option::is_none")] pub id: Option<ObjectId>, pub vector: Vec<f32>, pub metadata: bson::Document, pub timestamp: DateTime, } impl VectorDocument { pub fn collection_name() -> &'static str { "vectors" } }
```

# src/models/token_analytics.rs

```rs
use bigdecimal::BigDecimal; // use crate::MongoDbPool; use bson::{oid::ObjectId, DateTime, Document}; use serde::{Deserialize, Serialize}; // use time::OffsetDateTime; #[derive(Debug, Clone, Serialize, Deserialize)] pub struct TokenAnalytics { #[serde(rename = "_id", skip_serializing_if = "Option::is_none")] pub id: Option<ObjectId>, pub token_address: String, pub token_name: String, pub token_symbol: String, pub price: BigDecimal, pub volume_24h: Option<BigDecimal>, pub market_cap: Option<BigDecimal>, pub total_supply: Option<BigDecimal>, pub holder_count: Option<i32>, pub timestamp: DateTime, pub created_at: Option<DateTime>, #[serde(skip_serializing_if = "Option::is_none")] pub metadata: Option<Document>, }
```

# src/personality/mod.rs

```rs
pub async fn generate_trade_tweet(&self, agent: &CompletionModel, trade_details: String) -> Result<String> { info!("Generating trade tweet with details: {}", trade_details); let prompt = format!( "{}\n\nPlease generate a tweet about this trade that:\n1. Is concise and professional\n2. Includes key metrics (amount, price, volume)\n3. Includes contract address and tx link\n4. Ends with stoic analysis based on market indicators\n5. Stays under 280 characters", trade_details ); let tweet = agent.complete(&prompt).await?; info!("Generated tweet: {}", tweet); Ok(tweet) }
```

# src/prompts/system.txt

```txt
You are an autonomous trading agent specializing in Solana cryptocurrency markets. Your personality is confident but not arrogant, data-driven but also intuitive, and you communicate with a mix of professional insight and engaging personality. Your responsibilities: 1. Analyze market data and trends using Birdeye API and other Solana data sources 2. Make informed trading decisions based on technical and fundamental analysis 3. Execute trades when confidence levels are high 4. Communicate trading activities and rationale on Twitter in an engaging manner Trading Guidelines: - Prioritize risk management and capital preservation - Look for clear patterns and correlations in market data - Consider both technical and fundamental factors - Maintain a clear record of your decision-making process Communication Style: - Be clear and concise in your analysis - Use emojis appropriately but not excessively - Maintain professionalism while being engaging - Share insights that provide value to followers - Be transparent about your reasoning When making decisions, consider: - Market volatility and liquidity - Historical price patterns - Trading volume and market depth - Token fundamentals and security metrics Response Format for Trade Decisions: { "action": "buy" | "sell" | "hold", "symbol": "token_symbol", "amount": float_value, "reason": "detailed_explanation", "confidence": float_between_0_and_1 } Remember: Your goal is to make profitable trades while building a following through insightful and engaging communications.
```

# src/providers/birdeye.rs

```rs
use anyhow::Result; use async_trait::async_trait; use reqwest::Client; use serde::{Deserialize, Serialize}; use std::sync::Arc; const BIRDEYE_API_URL: &str = "https://public-api.birdeye.so"; #[derive(Debug, Clone)] pub struct BirdeyeProvider { client: Arc<Client>, api_key: String, } #[derive(Debug, Serialize, Deserialize)] pub struct TokenInfo { pub address: String, pub symbol: String, pub name: String, pub decimals: u8, pub price_usd: f64, pub volume_24h: f64, pub market_cap: f64, } #[derive(Debug, Serialize, Deserialize)] pub struct MarketDepth { pub bids: Vec<OrderBookEntry>, pub asks: Vec<OrderBookEntry>, } #[derive(Debug, Serialize, Deserialize)] pub struct OrderBookEntry { pub price: f64, pub size: f64, } #[async_trait] pub trait MarketDataProvider: Send + Sync { async fn get_token_info(&self, address: &str) -> Result<TokenInfo>; async fn get_market_depth(&self, address: &str) -> Result<MarketDepth>; async fn get_price_history(&self, address: &str, interval: &str) -> Result<Vec<PricePoint>>; } #[derive(Debug, Serialize, Deserialize)] pub struct PricePoint { pub timestamp: i64, pub price: f64, pub volume: f64, } impl BirdeyeProvider { pub fn new(api_key: &str) -> Self { Self { client: Arc::new(Client::new()), api_key: api_key.to_string(), } } async fn make_request<T: for<'de> Deserialize<'de>>( &self, endpoint: &str, params: &[(&str, &str)], ) -> Result<T> { let url = format!("{}{}", BIRDEYE_API_URL, endpoint); let response = self.client .get(&url) .header("X-API-KEY", &self.api_key) .query(params) .send() .await? .error_for_status()?; let data = response.json::<T>().await?; Ok(data) } } #[async_trait] impl MarketDataProvider for BirdeyeProvider { async fn get_token_info(&self, address: &str) -> Result<TokenInfo> { self.make_request( "/public/token", &[("address", address)], ).await } async fn get_market_depth(&self, address: &str) -> Result<MarketDepth> { self.make_request( "/public/orderbook", &[("address", address)], ).await } async fn get_price_history(&self, address: &str, interval: &str) -> Result<Vec<PricePoint>> { self.make_request( "/public/price_history", &[ ("address", address), ("interval", interval), ], ).await } } // Additional helper functions for market analysis impl BirdeyeProvider { pub async fn analyze_liquidity(&self, address: &str) -> Result<LiquidityAnalysis> { let depth = self.get_market_depth(address).await?; let total_bid_liquidity: f64 = depth.bids .iter() .map(|entry| entry.price * entry.size) .sum(); let total_ask_liquidity: f64 = depth.asks .iter() .map(|entry| entry.price * entry.size) .sum(); Ok(LiquidityAnalysis { total_bid_liquidity, total_ask_liquidity, bid_ask_ratio: total_bid_liquidity / total_ask_liquidity, depth_quality: calculate_depth_quality(&depth), }) } pub async fn get_market_impact(&self, address: &str, size_usd: f64) -> Result<MarketImpact> { let depth = self.get_market_depth(address).await?; let token_info = self.get_token_info(address).await?; let size_tokens = size_usd / token_info.price_usd; let (price_impact, executed_price) = calculate_price_impact(&depth, size_tokens, token_info.price_usd); Ok(MarketImpact { price_impact, executed_price, size_usd, size_tokens, }) } } #[derive(Debug)] pub struct LiquidityAnalysis { pub total_bid_liquidity: f64, pub total_ask_liquidity: f64, pub bid_ask_ratio: f64, pub depth_quality: f64, } #[derive(Debug)] pub struct MarketImpact { pub price_impact: f64, pub executed_price: f64, pub size_usd: f64, pub size_tokens: f64, } fn calculate_depth_quality(depth: &MarketDepth) -> f64 { // Implement depth quality calculation // This could consider factors like: // - Spread // - Depth distribution // - Number of price levels 0.0 // Placeholder } fn calculate_price_impact( depth: &MarketDepth, size_tokens: f64, current_price: f64, ) -> (f64, f64) { // Implement price impact calculation // This should walk the order book to determine: // - Average execution price // - Price impact percentage (0.0, current_price) // Placeholder }
```

# src/providers/discord.rs

```rs
use reqwest::Client; use serde_json::json; use std::error::Error; pub struct Discord { webhook_url: String, } impl Discord { pub fn new(webhook_url: &str) -> Self { Discord { webhook_url: webhook_url.to_string(), } } pub async fn send_channel_message(&self, message: &str) -> Result<(), Box<dyn Error>> { // Create an HTTP client let client = Client::new(); // Create the payload as JSON let payload = json!({ "content": message }); // Send a POST request to the webhook URL let response = client.post(&self.webhook_url).json(&payload).send().await?; // Check if the request was successful if response.status().is_success() { println!("Message sent successfully!"); Ok(()) } else { let status = response.status(); let text = response.text().await?; Err(format!( "Failed to send message. Status: {}, Response: {}", status, text ) .into()) } } }
```

# src/providers/mod.rs

```rs
pub mod birdeye; pub mod discord;
```

# src/providers/twitter.rs

```rs
use twitter_v2::{authorization::Oauth1aToken, TwitterApi}; pub struct Twitter { auth: Oauth1aToken, } impl Twitter { pub fn new( twitter_consumer_key: &str, twitter_consumer_secret: &str, twitter_access_token: &str, twitter_access_token_secret: &str, ) -> Self { let auth = Oauth1aToken::new( twitter_consumer_key.to_string(), twitter_consumer_secret.to_string(), twitter_access_token.to_string(), twitter_access_token_secret.to_string(), ); Twitter { auth } } pub async fn tweet(&self, text: String) -> Result<(), anyhow::Error> { let tweet = TwitterApi::new(self.auth.clone()) .post_tweet() .text(text) .send() .await? .into_data() .expect("this tweet should exist"); println!("Tweet posted successfully with ID: {}", tweet.id); Ok(()) } }
```

# src/services/mod.rs

```rs
pub mod token_analytics; // #[cfg(test)] // mod tests;
```

# src/services/tests/mod.rs

```rs
mod token_analytics_tests; pub use token_analytics_tests::*;
```

# src/services/tests/token_analytics_tests.rs

```rs
use crate::config::MarketConfig; use crate::models::token_analytics::TokenAnalytics; use crate::services::token_analytics::TokenAnalyticsService; use crate::{ birdeye::{MockBirdeyeApi, TokenInfo}, error::AgentError, models::market_signal::SignalType, }; use rig_mongodb::MongoDbPool::MongoDbPool; #[cfg(test)] mod tests { use super::*; use crate::test_utils::{cleanup_test_db, setup_test_db}; use rig_mongodb::MongoDbPool; use std::sync::Arc; async fn setup_test_environment() -> ( Arc<MongoDbPool>, Arc<MockBirdeyeApi>, Arc<BirdeyeExtendedClient>, ) { let db = setup_test_db() .await .expect("Failed to setup test database"); let (birdeye, birdeye_extended) = setup_mock_birdeye(); (db, birdeye, birdeye_extended) } async fn cleanup_test_environment(pool: &MongoDbPool) { cleanup_test_db(pool, "cainam_test") .await .expect("Failed to cleanup test database"); } async fn setup_test_db() -> Arc<MongoDbPool> { let connection_string = "mongodb://localhost:32770"; MongoDbPool::new_from_uri(connection_string, "cainam_test") .await .expect("Failed to create test database pool") .into() } fn setup_mock_birdeye() -> (Arc<MockBirdeyeApi>, Arc<cainam_birdeye::BirdeyeClient>) { let mut mock = MockBirdeyeApi::new(); mock.expect_get_token_info().returning(|_| { Ok(TokenInfo { price: 100.0, volume_24h: 1000000.0, price_change_24h: 5.0, liquidity: 500000.0, trade_24h: 1000, }) }); ( Arc::new(mock), Arc::new(cainam_birdeye::BirdeyeClient::new("test_key")), ) } #[tokio::test] async fn test_fetch_token_info_success() -> AgentResult<()> { let db = setup_test_db().await; let (birdeye, birdeye_extended) = setup_mock_birdeye(); let market_config = MarketConfig::default(); let service = TokenAnalyticsService::new(db, birdeye, birdeye_extended, Some(market_config)); let analytics = service .fetch_and_store_token_info("SOL", "test_address") .await?; assert_eq!(analytics.token_symbol, "SOL"); assert_eq!(analytics.price, f64_to_decimal(100.0)); Ok(()) } #[tokio::test] async fn test_invalid_token_price() -> AgentResult<()> { let db = setup_test_db().await; let mut mock = MockBirdeyeApi::new(); mock.expect_get_token_info().returning(|_| { Ok(TokenInfo { price: -1.0, // Invalid price volume_24h: 1000000.0, price_change_24h: 5.0, liquidity: 500000.0, trade_24h: 1000, }) }); let service = TokenAnalyticsService::new( db, Arc::new(mock), Arc::new(cainam_birdeye::BirdeyeClient::new("test_key")), Some(MarketConfig::default()), ); let result = service .fetch_and_store_token_info("SOL", "test_address") .await; assert!(matches!(result, Err(AgentError::Validation(_)))); Ok(()) } #[tokio::test] async fn test_invalid_signal_confidence() -> AgentResult<()> { let db = setup_test_db().await; let (birdeye, birdeye_extended) = setup_mock_birdeye(); let mut market_config = MarketConfig::default(); // Set up config to generate invalid confidence market_config.base_confidence = f64_to_decimal(2.0); // Will result in confidence > 1 let service = TokenAnalyticsService::new(db, birdeye, birdeye_extended, Some(market_config)); let result = service .fetch_and_store_token_info("SOL", "test_address") .await; assert!(matches!(result, Err(AgentError::Validation(_)))); Ok(()) } #[tokio::test] async fn test_market_signal_generation() -> AgentResult<()> { let db = setup_test_db().await; let (birdeye, birdeye_extended) = setup_mock_birdeye(); let market_config = MarketConfig::default(); let service = TokenAnalyticsService::new(db.clone(), birdeye, birdeye_extended, Some(market_config)); // First store some historical data let mut tx = db.begin().await?; let analytics = TokenAnalytics { id: None, token_address: "test_address".to_string(), token_name: "Test Token".to_string(), token_symbol: "TEST".to_string(), price: f64_to_decimal(90.0), // Lower price to trigger price spike volume_24h: Some(f64_to_decimal(500000.0)), market_cap: Some(f64_to_decimal(1000000.0)), total_supply: Some(f64_to_decimal(10000.0)), holder_count: None, timestamp: Utc::now() - chrono::Duration::hours(1), created_at: None, }; service .store_token_analytics_tx(&mut tx, &analytics) .await?; tx.commit().await?; // Now fetch current data which should generate a signal let result = service .fetch_and_store_token_info("TEST", "test_address") .await?; let signal = service.generate_market_signals(&result).await?; assert!(signal.is_some()); let signal = signal.unwrap(); assert_eq!(signal.signal_type, SignalType::PriceSpike); assert!(signal.confidence > f64_to_decimal(0.0)); assert!(signal.confidence <= f64_to_decimal(1.0)); Ok(()) } #[tokio::test] async fn test_transaction_rollback() -> AgentResult<()> { let db = setup_test_db().await; let (birdeye, birdeye_extended) = setup_mock_birdeye(); let market_config = MarketConfig::default(); let service = TokenAnalyticsService::new(db.clone(), birdeye, birdeye_extended, Some(market_config)); // Start a transaction let mut tx = db.begin().await?; // Store valid analytics let analytics = TokenAnalytics { id: None, token_address: "test_address".to_string(), token_name: "Test Token".to_string(), token_symbol: "TEST".to_string(), price: f64_to_decimal(100.0), volume_24h: Some(f64_to_decimal(1000000.0)), market_cap: Some(f64_to_decimal(10000000.0)), total_supply: Some(f64_to_decimal(100000.0)), holder_count: None, timestamp: Utc::now(), created_at: None, }; service .store_token_analytics_tx(&mut tx, &analytics) .await?; // Rollback the transaction tx.rollback().await?; // Verify the data wasn't stored let result = service.get_latest_token_analytics("test_address").await?; assert!(result.is_none()); Ok(()) } }
```

# src/services/token_analytics.rs

```rs
use crate::birdeye::{BirdeyeApi, TokenInfo}; use crate::config::mongodb::MongoDbPool; use crate::config::MarketConfig; use crate::error::{AgentError, AgentResult}; use crate::logging::RequestLogger; use crate::models::market_signal::{MarketSignal, MarketSignalBuilder, SignalType}; use crate::models::token_analytics::TokenAnalytics; use crate::utils::f64_to_decimal; use bigdecimal::{BigDecimal, ToPrimitive}; use bson::{doc, DateTime}; use futures::StreamExt; use mongodb::options::{FindOneOptions, FindOptions}; use mongodb::Collection; use rig::providers::openai::{self, EmbeddingModel}; use rig_mongodb::{MongoDbVectorIndex, SearchParams}; use std::sync::Arc; use tracing::info; use uuid::Uuid; const TEXT_EMBEDDING_ADA_002: &str = "text-embedding-ada-002"; #[derive(Debug, thiserror::Error)] pub enum TokenAnalyticsError { #[error("Database error: {0}")] Database(String), #[error("Birdeye API error: {0}")] BirdeyeApi(String), #[error("Validation error: {0}")] Validation(String), } // impl From<MongoDbError> for TokenAnalyticsError { // fn from(err: MongoDbError) -> Self { // Self::Database(err.to_string()) // } // } // Remove the conflicting From implementation and use map_err where needed #[derive(Debug, Clone, serde::Serialize)] #[serde(rename_all = "camelCase")] pub struct MarketMetrics { pub symbol: String, pub price: f64, pub volume_24h: Option<f64>, pub signal_type: Option<String>, pub confidence: Option<f64>, } #[derive(Debug, Clone, serde::Serialize)] #[serde(rename_all = "camelCase")] pub struct MarketSignalLog { pub id: Uuid, pub timestamp: DateTime, pub token_address: String, pub token_symbol: String, pub signal_type: String, pub price: f64, pub price_change_24h: Option<f64>, pub volume_change_24h: Option<f64>, pub confidence: f64, pub risk_score: f64, pub created_at: DateTime, } // Helper functions for logging with proper references fn log_market_metrics(metrics: &MarketMetrics) { info!( symbol = %metrics.symbol, price = %metrics.price, volume_24h = ?metrics.volume_24h, signal_type = ?metrics.signal_type, confidence = ?metrics.confidence, "Market metrics recorded" ); } fn log_market_signal(signal: &MarketSignalLog) { info!( token = %signal.token_symbol, signal_type = %signal.signal_type, price_change = ?signal.price_change_24h, volume_change = ?signal.volume_change_24h, confidence = %signal.confidence, risk_score = %signal.risk_score, "Market signal generated" ); } pub struct TokenAnalyticsService { pool: Arc<MongoDbPool>, collection: Collection<TokenAnalytics>, signals_collection: Collection<MarketSignal>, vector_index: MongoDbVectorIndex<EmbeddingModel, TokenAnalytics>, birdeye: Arc<dyn BirdeyeApi>, market_config: MarketConfig, } impl TokenAnalyticsService { pub async fn new( pool: Arc<MongoDbPool>, birdeye: Arc<dyn BirdeyeApi>, market_config: Option<MarketConfig>, ) -> AgentResult<Self> { let db = pool.database(&pool.get_config().database); let collection = db.collection("token_analytics"); println!(">> token_analytics collections {:?}", collection); let signals_collection = db.collection("market_signals"); println!(">> market_signals collections {:?}", signals_collection); let openai_client = openai::Client::from_env(); let model = openai_client.embedding_model(openai::TEXT_EMBEDDING_ADA_002); // Check if vector search index exists let list_indexes_command = doc! { "listSearchIndexes": "token_analytics" }; let index_exists = match db.run_command(list_indexes_command).await { Ok(result) => { let indexes = result .get_document("cursor") .and_then(|cursor| cursor.get_array("firstBatch")) .map(|batch| !batch.is_empty()) .unwrap_or(false); if indexes { info!("Vector search index already exists for token_analytics"); } indexes } Err(_) => false, }; // Create vector search index if it doesn't exist if !index_exists { info!("Creating vector search index for token_analytics"); let command = doc! { "createSearchIndexes": "token_analytics", "indexes": [{ "name": "vector_index", "definition": { "mappings": { "dynamic": true, "fields": { "id": { "type": "string" }, "token_address": { "type": "string" }, "token_name": { "type": "string" }, "token_symbol": { "type": "string" }, "embedding": { "type": "knnVector", "dimensions": 1536, "similarity": "cosine" } } } } }] }; match db.run_command(command).await { Ok(_) => info!("Created vector index for token_analytics"), Err(e) => { info!("Failed to create vector index: {}", e); return Err(AgentError::Database(e)); } } } let search_params = SearchParams::new() .exact(true) .num_candidates(100); let vector_index = MongoDbVectorIndex::new( collection.clone(), model, "vector_index", search_params ) .await .map_err(|e| AgentError::VectorStore(e.to_string()))?; Ok(Self { pool: pool, collection, signals_collection, vector_index, birdeye, market_config: market_config.unwrap_or_default(), }) } pub async fn fetch_and_store_token_info( &self, symbol: &str, address: &str, ) -> AgentResult<TokenAnalytics> { let logger = RequestLogger::new("token_analytics", "fetch_and_store_token_info"); // Fetch basic token info from Birdeye using address let token_info = match self.birdeye.get_token_info_by_address(address).await { Ok(info) => info, Err(e) => { let err = AgentError::BirdeyeApi(format!("Failed to fetch token info: {}", e)); logger.error(&err.to_string()); return Err(err); } }; // Fetch extended token info using the comprehensive client let token_overview = match self.birdeye.get_token_info(address).await { Ok(overview) => overview, Err(e) => { let err = AgentError::BirdeyeApi(format!("Failed to fetch token overview: {}", e)); logger.error(&err.to_string()); return Err(err); } }; // Validate token data and log metrics if token_info.price <= 0.0 { let err = AgentError::validation("Token price must be positive"); logger.error(&err.to_string()); return Err(err); } if token_info.volume_24h < 0.0 { let err = AgentError::validation("Token volume cannot be negative"); logger.error(&err.to_string()); return Err(err); } // Log market metrics let metrics = MarketMetrics { symbol: symbol.to_string(), price: token_info.price, volume_24h: Some(token_info.volume_24h), signal_type: None, confidence: None, }; log_market_metrics(&metrics); // Convert to TokenAnalytics let analytics = match self .convert_to_analytics(address, symbol, token_info, token_overview) .await { Ok(analytics) => analytics, Err(e) => { logger.error(&e.to_string()); return Err(e); } }; // Store in database let stored = self.store_token_analytics(&analytics).await?; // Generate and process market signals let signal = self.generate_market_signals(&stored).await?; // Store the signal if present if let Some(ref signal) = signal { let zero = BigDecimal::from(0); let one = BigDecimal::from(1); if signal.confidence < zero || signal.confidence > one { return Err(AgentError::validation( "Signal confidence must be between 0 and 1", )); } if signal.risk_score < zero || signal.risk_score > one { return Err(AgentError::validation("Risk score must be between 0 and 1")); } self.store_market_signal(signal).await?; } Ok(stored) } // TODO: zTgx hardcoded async fn convert_to_analytics( &self, address: &str, symbol: &str, info: TokenInfo, overview: TokenInfo, ) -> AgentResult<TokenAnalytics> { Ok(TokenAnalytics { id: None, token_address: address.to_string(), token_name: "overview.name".to_string(), token_symbol: symbol.to_string(), price: f64_to_decimal(info.price), volume_24h: Some(f64_to_decimal(info.volume_24h)), market_cap: Some(f64_to_decimal(11.0)), // market_cap: Some(f64_to_decimal(overview.market_cap)), total_supply: Some(f64_to_decimal(11.1)), // total_supply: Some(f64_to_decimal(overview.total_supply)), holder_count: None, timestamp: DateTime::now(), created_at: None, metadata: Some(doc! {}), }) } pub async fn generate_market_signals( &self, analytics: &TokenAnalytics, ) -> AgentResult<Option<MarketSignal>> { let logger = RequestLogger::new("token_analytics", "generate_market_signals"); // Get previous analytics for comparison let previous = match self.get_previous_analytics(&analytics.token_address).await { Ok(prev) => prev, Err(e) => { logger.error(&e.to_string()); return Err(e); } }; if let Some(prev) = previous { let price_change = (analytics.price.clone() - prev.price.clone()) / prev.price.clone(); let volume_change = analytics.volume_24h.as_ref().map(|current| { let binding = BigDecimal::from(0); let prev = prev.volume_24h.as_ref().unwrap_or(&binding); (current.clone() - prev.clone()) / prev.clone() }); if price_change > self.market_config.price_change_threshold.clone() { let signal = self.create_market_signal( analytics, SignalType::PriceSpike, price_change.clone(), volume_change.clone(), ); self.log_signal(&signal, analytics); return Ok(Some(signal)); } else if price_change < -self.market_config.price_change_threshold.clone() { let signal = self.create_market_signal( analytics, SignalType::PriceDrop, price_change.abs(), volume_change.clone(), ); self.log_signal(&signal, analytics); return Ok(Some(signal)); } if let Some(vol_change) = volume_change { if vol_change > self.market_config.volume_surge_threshold { let signal = self.create_market_signal( analytics, SignalType::VolumeSurge, price_change, Some(vol_change), ); self.log_signal(&signal, analytics); return Ok(Some(signal)); } } } Ok(None) } fn create_market_signal( &self, analytics: &TokenAnalytics, signal_type: SignalType, price_change: BigDecimal, volume_change: Option<BigDecimal>, ) -> MarketSignal { let confidence = self.calculate_confidence( price_change.clone(), volume_change.clone().unwrap_or_else(|| BigDecimal::from(0)), ); MarketSignalBuilder::new( analytics.token_address.clone(), signal_type, analytics.price.clone(), ) .confidence(confidence) .risk_score(f64_to_decimal(0.5)) .price_change_24h(price_change) .volume_change_24h(volume_change.clone().unwrap_or_else(|| BigDecimal::from(0))) .volume_change(volume_change.unwrap_or_else(|| BigDecimal::from(0))) .timestamp(analytics.timestamp) .build() } async fn store_market_signal(&self, signal: &MarketSignal) -> AgentResult<()> { self.signals_collection .insert_one(signal) .await .map_err(|e| AgentError::Database(e))?; Ok(()) } pub async fn get_previous_analytics( &self, address: &str, ) -> AgentResult<Option<TokenAnalytics>> { let filter = doc! { "token_address": address, "timestamp": { "$lt": DateTime::now() } }; let options = FindOneOptions::builder() .sort(doc! { "timestamp": -1 }) .build(); self.collection .find_one(filter) .await .map_err(|e| AgentError::Database(e)) } async fn store_token_analytics( &self, analytics: &TokenAnalytics, ) -> AgentResult<TokenAnalytics> { let result = self .collection .insert_one(analytics) .await .map_err(|e| AgentError::Database(e))?; let mut stored = analytics.clone(); stored.id = result.inserted_id.as_object_id(); Ok(stored) } pub async fn get_token_history( &self, address: &str, start_time: DateTime, end_time: DateTime, limit: i64, offset: i64, ) -> AgentResult<Vec<TokenAnalytics>> { let filter = doc! { "token_address": address, "timestamp": { "$gte": start_time, "$lte": end_time } }; let options = FindOptions::builder() .sort(doc! { "timestamp": -1 }) .skip(Some(offset as u64)) .limit(Some(limit)) .build(); let mut cursor = self .collection .find(filter) .await .map_err(AgentError::Database)?; let mut results = Vec::new(); while let Some(doc) = cursor.next().await { results.push(doc?); } Ok(results) } pub async fn get_latest_token_analytics( &self, address: &str, ) -> AgentResult<Option<TokenAnalytics>> { let filter = doc! { "token_address": address }; let analytics = self .collection .find_one(filter) .await .map_err(AgentError::Database)?; Ok(analytics) } pub fn calculate_volume_change( &self, current: &BigDecimal, prev: &TokenAnalytics, ) -> Option<BigDecimal> { prev.volume_24h.as_ref().map(|prev_vol| { let zero = BigDecimal::from(0); let prev_value = if prev_vol == &zero { BigDecimal::from(1) } else { prev_vol.clone() }; (current - prev_vol) / prev_value }) } } impl TokenAnalyticsService { fn log_signal(&self, signal: &MarketSignal, analytics: &TokenAnalytics) { let signal_log = MarketSignalLog { id: Uuid::new_v4(), timestamp: DateTime::now(), token_address: signal.asset_address.clone(), token_symbol: analytics.token_symbol.clone(), signal_type: signal.signal_type.to_string(), price: analytics.price.to_f64().unwrap_or_default(), price_change_24h: Some( signal .price_change_24h .as_ref() .and_then(|p| p.to_f64()) .unwrap_or_default(), ), volume_change_24h: signal.volume_change_24h.as_ref().and_then(|v| v.to_f64()), confidence: signal.confidence.to_f64().unwrap_or_default(), risk_score: signal.risk_score.to_f64().unwrap_or_default(), created_at: DateTime::now(), }; log_market_signal(&signal_log); } fn calculate_confidence( &self, price_change: BigDecimal, volume_change: BigDecimal, ) -> BigDecimal { self.market_config.base_confidence.clone() + (price_change * self.market_config.price_weight.clone()) + (volume_change * self.market_config.volume_weight.clone()) } pub async fn process_market_signal(&self, signal: MarketSignal) -> AgentResult<()> { let _logger = RequestLogger::new("token_analytics", "process_market_signal"); let signal_log = MarketSignalLog { id: Uuid::new_v4(), timestamp: DateTime::now(), token_address: signal.asset_address.clone(), token_symbol: signal .metadata .expect("Failed to get token symbol from metadata") .get("token_symbol") .and_then(|v| v.as_str()) .unwrap_or(&signal.asset_address) .to_string(), signal_type: signal.signal_type.to_string(), price: signal.price.to_f64().unwrap_or_default(), price_change_24h: signal .price_change_24h .map(|p| p.to_f64().unwrap_or_default()), volume_change_24h: signal .volume_change_24h .map(|v| v.to_f64().unwrap_or_default()), confidence: signal.confidence.to_f64().unwrap_or_default(), risk_score: signal.risk_score.to_f64().unwrap_or_default(), created_at: signal.created_at.unwrap_or_else(DateTime::now), }; log_market_signal(&signal_log); Ok(()) } } impl From<MarketSignal> for MarketSignalLog { fn from(signal: MarketSignal) -> Self { Self { id: Uuid::new_v4(), timestamp: DateTime::now(), token_address: signal.asset_address.clone(), token_symbol: signal .metadata .expect("Failed to get token symbol from metadata") .get("token_symbol") .and_then(|v| v.as_str()) .unwrap_or(&signal.asset_address) .to_string(), signal_type: signal.signal_type.to_string(), price: signal.price.to_f64().unwrap_or_default(), price_change_24h: Some( signal .price_change_24h .and_then(|p| p.to_f64()) .unwrap_or_default(), ), volume_change_24h: signal.volume_change_24h.and_then(|v| v.to_f64()), confidence: signal.confidence.to_f64().unwrap_or_default(), risk_score: signal.risk_score.to_f64().unwrap_or_default(), created_at: signal.created_at.unwrap_or_else(DateTime::now), } } } #[cfg(test)] mod tests { use super::*; use mockall::mock; use mockall::predicate::*; use mongodb; // Add explicit mongodb dependency mock! { pub BirdeyeApi { async fn get_token_info(&self, symbol: &str) -> AgentResult<TokenInfo>; async fn get_token_info_by_address(&self, address: &str) -> AgentResult<TokenInfo>; } } // async fn setup_test_db() -> MongoDbPool { // let client_options = mongodb::options::ClientOptions::parse("mongodb://localhost:32770") // .await // .expect("Failed to parse MongoDB URI"); // MongoDbPool::create_pool(client_options) // .await // .expect("Failed to create MongoDB pool") // } fn setup_mock_birdeye() -> Arc<MockBirdeyeApi> { let mut mock = MockBirdeyeApi::new(); mock.expect_get_token_info_by_address().returning(|_| { Ok(TokenInfo { price: 100.0, volume_24h: 1000000.0, price_change_24h: 5.0, liquidity: 500000.0, trade_24h: 1000, }) }); Arc::new(mock) } // #[tokio::test] // async fn test_market_signal_generation() -> AgentResult<()> { // let pool = setup_test_db().await; // let birdeye = setup_mock_birdeye(); // let market_config = MarketConfig { // price_change_threshold: f64_to_decimal(0.05), // volume_surge_threshold: f64_to_decimal(0.2), // base_confidence: f64_to_decimal(0.5), // price_weight: f64_to_decimal(0.3), // volume_weight: f64_to_decimal(0.2), // }; // let service = TokenAnalyticsService::new( // pool, // birdeye, // Some(market_config), // ).await?; // // Create test data // let analytics = TokenAnalytics { // id: None, // token_address: "test_address".to_string(), // token_name: "Test Token".to_string(), // token_symbol: "TEST".to_string(), // price: f64_to_decimal(100.0), // volume_24h: Some(f64_to_decimal(1000000.0)), // market_cap: Some(f64_to_decimal(1000000.0)), // total_supply: Some(f64_to_decimal(10000.0)), // holder_count: Some(1000), // timestamp: Utc::now(), // created_at: Some(Utc::now()), // metadata: Some(serde_json::json!({ // "network": "solana", // "decimals": 9 // })), // }; // let result = service.store_token_analytics(&analytics).await?; // assert!(result.id.is_some(), "Should have assigned an ID"); // let history = service.get_token_history( // "test_address", // Utc::now() - chrono::Duration::hours(24), // Utc::now(), // 10, // 0 // ).await?; // assert!(!history.is_empty(), "Should have historical data"); // Ok(()) // } }
```

# src/strategy/llm.rs

```rs
use crate::market_data::{birdeye::BirdEyeProvider, DataProvider}; use anyhow::Result; use std::sync::Arc; use tracing::{debug, instrument}; pub struct LLMStrategy { birdeye: Arc<BirdEyeProvider>, } impl LLMStrategy { pub fn new(birdeye: Arc<BirdEyeProvider>) -> Self { Self { birdeye } } #[instrument(skip(self))] pub async fn analyze_trading_opportunity(&self, prompt: &str, sol_balance: f64) -> Result<String> { debug!("Analyzing trading opportunity with prompt: {}", prompt); // Format the analysis with the available SOL balance let analysis = format!( "Available SOL: {}\n{}", sol_balance, prompt ); Ok(analysis) } }
```

# src/strategy/mod.rs

```rs
#[instrument(skip(self))] pub async fn analyze_trading_opportunity(&self, prompt: String, sol_balance: f64) -> Result<String> { info!("Analyzing trading opportunity with prompt: {}", prompt); // Format the prompt with market analysis requirements let formatted_prompt = format!( "{}\n\nAnalyze this trading opportunity and provide a detailed recommendation in the following JSON format:\n{{ \"action\": \"Buy|Sell|Hold\", \"token_address\": \"string\", \"amount_in_sol\": number, \"reasoning\": \"string\", \"confidence\": number (0.0-1.0), \"risk_assessment\": \"string\", \"market_analysis\": {{ \"volume_analysis\": {{ \"current_volume_usd\": number, \"volume_change_24h\": number, \"is_volume_bullish\": boolean, \"analysis\": \"string\" }}, \"price_trend\": {{ \"current_trend\": \"string\", \"support_levels\": [number], \"resistance_levels\": [number], \"trend_strength\": number (0.0-1.0) }}, \"liquidity_assessment\": {{ \"liquidity_score\": number (0.0-1.0), \"slippage_estimate\": number, \"is_liquid_enough\": boolean }}, \"momentum_indicators\": {{ \"rsi_14\": number, \"macd\": {{ \"value\": number, \"signal\": \"bullish|bearish|neutral\" }}, \"overall_momentum\": \"strong_buy|buy|neutral|sell|strong_sell\" }}, \"on_chain_metrics\": {{ \"unique_holders\": number, \"holder_concentration\": number (0.0-1.0), \"smart_money_flow\": \"inflow|outflow|neutral\" }} }}, \"execution_strategy\": {{ \"entry_type\": \"market|limit\", \"position_size_sol\": number, \"stop_loss_pct\": number, \"take_profit_levels\": [{{ \"price_target\": number, \"size_pct\": number }}], \"time_horizon\": \"short|medium|long\", \"dca_strategy\": {{ \"should_dca\": boolean, \"interval_hours\": number, \"num_entries\": number }} }} }}\n\nAvailable SOL balance: {} SOL\n\nConsider the following criteria for the analysis:\n1. Volume should show significant increase (>50% 24h change) with sustainable growth\n2. Price action should show clear trend with identifiable support/resistance levels\n3. Liquidity should be sufficient to enter/exit position with <2% slippage\n4. Momentum indicators should align with the overall trend\n5. Smart money flow should indicate institutional interest\n6. Risk:reward ratio should be at least 1:3 for any trade", prompt, sol_balance ); // Get analysis from LLM let analysis = self.agent.complete(&formatted_prompt).await?; info!("Received analysis from LLM"); Ok(analysis) }
```

# src/test_utils.rs

```rs
use rig_mongodb::{MongoDbPool, bson::doc}; use std::sync::Arc; use anyhow::Result; use once_cell::sync::Lazy; use tokio::sync::Mutex; use crate::config::mongodb::MongoConfig; // Ensure test databases are unique per test static TEST_DB_COUNTER: Lazy<Mutex<u32>> = Lazy::new(|| Mutex::new(0)); pub async fn get_unique_test_db_name() -> String { let mut counter = TEST_DB_COUNTER.lock().await; let db_name = format!("test_db_{}", *counter); *counter += 1; db_name } #[cfg(test)] pub mod test_utils { use super::*; use crate::config::{mongodb::MongoConfig, pool::MongoPoolConfig}; use rig_mongodb::MongoDbPool; use std::sync::Arc; use anyhow::Result; use std::time::Duration; pub async fn setup_test_db() -> Result<(Arc<MongoDbPool>, String)> { let db_name = get_unique_test_db_name().await; let config = MongoConfig { database: db_name.clone(), pool: crate::config::pool::MongoPoolConfig { min_pool_size: 1, max_pool_size: 2, connect_timeout: std::time::Duration::from_secs(5), }, ..Default::default() }; let pool = config.create_pool().await?; // Initialize test collections setup_test_collections(&pool, &db_name).await?; Ok((pool, db_name)) } pub async fn cleanup_test_db(pool: &MongoDbPool, db_name: &str) -> Result<()> { pool.database(db_name).drop().await?; Ok(()) } async fn setup_test_collections(pool: &MongoDbPool, db_name: &str) -> Result<()> { let db = pool.database(db_name); db.create_collection("test_market_signals", Some(doc! { "timeseries": { "timeField": "timestamp", "metaField": "asset_address", "granularity": "minutes" } })).await?; db.collection("test_market_signals").create_index( doc! { "asset_address": 1, "timestamp": -1 }, None, ).await?; Ok(()) } pub async fn insert_test_data(pool: &MongoDbPool, db_name: &str, collection: &str, data: Vec<bson::Document>) -> Result<()> { let coll = pool.database(db_name).collection(collection); coll.insert_many(data, None).await?; Ok(()) } }
```

# src/trading/mod.rs

```rs
pub mod trading_engine; use anyhow::Result; use solana_client::rpc_client::RpcClient; pub struct SolanaAgentKit { rpc_client: RpcClient, wallet_keypair: solana_sdk::signer::keypair::Keypair, } impl SolanaAgentKit { pub fn new(rpc_url: &str, wallet_keypair: solana_sdk::signer::keypair::Keypair) -> Self { Self { rpc_client: RpcClient::new(rpc_url.to_string()), wallet_keypair, } } pub fn new_from_env() -> Result<Self> { let rpc_url = std::env::var("SOLANA_RPC_URL")?; let wallet_key = std::env::var("SOLANA_PRIVATE_KEY")?; // Parse the base58 private key let wallet_keypair = solana_sdk::signer::keypair::Keypair::from_base58_string(&wallet_key); Ok(Self::new(&rpc_url, wallet_keypair)) } pub fn get_rpc_client(&self) -> &RpcClient { &self.rpc_client } pub fn get_wallet_keypair(&self) -> &solana_sdk::signer::keypair::Keypair { &self.wallet_keypair } }
```

# src/trading/trading_engine.rs

```rs
use super::SolanaAgentKit; use crate::models::market_signal::{MarketSignal, SignalType}; use crate::utils::{decimal_to_f64, f64_to_decimal}; use anyhow::Result; use tracing::{info, warn}; pub struct TradingEngine { min_confidence: f64, max_trade_size: f64, agent: SolanaAgentKit, } #[derive(Debug)] pub struct TradeDecision { pub action: String, pub symbol: String, pub amount: f64, pub reason: String, pub confidence: f64, pub mint_address: Option<String>, } impl TradingEngine { pub fn new(min_confidence: f64, max_trade_size: f64, agent: SolanaAgentKit) -> Self { Self { min_confidence, max_trade_size, agent, } } pub async fn execute_trade(&self, signal: &MarketSignal) -> Result<String> { let min_conf = f64_to_decimal(self.min_confidence); if signal.confidence < min_conf { warn!("Signal confidence too low for trading"); return Ok("Signal confidence too low".to_string()); } let max_size = f64_to_decimal(self.max_trade_size); let _amount = decimal_to_f64(&(max_size.clone() * signal.confidence.clone()).min(max_size)); let action = match signal.signal_type { SignalType::Buy | SignalType::StrongBuy | SignalType::PriceSpike | SignalType::VolumeSurge => "BUY", SignalType::Sell | SignalType::StrongSell | SignalType::PriceDrop => "SELL", SignalType::Hold => "HOLD", }; info!( "Executing {} trade for {} with confidence {:.2}", action, signal.asset_address, decimal_to_f64(&signal.confidence) ); // TODO: Implement actual Solana transaction execution // For now, just return a mock signature Ok(format!( "mock_tx_{}_{}", action.to_lowercase(), signal.asset_address )) } pub fn get_min_confidence(&self) -> f64 { self.min_confidence } pub fn get_max_trade_size(&self) -> f64 { self.max_trade_size } }
```

# src/twitter/mod.rs

```rs
use anyhow::{anyhow, Result}; use reqwest::{ header::{HeaderMap, HeaderValue, CONTENT_TYPE}, Client, }; use serde_json::json; use tracing::{error, info}; // Remove trait definition since we're not using trait objects pub struct TwitterClient { client: Client, email: String, username: String, password: String, auth_token: Option<String>, } impl TwitterClient { pub fn new(email: String, username: String, password: String) -> Self { Self { client: Client::new(), email, username, password, auth_token: None, } } pub async fn login(&mut self) -> Result<()> { let mut headers = HeaderMap::new(); headers.insert(CONTENT_TYPE, HeaderValue::from_static("application/json")); // Direct auth endpoint let payload = json!({ "email": self.email, "username": self.username, "password": self.password }); let response = self .client .post("https://x.com/i/flow/login") .headers(headers) .json(&payload) .send() .await?; if response.status().is_success() { // Extract auth token from cookies if let Some(cookies) = response.headers().get("set-cookie") { if let Ok(cookie_str) = cookies.to_str() { if let Some(auth_token) = extract_auth_token(cookie_str) { info!("Successfully logged in to Twitter"); self.auth_token = Some(auth_token); return Ok(()); } } } Err(anyhow!("No auth token found in response")) } else { let error_message = response.text().await.unwrap_or_default(); error!("Failed to login to Twitter: {}", error_message); Err(anyhow!("Failed to login to Twitter: {}", error_message)) } } pub async fn post_tweet(&self, text: &str) -> Result<()> { if self.auth_token.is_none() { return Err(anyhow!("Not authenticated")); } let mut headers = HeaderMap::new(); headers.insert(CONTENT_TYPE, HeaderValue::from_static("application/json")); // Add auth token cookie headers.insert( "cookie", HeaderValue::from_str(&format!("auth_token={}", self.auth_token.as_ref().unwrap()))?, ); let payload = json!({ "text": text, "queryId": "PvJGyyJKzm2-aIsTo6tLSg" // Twitter's internal query ID for posting tweets }); let response = self .client .post("https://x.com/i/api/graphql/PvJGyyJKzm2-aIsTo6tLSg/CreateTweet") .headers(headers) .json(&payload) .send() .await?; if response.status().is_success() { info!("Successfully posted tweet"); Ok(()) } else { let error_message = response.text().await.unwrap_or_default(); error!("Failed to post tweet: {}", error_message); Err(anyhow!("Failed to post tweet: {}", error_message)) } } pub async fn delete_tweet(&self, tweet_id: &str) -> Result<()> { if self.auth_token.is_none() { return Err(anyhow!("Not authenticated")); } let mut headers = HeaderMap::new(); headers.insert(CONTENT_TYPE, HeaderValue::from_static("application/json")); // Add auth token cookie headers.insert( "cookie", HeaderValue::from_str(&format!("auth_token={}", self.auth_token.as_ref().unwrap()))?, ); let payload = json!({ "tweet_id": tweet_id, "queryId": "VaenaVgh5q5ih7kvyVjgtg" // Twitter's internal query ID for deleting tweets }); let response = self .client .post("https://x.com/i/api/graphql/VaenaVgh5q5ih7kvyVjgtg/DeleteTweet") .headers(headers) .json(&payload) .send() .await?; if response.status().is_success() { info!("Successfully deleted tweet {}", tweet_id); Ok(()) } else { let error_message = response.text().await.unwrap_or_default(); error!("Failed to delete tweet {}: {}", tweet_id, error_message); Err(anyhow!("Failed to delete tweet: {}", error_message)) } } } // Helper function to extract auth token from cookies fn extract_auth_token(cookie_str: &str) -> Option<String> { cookie_str .split(';') .find(|s| s.trim().starts_with("auth_token=")) .and_then(|s| s.trim().strip_prefix("auth_token=")) .map(|s| s.to_string()) } #[cfg(test)] mod tests { use super::*; #[tokio::test] async fn test_extract_auth_token() { let cookie_str = "auth_token=abc123; Path=/; Domain=.x.com; Secure; HttpOnly"; assert_eq!(extract_auth_token(cookie_str), Some("abc123".to_string())); } #[tokio::test] async fn test_auth_token_none() { let client = TwitterClient::new( "test@example.com".to_string(), "testuser".to_string(), "password".to_string(), ); // Test that unauthorized operations fail let tweet_result = client.post_tweet("Test tweet").await; assert!(tweet_result.is_err()); let delete_result = client.delete_tweet("123").await; assert!(delete_result.is_err()); } }
```

# src/twitter/tests.rs

```rs
use mockall::predicate::*; use mockall::*; mock! { pub TwitterClient { fn login(&self) -> Result<()>; fn post_tweet(&self, text: String) -> Result<()>; // Add other methods you need to mock } } #[tokio::test] async fn test_twitter_client() { let mut mock_client = MockTwitterClient::new(); mock_client .expect_login() .times(1) .returning(|| Ok(())); mock_client .expect_post_tweet() .with(predicate::any()) .times(1) .returning(|_| Ok(())); // Use mock client in your tests assert!(mock_client.login().is_ok()); }
```

# src/utils/mod.rs

```rs
use bigdecimal::FromPrimitive; use bigdecimal::{BigDecimal, ToPrimitive}; pub fn f64_to_decimal(value: f64) -> BigDecimal { BigDecimal::from_f64(value).unwrap_or_else(|| BigDecimal::from(0)) } pub fn decimal_to_f64(value: &BigDecimal) -> f64 { value.to_f64().unwrap_or(0.0) }
```

# src/vector_store/mod.rs

```rs
use anyhow::{Context, Result}; use rig_core::vector_store::{Document, Store}; use rig_mongodb::MongoStore; use std::sync::Arc; use tracing::{info, warn}; use crate::config::mongodb::{MongoConfig, MongoDbPool}; use serde_json; pub struct VectorStore { store: Arc<MongoStore>, } impl VectorStore { pub async fn new() -> Result<Self> { // Use centralized MongoDB configuration let config = MongoConfig::from_env(); info!("Initializing vector store connection"); let pool = MongoDbPool::create_pool(config.clone()) .await .context("Failed to create MongoDB pool")?; // Configure vector store with optimized search parameters and fields let fields = serde_json::json!({ "fields": [{ "path": "embedding", "numDimensions": 1536, "similarity": "cosine" }] }); let store = MongoStore::new( pool.client(), &config.database, "token_analytics", fields ).await .context("Failed to create vector store")?; Ok(Self { store: Arc::new(store), }) } pub async fn insert_documents<T>(&self, documents: Vec<T>) -> Result<()> where T: Send + Sync + 'static + serde::Serialize + Document, { info!("Inserting documents into vector store"); self.store.insert_documents(&documents) .await .context("Failed to insert documents into vector store")?; Ok(()) } pub async fn top_n<T>(&self, query: &str, limit: usize) -> Result<Vec<(f32, T)>> where T: Send + Sync + for<'de> serde::de::Deserialize<'de> + 'static, { if limit == 0 { warn!("top_n called with limit=0, defaulting to 1"); let limit = 1; } info!("Performing vector similarity search with limit {}", limit); let results = self.store.search::<T>(query, limit) .await .context("Failed to perform vector similarity search")?; info!("Found {} matching documents", results.len()); Ok(results) } #[cfg(test)] pub async fn cleanup_test_data(&self) -> Result<()> { // Implement cleanup logic for MongoDB if necessary Ok(()) } } #[cfg(test)] mod tests { use super::*; use serde::{Deserialize, Serialize}; use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt}; #[derive(Serialize, Deserialize, Clone, Debug, Eq, PartialEq)] struct TestDocument { id: String, content: String, } impl Document for TestDocument { fn text(&self) -> &str { &self.content } } fn init_test_logging() { let _ = tracing_subscriber::registry() .with(tracing_subscriber::EnvFilter::try_from_default_env() .unwrap_or_else(|_| "info".into())) .with(tracing_subscriber::fmt::layer()) .try_init(); } #[tokio::test] async fn test_vector_store() -> Result<()> { init_test_logging(); dotenvy::dotenv().ok(); let store = VectorStore::new() .await .context("Failed to create vector store")?; // Clean up any existing test data store.cleanup_test_data() .await .context("Failed to cleanup existing test data")?; let docs = vec![ TestDocument { id: "1".to_string(), content: "Test document one".to_string(), }, TestDocument { id: "2".to_string(), content: "Test document two".to_string(), }, ]; store.insert_documents(docs) .await .context("Failed to insert test documents")?; let results = store.top_n::<TestDocument>("test document", 2) .await .context("Failed to perform similarity search")?; assert!(!results.is_empty(), "Expected non-empty search results"); assert_eq!(results.len(), 2, "Expected exactly 2 search results"); // Clean up test data store.cleanup_test_data() .await .context("Failed to cleanup test data")?; Ok(()) } }
```

# tests/integration/mod.rs

```rs
mod trade_flow_test; pub use trade_flow_test::*;
```

# tests/integration/trade_flow_test.rs

```rs
use cainam_core::{ agent::trader::{AgentConfig, TradingAgent}, config::{MarketConfig, mongodb::MongoConfig, pool::MongoPoolConfig}, error::AgentResult, models::{ market_signal::{MarketSignal, SignalType}, token_analytics::TokenAnalytics, }, services::token_analytics::TokenAnalyticsService, SolanaAgentKit, }; use chrono::Utc; use rig_mongodb::MongoDbPool; use std::sync::Arc; use bigdecimal::BigDecimal; use std::time::Duration; async fn setup_test_db() -> Arc<MongoDbPool> { let config = MongoConfig { database: "cainam_test".to_string(), pool: MongoPoolConfig { min_pool_size: 1, max_pool_size: 2, connect_timeout: Duration::from_secs(5), }, ..Default::default() }; config.create_pool() .await .expect("Failed to create database pool") } async fn cleanup_test_db(pool: &MongoDbPool) { pool.database("cainam_test") .drop() .await .expect("Failed to cleanup test database"); } async fn setup_test_config() -> AgentConfig { AgentConfig { openai_api_key: "test_key".to_string(), birdeye_api_key: "test_key".to_string(), twitter_email: "test@example.com".to_string(), twitter_username: "test_user".to_string(), twitter_password: "test_pass".to_string(), analysis_interval: std::time::Duration::from_secs(1), trade_min_confidence: 0.7, trade_max_amount: 1000.0, } } #[tokio::test] async fn test_full_trade_flow() -> AgentResult<()> { // Setup let db = setup_test_db().await; let config = setup_test_config().await; let solana_agent = SolanaAgentKit::new_from_env()?; // Initialize trading agent let agent = TradingAgent::new(config, db.clone(), solana_agent).await?; // Test market analysis let signal = agent.analyze_market( "SOL", "So11111111111111111111111111111111111111112" ).await?; assert!(signal.is_some()); // Test signal processing if let Some(signal) = signal { let action = agent.process_signal(&signal).await?; assert!(action.is_some()); // Test trade execution if let Some(action) = action { match action.as_str() { "BUY" | "SELL" => { let result = agent.execute_trade("SOL", &signal).await; assert!(result.is_ok()); // Test post-trade update let update_result = agent.post_trade_update( "SOL", &action, 100.0, &signal.signal_type ).await; assert!(update_result.is_ok()); } _ => {} } } } // Cleanup test data cleanup_test_db(&db).await; Ok(()) } #[tokio::test] async fn test_concurrent_market_analysis() -> AgentResult<()> { let db = setup_test_db().await; let config = setup_test_config().await; let solana_agent = SolanaAgentKit::new_from_env()?; let agent = TradingAgent::new(config, db.clone(), solana_agent).await?; // Run multiple market analyses concurrently let handles: Vec<_> = vec![ ("SOL", "So11111111111111111111111111111111111111112"), ("BONK", "DezXAZ8z7PnrnRJjz3wXBoRgixCa6xjnB7YaB1pPB263"), ] .into_iter() .map(|(symbol, address)| { let agent = agent.clone(); tokio::spawn(async move { agent.analyze_market(symbol, address).await }) }) .collect(); // Wait for all analyses to complete for handle in handles { let result = handle.await.expect("Task panicked")?; assert!(result.is_some()); } cleanup_test_db(&db).await; Ok(()) } #[tokio::test] async fn test_error_recovery() -> AgentResult<()> { let db = setup_test_db().await; let config = setup_test_config().await; let solana_agent = SolanaAgentKit::new_from_env()?; let agent = TradingAgent::new(config, db.clone(), solana_agent).await?; // Start the agent let agent_handle = { let agent = agent.clone(); tokio::spawn(async move { agent.run().await }) }; // Let it run for a bit tokio::time::sleep(Duration::from_secs(2)).await; // Stop the agent agent.stop(); // Verify clean shutdown let result = agent_handle.await.expect("Task panicked"); assert!(result.is_ok()); cleanup_test_db(&db).await; Ok(()) } #[tokio::test] async fn test_performance() -> AgentResult<()> { use tokio::time::Instant; let db = setup_test_db().await; let config = setup_test_config().await; let solana_agent = SolanaAgentKit::new_from_env()?; let agent = TradingAgent::new(config, db.clone(), solana_agent).await?; // Measure market analysis performance let start = Instant::now(); let signal = agent.analyze_market( "SOL", "So11111111111111111111111111111111111111112" ).await?; let duration = start.elapsed(); // Analysis should complete within reasonable time assert!(duration.as_secs() < 5); assert!(signal.is_some()); cleanup_test_db(&db).await; Ok(()) }
```

